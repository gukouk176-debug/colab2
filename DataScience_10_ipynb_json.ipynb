{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gukouk176-debug/colab2/blob/main/DataScience_10_ipynb_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChljWf-IGYjg"
      },
      "source": [
        "# 第10回講義 分類３\n",
        "\n",
        "+ 近傍法\n",
        "+ ニューラルネットワーク\n",
        "+ XGboost\n",
        "+ （参考）ナイーブベイズ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCfBQ4osGYjZ"
      },
      "source": [
        "## 全講義共通初期設定\n",
        "+ 警告の非表示(実装時は非推奨)\n",
        "+ numpy pandas小数点以下桁数の表示設定\n",
        "+ pandas全データ表示設定\n",
        "+ Google driveへの接続"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0CXCmd3GYjf"
      },
      "source": [
        "# ワーニングを非表示にする\n",
        "# この設定は不都合が見えなくなる為、お勧めしない\n",
        "# 今回は教育資料用に、出力を簡素化する為に利用する\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "# モジュールの読み込み\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 小数点以下桁数の表示設定\n",
        "np.set_printoptions(precision = 3)\n",
        "pd.options.display.precision = 3\n",
        "\n",
        "# pandasの全データ表示設定\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTggs-UvG588",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df68e66-f534-430f-df5d-47b93065f953"
      },
      "source": [
        "#google driveに接続\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YestgmkLG5_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5454bb8-6236-4a77-b197-d64d4cf6c5be"
      },
      "source": [
        "#google driveと接続できたかを確認\n",
        "!ls drive/MyDrive/DataScience"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'3(00000).txt'\t'3(00004).txt'\t\t       iris.csv\t\t   ファイル_000\n",
            "'3(00001).txt'\t breast-cancer-wisconsin.csv   titanic_train.csv\n",
            "'3(00002).txt'\t example.xlsx\t\t      'wine 2.csv'\n",
            "'3(00003).txt'\t imports-85.csv\t\t       wine.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVihzEnzGYjk"
      },
      "source": [
        "## K近傍法\n",
        "+ タイタニックデータを用いる\n",
        "+ 生存したかどうかを分類する"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#タイタニックデータの読み込み\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "titanic_source_df = pd.read_csv('/content/drive/MyDrive/DataScience/titanic_train.csv',sep=',')\n",
        "\n",
        "titanic_source_df.head()"
      ],
      "metadata": {
        "id": "utXxyuaQGgbj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "698780f6-56ac-48eb-bcb5-040e93204e61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "4            5         0       3   \n",
              "\n",
              "                                                Name     Sex   Age  SibSp  \\\n",
              "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
              "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "4                           Allen, Mr. William Henry    male  35.0      0   \n",
              "\n",
              "   Parch            Ticket    Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.250   NaN        S  \n",
              "1      0          PC 17599  71.283   C85        C  \n",
              "2      0  STON/O2. 3101282   7.925   NaN        S  \n",
              "3      0            113803  53.100  C123        S  \n",
              "4      0            373450   8.050   NaN        S  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0384fd73-0458-41b1-be25-4f9069aefc66\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.283</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.925</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.100</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Allen, Mr. William Henry</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>373450</td>\n",
              "      <td>8.050</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0384fd73-0458-41b1-be25-4f9069aefc66')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0384fd73-0458-41b1-be25-4f9069aefc66 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0384fd73-0458-41b1-be25-4f9069aefc66');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "titanic_source_df",
              "summary": "{\n  \"name\": \"titanic_source_df\",\n  \"rows\": 891,\n  \"fields\": [\n    {\n      \"column\": \"PassengerId\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 257,\n        \"min\": 1,\n        \"max\": 891,\n        \"num_unique_values\": 891,\n        \"samples\": [\n          710,\n          440,\n          841\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Survived\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pclass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 891,\n        \"samples\": [\n          \"Moubarek, Master. Halim Gonios (\\\"William George\\\")\",\n          \"Kvillner, Mr. Johan Henrik Johannesson\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"female\",\n          \"male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.526497332334044,\n        \"min\": 0.42,\n        \"max\": 80.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          0.75,\n          22.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SibSp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ticket\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 681,\n        \"samples\": [\n          \"11774\",\n          \"248740\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49.693428597180905,\n        \"min\": 0.0,\n        \"max\": 512.3292,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          11.2417,\n          51.8625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cabin\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 147,\n        \"samples\": [\n          \"D45\",\n          \"B49\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embarked\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"S\",\n          \"C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_source_df.info()"
      ],
      "metadata": {
        "id": "NQgXSPAhIToi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d399b28c-bdf5-4cb6-ef99-2bdae5661de1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# すべての欠損値を平均で置換する、※数値型のみ\n",
        "#titanic_df = titanic_source_df.fillna(titanic_source_df.mean())\n",
        "\n",
        "#変数の設定\n",
        "#X = titanic_df.iloc[:, [2,5,6,7,9]]\n",
        "#y = titanic_df.iloc[:,[1]]\n",
        "\n",
        "numeric_columns = titanic_source_df.select_dtypes(include=np.number).columns\n",
        "titanic_df = titanic_source_df.copy() # 元のデータフレームを保護するためにコピーを作成\n",
        "titanic_df[numeric_columns] = titanic_df[numeric_columns].fillna(titanic_df[numeric_columns].mean())\n",
        "\n",
        "#変数の設定\n",
        "X = titanic_df.iloc[:, [2,5,6,7,9]]\n",
        "y = titanic_df.iloc[:,[1]]"
      ],
      "metadata": {
        "id": "6K1EH_woG4k8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#トレーニングデータとテストデータに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None )\n",
        "\n",
        "#説明変数の標準化\n",
        "sc = StandardScaler()\n",
        "sc.fit(X_train)\n",
        "X_train_std = sc.transform(X_train)\n",
        "X_test_std = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "YS6E6v-iJiSL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "HNY4ewS7GYjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af4f0a8c-404a-4390-dd65-ae852b4f5eda"
      },
      "source": [
        "# k近傍法によるモデルの生成と学習\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_std, y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(knn.score(X_train_std, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(knn.score(X_test_std, y_test)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.8026\n",
            "正解率（test):  0.7201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(knn.predict(X_test_std))"
      ],
      "metadata": {
        "id": "hEPKnsQqM30h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fab2f04-a1ec-4b56-8304-c60ca398474c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 1 1\n",
            " 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 1\n",
            " 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 1 0 0 0 0 0 1 0 1 1\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0\n",
            " 0 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0\n",
            " 0 1 1 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0\n",
            " 1 1 1 0 0 0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHAYlrPpGYjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41fcac71-8ec4-4920-9c75-5dcc3b7ea120"
      },
      "source": [
        "help(KNeighborsClassifier)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class KNeighborsClassifier in module sklearn.neighbors._classification:\n",
            "\n",
            "class KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
            " |  KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
            " |\n",
            " |  Classifier implementing the k-nearest neighbors vote.\n",
            " |\n",
            " |  Read more in the :ref:`User Guide <classification>`.\n",
            " |\n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  n_neighbors : int, default=5\n",
            " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
            " |\n",
            " |  weights : {'uniform', 'distance'}, callable or None, default='uniform'\n",
            " |      Weight function used in prediction.  Possible values:\n",
            " |\n",
            " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
            " |        are weighted equally.\n",
            " |      - 'distance' : weight points by the inverse of their distance.\n",
            " |        in this case, closer neighbors of a query point will have a\n",
            " |        greater influence than neighbors which are further away.\n",
            " |      - [callable] : a user-defined function which accepts an\n",
            " |        array of distances, and returns an array of the same shape\n",
            " |        containing the weights.\n",
            " |\n",
            " |      Refer to the example entitled\n",
            " |      :ref:`sphx_glr_auto_examples_neighbors_plot_classification.py`\n",
            " |      showing the impact of the `weights` parameter on the decision\n",
            " |      boundary.\n",
            " |\n",
            " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
            " |      Algorithm used to compute the nearest neighbors:\n",
            " |\n",
            " |      - 'ball_tree' will use :class:`BallTree`\n",
            " |      - 'kd_tree' will use :class:`KDTree`\n",
            " |      - 'brute' will use a brute-force search.\n",
            " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
            " |        based on the values passed to :meth:`fit` method.\n",
            " |\n",
            " |      Note: fitting on sparse input will override the setting of\n",
            " |      this parameter, using brute force.\n",
            " |\n",
            " |  leaf_size : int, default=30\n",
            " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
            " |      speed of the construction and query, as well as the memory\n",
            " |      required to store the tree.  The optimal value depends on the\n",
            " |      nature of the problem.\n",
            " |\n",
            " |  p : float, default=2\n",
            " |      Power parameter for the Minkowski metric. When p = 1, this is equivalent\n",
            " |      to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.\n",
            " |      For arbitrary p, minkowski_distance (l_p) is used. This parameter is expected\n",
            " |      to be positive.\n",
            " |\n",
            " |  metric : str or callable, default='minkowski'\n",
            " |      Metric to use for distance computation. Default is \"minkowski\", which\n",
            " |      results in the standard Euclidean distance when p = 2. See the\n",
            " |      documentation of `scipy.spatial.distance\n",
            " |      <https://docs.scipy.org/doc/scipy/reference/spatial.distance.html>`_ and\n",
            " |      the metrics listed in\n",
            " |      :class:`~sklearn.metrics.pairwise.distance_metrics` for valid metric\n",
            " |      values.\n",
            " |\n",
            " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
            " |      must be square during fit. X may be a :term:`sparse graph`, in which\n",
            " |      case only \"nonzero\" elements may be considered neighbors.\n",
            " |\n",
            " |      If metric is a callable function, it takes two arrays representing 1D\n",
            " |      vectors as inputs and must return one value indicating the distance\n",
            " |      between those vectors. This works for Scipy's metrics, but is less\n",
            " |      efficient than passing the metric name as a string.\n",
            " |\n",
            " |  metric_params : dict, default=None\n",
            " |      Additional keyword arguments for the metric function.\n",
            " |\n",
            " |  n_jobs : int, default=None\n",
            " |      The number of parallel jobs to run for neighbors search.\n",
            " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
            " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
            " |      for more details.\n",
            " |      Doesn't affect :meth:`fit` method.\n",
            " |\n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  classes_ : array of shape (n_classes,)\n",
            " |      Class labels known to the classifier\n",
            " |\n",
            " |  effective_metric_ : str or callble\n",
            " |      The distance metric used. It will be same as the `metric` parameter\n",
            " |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
            " |      'minkowski' and `p` parameter set to 2.\n",
            " |\n",
            " |  effective_metric_params_ : dict\n",
            " |      Additional keyword arguments for the metric function. For most metrics\n",
            " |      will be same with `metric_params` parameter, but may also contain the\n",
            " |      `p` parameter value if the `effective_metric_` attribute is set to\n",
            " |      'minkowski'.\n",
            " |\n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |\n",
            " |      .. versionadded:: 0.24\n",
            " |\n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |\n",
            " |      .. versionadded:: 1.0\n",
            " |\n",
            " |  n_samples_fit_ : int\n",
            " |      Number of samples in the fitted data.\n",
            " |\n",
            " |  outputs_2d_ : bool\n",
            " |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
            " |      otherwise True.\n",
            " |\n",
            " |  See Also\n",
            " |  --------\n",
            " |  RadiusNeighborsClassifier: Classifier based on neighbors within a fixed radius.\n",
            " |  KNeighborsRegressor: Regression based on k-nearest neighbors.\n",
            " |  RadiusNeighborsRegressor: Regression based on neighbors within a fixed radius.\n",
            " |  NearestNeighbors: Unsupervised learner for implementing neighbor searches.\n",
            " |\n",
            " |  Notes\n",
            " |  -----\n",
            " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
            " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
            " |\n",
            " |  .. warning::\n",
            " |\n",
            " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
            " |     neighbors, neighbor `k+1` and `k`, have identical distances\n",
            " |     but different labels, the results will depend on the ordering of the\n",
            " |     training data.\n",
            " |\n",
            " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
            " |\n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> X = [[0], [1], [2], [3]]\n",
            " |  >>> y = [0, 0, 1, 1]\n",
            " |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
            " |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
            " |  >>> neigh.fit(X, y)\n",
            " |  KNeighborsClassifier(...)\n",
            " |  >>> print(neigh.predict([[1.1]]))\n",
            " |  [0]\n",
            " |  >>> print(neigh.predict_proba([[0.9]]))\n",
            " |  [[0.666... 0.333...]]\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      KNeighborsClassifier\n",
            " |      sklearn.neighbors._base.KNeighborsMixin\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.neighbors._base.NeighborsBase\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            " |      sklearn.utils._metadata_requests._MetadataRequester\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  __sklearn_tags__(self)\n",
            " |\n",
            " |  fit(self, X, y)\n",
            " |      Fit the k-nearest neighbors classifier from the training dataset.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
            " |          Training data.\n",
            " |\n",
            " |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
            " |          Target values.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : KNeighborsClassifier\n",
            " |          The fitted k-nearest neighbors classifier.\n",
            " |\n",
            " |  predict(self, X)\n",
            " |      Predict the class labels for the provided data.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None\n",
            " |          Test samples. If `None`, predictions for all indexed points are\n",
            " |          returned; in this case, points are not considered their own\n",
            " |          neighbors.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
            " |          Class labels for each data sample.\n",
            " |\n",
            " |  predict_proba(self, X)\n",
            " |      Return probability estimates for the test data X.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed', or None\n",
            " |          Test samples. If `None`, predictions for all indexed points are\n",
            " |          returned; in this case, points are not considered their own\n",
            " |          neighbors.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      p : ndarray of shape (n_queries, n_classes), or a list of n_outputs                 of such arrays if n_outputs > 1.\n",
            " |          The class probabilities of the input samples. Classes are ordered\n",
            " |          by lexicographic order.\n",
            " |\n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |\n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features), or None\n",
            " |          Test samples. If `None`, predictions for all indexed points are\n",
            " |          used; in this case, points are not considered their own\n",
            " |          neighbors. This means that `knn.fit(X, y).score(None, y)`\n",
            " |          implicitly performs a leave-one-out cross-validation procedure\n",
            " |          and is equivalent to `cross_val_score(knn, X, y, cv=LeaveOneOut())`\n",
            " |          but typically much faster.\n",
            " |\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |\n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
            " |\n",
            " |  set_score_request(self: sklearn.neighbors._classification.KNeighborsClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.neighbors._classification.KNeighborsClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            " |      Request metadata passed to the ``score`` method.\n",
            " |\n",
            " |      Note that this method is only relevant if\n",
            " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |\n",
            " |      The options for each parameter are:\n",
            " |\n",
            " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
            " |\n",
            " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
            " |\n",
            " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            " |\n",
            " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            " |\n",
            " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            " |      existing request. This allows you to change the request for some\n",
            " |      parameters and not others.\n",
            " |\n",
            " |      .. versionadded:: 1.3\n",
            " |\n",
            " |      .. note::\n",
            " |          This method is only relevant if this estimator is used as a\n",
            " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          The updated object.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __abstractmethods__ = frozenset()\n",
            " |\n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
            " |\n",
            " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
            " |      Find the K-neighbors of a point.\n",
            " |\n",
            " |      Returns indices of and distances to the neighbors of each point.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix}, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n",
            " |          The query point or points.\n",
            " |          If not provided, neighbors of each indexed point are returned.\n",
            " |          In this case, the query point is not considered its own neighbor.\n",
            " |\n",
            " |      n_neighbors : int, default=None\n",
            " |          Number of neighbors required for each sample. The default is the\n",
            " |          value passed to the constructor.\n",
            " |\n",
            " |      return_distance : bool, default=True\n",
            " |          Whether or not to return the distances.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
            " |          Array representing the lengths to points, only present if\n",
            " |          return_distance=True.\n",
            " |\n",
            " |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
            " |          Indices of the nearest points in the population matrix.\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      In the following example, we construct a NearestNeighbors\n",
            " |      class from an array representing our data set and ask who's\n",
            " |      the closest point to [1,1,1]\n",
            " |\n",
            " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
            " |      >>> from sklearn.neighbors import NearestNeighbors\n",
            " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
            " |      >>> neigh.fit(samples)\n",
            " |      NearestNeighbors(n_neighbors=1)\n",
            " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
            " |      (array([[0.5]]), array([[2]]))\n",
            " |\n",
            " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
            " |      element is at distance 0.5 and is the third element of samples\n",
            " |      (indexes start at 0). You can also query for multiple points:\n",
            " |\n",
            " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
            " |      >>> neigh.kneighbors(X, return_distance=False)\n",
            " |      array([[1],\n",
            " |             [2]]...)\n",
            " |\n",
            " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
            " |      Compute the (weighted) graph of k-Neighbors for points in X.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed', default=None\n",
            " |          The query point or points.\n",
            " |          If not provided, neighbors of each indexed point are returned.\n",
            " |          In this case, the query point is not considered its own neighbor.\n",
            " |          For ``metric='precomputed'`` the shape should be\n",
            " |          (n_queries, n_indexed). Otherwise the shape should be\n",
            " |          (n_queries, n_features).\n",
            " |\n",
            " |      n_neighbors : int, default=None\n",
            " |          Number of neighbors for each sample. The default is the value\n",
            " |          passed to the constructor.\n",
            " |\n",
            " |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
            " |          Type of returned matrix: 'connectivity' will return the\n",
            " |          connectivity matrix with ones and zeros, in 'distance' the\n",
            " |          edges are distances between points, type of distance\n",
            " |          depends on the selected metric parameter in\n",
            " |          NearestNeighbors class.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
            " |          `n_samples_fit` is the number of samples in the fitted data.\n",
            " |          `A[i, j]` gives the weight of the edge connecting `i` to `j`.\n",
            " |          The matrix is of CSR format.\n",
            " |\n",
            " |      See Also\n",
            " |      --------\n",
            " |      NearestNeighbors.radius_neighbors_graph : Compute the (weighted) graph\n",
            " |          of Neighbors for points in X.\n",
            " |\n",
            " |      Examples\n",
            " |      --------\n",
            " |      >>> X = [[0], [3], [1]]\n",
            " |      >>> from sklearn.neighbors import NearestNeighbors\n",
            " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
            " |      >>> neigh.fit(X)\n",
            " |      NearestNeighbors(n_neighbors=2)\n",
            " |      >>> A = neigh.kneighbors_graph(X)\n",
            " |      >>> A.toarray()\n",
            " |      array([[1., 0., 1.],\n",
            " |             [0., 1., 1.],\n",
            " |             [1., 0., 1.]])\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |\n",
            " |  __getstate__(self)\n",
            " |      Helper for pickle.\n",
            " |\n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  __setstate__(self, state)\n",
            " |\n",
            " |  __sklearn_clone__(self)\n",
            " |\n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |\n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |\n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            " |\n",
            " |  get_metadata_routing(self)\n",
            " |      Get metadata routing of this object.\n",
            " |\n",
            " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      routing : MetadataRequest\n",
            " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            " |          routing information.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            " |\n",
            " |  __init_subclass__(**kwargs)\n",
            " |      Set the ``set_{method}_request`` methods.\n",
            " |\n",
            " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            " |      looks for the information available in the set default values which are\n",
            " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            " |      from method signatures.\n",
            " |\n",
            " |      The ``__metadata_request__*`` class attributes are used when a method\n",
            " |      does not explicitly accept a metadata through its arguments or if the\n",
            " |      developer would like to specify a request value for those metadata\n",
            " |      which are different from the default ``None``.\n",
            " |\n",
            " |      References\n",
            " |      ----------\n",
            " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaYaqKf3GYjl"
      },
      "source": [
        "## ニューラルネットワークモデル（多層パーセプトロン）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOL8V8LpGYjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5744a1a7-d7c4-4605-b76f-cb9d069ae97b"
      },
      "source": [
        "#多層パーセプトロンによる学習\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#ニューラルネットワークモデル（多層パーセプトロン）による宣言と学習\n",
        "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
        "mlp.fit(X_train_std,y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(mlp.score(X_train_std, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(mlp.score(X_test_std, y_test)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.7416\n",
            "正解率（test):  0.7090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2zBv1aFGYjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518c42ed-4b52-4b17-d25e-f3b522ef191a"
      },
      "source": [
        "#パーセプトロンによる学習\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "#ニューラルネットワークモデル（パーセプトロン）による宣言と学習\n",
        "per = Perceptron(tol=1e-3, random_state=0)\n",
        "per.fit(X_train_std, y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(per.score(X_train_std, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(per.score(X_test_std, y_test)))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.6244\n",
            "正解率（test):  0.6119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjVpST3JGYjn"
      },
      "source": [
        "## XGboost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "031BLb9mGYjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f4dcd8-a0a0-417f-b26e-741454ba7882"
      },
      "source": [
        "#Irisデータに対するXGBoostによる学習と計算結果\n",
        "import xgboost as xgb\n",
        "\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "xgb_model.fit(X_train_std, y_train)\n",
        "\n",
        "predict = xgb_model.predict(X_test_std)\n",
        "\n",
        "print('正解率： {:.4f}'.format(accuracy_score(predict,y_test)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率： 0.7201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7UGyFCFGYjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        },
        "outputId": "c3285cd1-ad32-4144-fe35-ea90ed1216e0"
      },
      "source": [
        "#別の書き方のXGBoosting．パラメータを設定．\n",
        "\n",
        "#データをXGBoostに使えるように処理\n",
        "xgb_train = xgb.DMatrix(X_train_std,label=y_train)\n",
        "xgb_test = xgb.DMatrix(X_test_std, label=y_test)\n",
        "\n",
        "#ハイパーパラメータ設定\n",
        "params = {\"objective\":\"multi:softmax\",\"num_class\":3,\"max_depth\":10,\"learning_rate\":0.1,'gamma': 10,'subsample': 0.8}\n",
        "\n",
        "#XGBoostの学習\n",
        "xgb_fit = xgb.train(params, xgb_train)\n",
        "\n",
        "#テストデータによる精度評価\n",
        "predict = xgb_fit.predict(xgb_test)\n",
        "print('正解率： {:.4f}'.format(accuracy_score(predict,y_test)))\n",
        "\n",
        "# 特徴量の重要度を表示\n",
        "xgb.plot_importance(xgb_fit)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率： 0.6679\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Feature importance'}, xlabel='Importance score', ylabel='Features'>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LN8pLHfXGYjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b910c8f4-ddf9-40ff-d119-ea639ffe96e8"
      },
      "source": [
        "help(xgb)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on package xgboost:\n",
            "\n",
            "NAME\n",
            "    xgboost - XGBoost: eXtreme Gradient Boosting library.\n",
            "\n",
            "DESCRIPTION\n",
            "    Contributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "    _data_utils\n",
            "    _typing\n",
            "    callback\n",
            "    collective\n",
            "    compat\n",
            "    config\n",
            "    core\n",
            "    dask (package)\n",
            "    data\n",
            "    federated\n",
            "    libpath\n",
            "    plotting\n",
            "    sklearn\n",
            "    spark (package)\n",
            "    testing (package)\n",
            "    tracker\n",
            "    training\n",
            "\n",
            "CLASSES\n",
            "    abc.ABC(builtins.object)\n",
            "        xgboost.core.DataIter\n",
            "    builtins.object\n",
            "        xgboost.core.Booster\n",
            "        xgboost.core.DMatrix\n",
            "            xgboost.core.ExtMemQuantileDMatrix(xgboost.core.DMatrix, xgboost.core._RefMixIn)\n",
            "            xgboost.core.QuantileDMatrix(xgboost.core.DMatrix, xgboost.core._RefMixIn)\n",
            "        xgboost.tracker.RabitTracker\n",
            "    sklearn.base.BaseEstimator(sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin, sklearn.utils._metadata_requests._MetadataRequester)\n",
            "        xgboost.sklearn.XGBModel\n",
            "            xgboost.sklearn.XGBClassifier(sklearn.base.ClassifierMixin, xgboost.sklearn.XGBModel)\n",
            "                xgboost.sklearn.XGBRFClassifier\n",
            "            xgboost.sklearn.XGBRanker(xgboost.sklearn.XGBRankerMixIn, xgboost.sklearn.XGBModel)\n",
            "            xgboost.sklearn.XGBRegressor(sklearn.base.RegressorMixin, xgboost.sklearn.XGBModel)\n",
            "                xgboost.sklearn.XGBRFRegressor\n",
            "    sklearn.base.ClassifierMixin(builtins.object)\n",
            "        xgboost.sklearn.XGBClassifier(sklearn.base.ClassifierMixin, xgboost.sklearn.XGBModel)\n",
            "            xgboost.sklearn.XGBRFClassifier\n",
            "    sklearn.base.RegressorMixin(builtins.object)\n",
            "        xgboost.sklearn.XGBRegressor(sklearn.base.RegressorMixin, xgboost.sklearn.XGBModel)\n",
            "            xgboost.sklearn.XGBRFRegressor\n",
            "    xgboost.sklearn.XGBRankerMixIn(builtins.object)\n",
            "        xgboost.sklearn.XGBRanker(xgboost.sklearn.XGBRankerMixIn, xgboost.sklearn.XGBModel)\n",
            "\n",
            "    class Booster(builtins.object)\n",
            "     |  Booster(params: Union[List, Dict[str, Any], NoneType] = None, cache: Optional[Sequence[xgboost.core.DMatrix]] = None, model_file: Union[ForwardRef('Booster'), bytearray, os.PathLike, str, NoneType] = None) -> None\n",
            "     |\n",
            "     |  A Booster of XGBoost.\n",
            "     |\n",
            "     |  Booster is the model of xgboost, that contains low level routines for\n",
            "     |  training, prediction and evaluation.\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __copy__(self) -> 'Booster'\n",
            "     |\n",
            "     |  __deepcopy__(self, _: Any) -> 'Booster'\n",
            "     |      Return a copy of booster.\n",
            "     |\n",
            "     |  __del__(self) -> None\n",
            "     |\n",
            "     |  __getitem__(self, val: Union[int, numpy.integer, tuple, slice, ellipsis]) -> 'Booster'\n",
            "     |      Get a slice of the tree-based model. Attributes like `best_iteration` and\n",
            "     |      `best_score` are removed in the resulting booster.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3.0\n",
            "     |\n",
            "     |  __getstate__(self) -> Dict\n",
            "     |      Helper for pickle.\n",
            "     |\n",
            "     |  __init__(self, params: Union[List, Dict[str, Any], NoneType] = None, cache: Optional[Sequence[xgboost.core.DMatrix]] = None, model_file: Union[ForwardRef('Booster'), bytearray, os.PathLike, str, NoneType] = None) -> None\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      params :\n",
            "     |          Parameters for boosters.\n",
            "     |      cache :\n",
            "     |          List of cache items.\n",
            "     |      model_file :\n",
            "     |          Path to the model file if it's string or PathLike.\n",
            "     |\n",
            "     |  __iter__(self) -> Generator[ForwardRef('Booster'), NoneType, NoneType]\n",
            "     |      Iterator method for getting individual trees.\n",
            "     |\n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |  __setstate__(self, state: Dict) -> None\n",
            "     |\n",
            "     |  attr(self, key: str) -> Optional[str]\n",
            "     |      Get attribute string from the Booster.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      key :\n",
            "     |          The key to get attribute from.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      value :\n",
            "     |          The attribute value of the key, returns None if attribute do not exist.\n",
            "     |\n",
            "     |  attributes(self) -> Dict[str, Optional[str]]\n",
            "     |      Get attributes stored in the Booster as a dictionary.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      result : dictionary of  attribute_name: attribute_value pairs of strings.\n",
            "     |          Returns an empty dict if there's no attributes.\n",
            "     |\n",
            "     |  boost(self, dtrain: xgboost.core.DMatrix, iteration: int, grad: Any, hess: Any) -> None\n",
            "     |      Boost the booster for one iteration with customized gradient statistics.\n",
            "     |      Like :py:func:`xgboost.Booster.update`, this function should not be called\n",
            "     |      directly by users.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      dtrain :\n",
            "     |          The training DMatrix.\n",
            "     |      grad :\n",
            "     |          The first order of gradient.\n",
            "     |      hess :\n",
            "     |          The second order of gradient.\n",
            "     |\n",
            "     |  copy(self) -> 'Booster'\n",
            "     |      Copy the booster object.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      booster :\n",
            "     |          A copied booster model\n",
            "     |\n",
            "     |  dump_model(self, fout: Union[str, os.PathLike], fmap: Union[str, os.PathLike] = '', with_stats: bool = False, dump_format: str = 'text') -> None\n",
            "     |      Dump model into a text or JSON file.  Unlike :py:meth:`save_model`, the\n",
            "     |      output format is primarily used for visualization or interpretation,\n",
            "     |      hence it's more human readable but cannot be loaded back to XGBoost.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fout :\n",
            "     |          Output file name.\n",
            "     |      fmap :\n",
            "     |          Name of the file containing feature map names.\n",
            "     |      with_stats :\n",
            "     |          Controls whether the split statistics are output.\n",
            "     |      dump_format :\n",
            "     |          Format of model dump file. Can be 'text' or 'json'.\n",
            "     |\n",
            "     |  eval(self, data: xgboost.core.DMatrix, name: str = 'eval', iteration: int = 0) -> str\n",
            "     |      Evaluate the model on mat.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data :\n",
            "     |          The dmatrix storing the input.\n",
            "     |\n",
            "     |      name :\n",
            "     |          The name of the dataset.\n",
            "     |\n",
            "     |      iteration :\n",
            "     |          The current iteration number.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      result: str\n",
            "     |          Evaluation result string.\n",
            "     |\n",
            "     |  eval_set(self, evals: Sequence[Tuple[xgboost.core.DMatrix, str]], iteration: int = 0, feval: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]]] = None, output_margin: bool = True) -> str\n",
            "     |      Evaluate a set of data.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      evals :\n",
            "     |          List of items to be evaluated.\n",
            "     |      iteration :\n",
            "     |          Current iteration.\n",
            "     |      feval :\n",
            "     |          Custom evaluation function.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      result: str\n",
            "     |          Evaluation result string.\n",
            "     |\n",
            "     |  get_categories(self, export_to_arrow: bool = False) -> xgboost._data_utils.Categories\n",
            "     |      Same method as :py:meth:`DMatrix.get_categories`.\n",
            "     |\n",
            "     |  get_dump(self, fmap: Union[str, os.PathLike] = '', with_stats: bool = False, dump_format: str = 'text') -> List[str]\n",
            "     |      Returns the model dump as a list of strings.  Unlike :py:meth:`save_model`,\n",
            "     |      the output format is primarily used for visualization or interpretation, hence\n",
            "     |      it's more human readable but cannot be loaded back to XGBoost.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fmap :\n",
            "     |          Name of the file containing feature map names.\n",
            "     |      with_stats :\n",
            "     |          Controls whether the split statistics should be included.\n",
            "     |      dump_format :\n",
            "     |          Format of model dump. Can be 'text', 'json' or 'dot'.\n",
            "     |\n",
            "     |  get_fscore(self, fmap: Union[str, os.PathLike] = '') -> Dict[str, Union[float, List[float]]]\n",
            "     |      Get feature importance of each feature.\n",
            "     |\n",
            "     |      .. note:: Zero-importance features will not be included\n",
            "     |\n",
            "     |         Keep in mind that this function does not include zero-importance feature,\n",
            "     |         i.e.  those features that have not been used in any split conditions.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fmap :\n",
            "     |         The name of feature map file\n",
            "     |\n",
            "     |  get_score(self, fmap: Union[str, os.PathLike] = '', importance_type: str = 'weight') -> Dict[str, Union[float, List[float]]]\n",
            "     |      Get feature importance of each feature.\n",
            "     |      For tree model Importance type can be defined as:\n",
            "     |\n",
            "     |      * 'weight': the number of times a feature is used to split the data across all\n",
            "     |         trees.\n",
            "     |      * 'gain': the average gain across all splits the feature is used in.\n",
            "     |      * 'cover': the average coverage across all splits the feature is used in.\n",
            "     |      * 'total_gain': the total gain across all splits the feature is used in.\n",
            "     |      * 'total_cover': the total coverage across all splits the feature is used in.\n",
            "     |\n",
            "     |      .. note::\n",
            "     |\n",
            "     |         For linear model, only \"weight\" is defined and it's the normalized\n",
            "     |         coefficients without bias.\n",
            "     |\n",
            "     |      .. note:: Zero-importance features will not be included\n",
            "     |\n",
            "     |         Keep in mind that this function does not include zero-importance feature,\n",
            "     |         i.e.  those features that have not been used in any split conditions.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fmap :\n",
            "     |         The name of feature map file.\n",
            "     |      importance_type :\n",
            "     |          One of the importance types defined above.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      A map between feature names and their scores.  When `gblinear` is used for\n",
            "     |      multi-class classification the scores for each feature is a list with length\n",
            "     |      `n_classes`, otherwise they're scalars.\n",
            "     |\n",
            "     |  get_split_value_histogram(self, feature: str, fmap: Union[str, os.PathLike] = '', bins: Optional[int] = None, as_pandas: bool = True) -> Union[numpy.ndarray, ForwardRef('PdDataFrame')]\n",
            "     |      Get split value histogram of a feature\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      feature :\n",
            "     |          The name of the feature.\n",
            "     |      fmap:\n",
            "     |          The name of feature map file.\n",
            "     |      bin :\n",
            "     |          The maximum number of bins.\n",
            "     |          Number of bins equals number of unique split values n_unique,\n",
            "     |          if bins == None or bins > n_unique.\n",
            "     |      as_pandas :\n",
            "     |          Return pd.DataFrame when pandas is installed.\n",
            "     |          If False or pandas is not installed, return numpy ndarray.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      a histogram of used splitting values for the specified feature\n",
            "     |      either as numpy array or pandas DataFrame.\n",
            "     |\n",
            "     |  inplace_predict(self, data: Any, *, iteration_range: Tuple[Union[int, numpy.integer], Union[int, numpy.integer]] = (0, 0), predict_type: str = 'value', missing: float = nan, validate_features: bool = True, base_margin: Any = None, strict_shape: bool = False) -> Any\n",
            "     |      Run prediction in-place when possible, Unlike :py:meth:`predict` method,\n",
            "     |      inplace prediction does not cache the prediction result.\n",
            "     |\n",
            "     |      Calling only ``inplace_predict`` in multiple threads is safe and lock\n",
            "     |      free.  But the safety does not hold when used in conjunction with other\n",
            "     |      methods. E.g. you can't train the booster in one thread and perform\n",
            "     |      prediction in the other.\n",
            "     |\n",
            "     |      .. note::\n",
            "     |\n",
            "     |          If the device ordinal of the input data doesn't match the one configured for\n",
            "     |          the booster, data will be copied to the booster device.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |          booster.set_param({\"device\": \"cuda:0\"})\n",
            "     |          booster.inplace_predict(cupy_array)\n",
            "     |\n",
            "     |          booster.set_param({\"device\": \"cpu\"})\n",
            "     |          booster.inplace_predict(numpy_array)\n",
            "     |\n",
            "     |      .. versionadded:: 1.1.0\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data :\n",
            "     |          The input data.\n",
            "     |      iteration_range :\n",
            "     |          See :py:meth:`predict` for details.\n",
            "     |      predict_type :\n",
            "     |          * `value` Output model prediction values.\n",
            "     |          * `margin` Output the raw untransformed margin value.\n",
            "     |      missing :\n",
            "     |          See :py:obj:`xgboost.DMatrix` for details.\n",
            "     |      validate_features:\n",
            "     |          See :py:meth:`xgboost.Booster.predict` for details.\n",
            "     |      base_margin:\n",
            "     |          See :py:obj:`xgboost.DMatrix` for details.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      strict_shape:\n",
            "     |          See :py:meth:`xgboost.Booster.predict` for details.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction : numpy.ndarray/cupy.ndarray\n",
            "     |          The prediction result.  When input data is on GPU, prediction result is\n",
            "     |          stored in a cupy array.\n",
            "     |\n",
            "     |  load_config(self, config: str) -> None\n",
            "     |      Load configuration returned by `save_config`.\n",
            "     |\n",
            "     |      .. versionadded:: 1.0.0\n",
            "     |\n",
            "     |  load_model(self, fname: Union[os.PathLike[~AnyStr], bytearray, str]) -> None\n",
            "     |      Load the model from a file or a bytearray.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        model.load_model(\"model.json\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |        model.load_model(\"model.ubj\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        buf = model.save_raw()\n",
            "     |        model.load_model(buf)\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Input file name or memory buffer(see also save_raw)\n",
            "     |\n",
            "     |  num_boosted_rounds(self) -> int\n",
            "     |      Get number of boosted rounds.  For gblinear this is reset to 0 after\n",
            "     |      serializing the model.\n",
            "     |\n",
            "     |  num_features(self) -> int\n",
            "     |      Number of features in booster.\n",
            "     |\n",
            "     |  predict(self, data: xgboost.core.DMatrix, *, output_margin: bool = False, pred_leaf: bool = False, pred_contribs: bool = False, approx_contribs: bool = False, pred_interactions: bool = False, validate_features: bool = True, training: bool = False, iteration_range: Tuple[Union[int, numpy.integer], Union[int, numpy.integer]] = (0, 0), strict_shape: bool = False) -> numpy.ndarray\n",
            "     |      Predict with data.  The full model will be used unless `iteration_range` is\n",
            "     |      specified, meaning user have to either slice the model or use the\n",
            "     |      ``best_iteration`` attribute to get prediction from best model returned from\n",
            "     |      early stopping.\n",
            "     |\n",
            "     |      .. note::\n",
            "     |\n",
            "     |          See :doc:`Prediction </prediction>` for issues like thread safety and a\n",
            "     |          summary of outputs from this function.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data :\n",
            "     |          The dmatrix storing the input.\n",
            "     |\n",
            "     |      output_margin :\n",
            "     |          Whether to output the raw untransformed margin value.\n",
            "     |\n",
            "     |      pred_leaf :\n",
            "     |          When this option is on, the output will be a matrix of (nsample,\n",
            "     |          ntrees) with each record indicating the predicted leaf index of\n",
            "     |          each sample in each tree.  Note that the leaf index of a tree is\n",
            "     |          unique per tree, so you may find leaf 1 in both tree 1 and tree 0.\n",
            "     |\n",
            "     |      pred_contribs :\n",
            "     |          When this is True the output will be a matrix of size (nsample,\n",
            "     |          nfeats + 1) with each record indicating the feature contributions\n",
            "     |          (SHAP values) for that prediction. The sum of all feature\n",
            "     |          contributions is equal to the raw untransformed margin value of the\n",
            "     |          prediction. Note the final column is the bias term.\n",
            "     |\n",
            "     |      approx_contribs :\n",
            "     |          Approximate the contributions of each feature.  Used when ``pred_contribs``\n",
            "     |          or ``pred_interactions`` is set to True.  Changing the default of this\n",
            "     |          parameter (False) is not recommended.\n",
            "     |\n",
            "     |      pred_interactions :\n",
            "     |          When this is True the output will be a matrix of size (nsample,\n",
            "     |          nfeats + 1, nfeats + 1) indicating the SHAP interaction values for\n",
            "     |          each pair of features. The sum of each row (or column) of the\n",
            "     |          interaction values equals the corresponding SHAP value (from\n",
            "     |          pred_contribs), and the sum of the entire matrix equals the raw\n",
            "     |          untransformed margin value of the prediction. Note the last row and\n",
            "     |          column correspond to the bias term.\n",
            "     |\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's\n",
            "     |          feature_names are identical.  Otherwise, it is assumed that the\n",
            "     |          feature_names are the same.\n",
            "     |\n",
            "     |      training :\n",
            "     |          Whether the prediction value is used for training.  This can effect `dart`\n",
            "     |          booster, which performs dropouts during training iterations but use all\n",
            "     |          trees for inference. If you want to obtain result with dropouts, set this\n",
            "     |          parameter to `True`.  Also, the parameter is set to true when obtaining\n",
            "     |          prediction for custom objective function.\n",
            "     |\n",
            "     |          .. versionadded:: 1.0.0\n",
            "     |\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
            "     |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
            "     |          used in this prediction.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      strict_shape :\n",
            "     |          When set to True, output shape is invariant to whether classification is\n",
            "     |          used.  For both value and margin prediction, the output shape is (n_samples,\n",
            "     |          n_groups), n_groups == 1 when multi-class is not used.  Default to False, in\n",
            "     |          which case the output shape can be (n_samples, ) if multi-class is not used.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction : numpy array\n",
            "     |\n",
            "     |  reset(self) -> 'Booster'\n",
            "     |      Reset the booster object to release data caches used for training.\n",
            "     |\n",
            "     |      .. versionadded:: 3.0.0\n",
            "     |\n",
            "     |  save_config(self) -> str\n",
            "     |      Output internal parameter configuration of Booster as a JSON\n",
            "     |      string.\n",
            "     |\n",
            "     |      .. versionadded:: 1.0.0\n",
            "     |\n",
            "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
            "     |      Save the model to a file.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Output file name\n",
            "     |\n",
            "     |  save_raw(self, raw_format: str = 'ubj') -> bytearray\n",
            "     |      Save the model to a in memory buffer representation instead of file.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      raw_format :\n",
            "     |          Format of output buffer. Can be `json` or `ubj`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      An in memory buffer representation of the model\n",
            "     |\n",
            "     |  set_attr(self, **kwargs: Optional[Any]) -> None\n",
            "     |      Set the attribute of the Booster.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      **kwargs\n",
            "     |          The attributes to set. Setting a value to None deletes an attribute.\n",
            "     |\n",
            "     |  set_param(self, params: Union[Dict, Iterable[Tuple[str, Any]], str], value: Optional[str] = None) -> None\n",
            "     |      Set parameters into the Booster.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      params :\n",
            "     |         list of key,value pairs, dict of key to value or simply str key\n",
            "     |      value :\n",
            "     |         value of the specified parameter, when params is str key\n",
            "     |\n",
            "     |  trees_to_dataframe(self, fmap: Union[str, os.PathLike] = '') -> 'PdDataFrame'\n",
            "     |      Parse a boosted tree model text dump into a pandas DataFrame structure.\n",
            "     |\n",
            "     |      This feature is only defined when the decision tree model is chosen as base\n",
            "     |      learner (`booster in {gbtree, dart}`). It is not defined for other base learner\n",
            "     |      types, such as linear learners (`booster=gblinear`).\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fmap :\n",
            "     |         The name of feature map file.\n",
            "     |\n",
            "     |  update(self, dtrain: xgboost.core.DMatrix, iteration: int, fobj: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[numpy.ndarray, numpy.ndarray]]] = None) -> None\n",
            "     |      Update for one iteration, with objective function calculated\n",
            "     |      internally.  This function should not be called directly by users.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      dtrain :\n",
            "     |          Training data.\n",
            "     |      iteration :\n",
            "     |          Current iteration number.\n",
            "     |      fobj :\n",
            "     |          Customized objective function.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  best_iteration\n",
            "     |      The best iteration during training.\n",
            "     |\n",
            "     |  best_score\n",
            "     |      The best evaluation score during training.\n",
            "     |\n",
            "     |  feature_names\n",
            "     |      Feature names for this booster.  Can be directly set by input data or by\n",
            "     |      assignment.\n",
            "     |\n",
            "     |  feature_types\n",
            "     |      Feature types for this booster.  Can be directly set by input data or by\n",
            "     |      assignment.  See :py:class:`DMatrix` for details.\n",
            "\n",
            "    class DMatrix(builtins.object)\n",
            "     |  DMatrix(data: Any, label: Optional[Any] = None, *, weight: Optional[Any] = None, base_margin: Optional[Any] = None, missing: Optional[float] = None, silent: bool = False, feature_names: Optional[Sequence[str]] = None, feature_types: Union[Sequence[str], xgboost._data_utils.Categories, NoneType] = None, nthread: Optional[int] = None, group: Optional[Any] = None, qid: Optional[Any] = None, label_lower_bound: Optional[Any] = None, label_upper_bound: Optional[Any] = None, feature_weights: Optional[Any] = None, enable_categorical: bool = False, data_split_mode: xgboost.core.DataSplitMode = <DataSplitMode.ROW: 0>) -> None\n",
            "     |\n",
            "     |  Data Matrix used in XGBoost.\n",
            "     |\n",
            "     |  DMatrix is an internal data structure that is used by XGBoost, which is optimized\n",
            "     |  for both memory efficiency and training speed.  You can construct DMatrix from\n",
            "     |  multiple different sources of data.\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __del__(self) -> None\n",
            "     |\n",
            "     |  __init__(self, data: Any, label: Optional[Any] = None, *, weight: Optional[Any] = None, base_margin: Optional[Any] = None, missing: Optional[float] = None, silent: bool = False, feature_names: Optional[Sequence[str]] = None, feature_types: Union[Sequence[str], xgboost._data_utils.Categories, NoneType] = None, nthread: Optional[int] = None, group: Optional[Any] = None, qid: Optional[Any] = None, label_lower_bound: Optional[Any] = None, label_upper_bound: Optional[Any] = None, feature_weights: Optional[Any] = None, enable_categorical: bool = False, data_split_mode: xgboost.core.DataSplitMode = <DataSplitMode.ROW: 0>) -> None\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data :\n",
            "     |          Data source of DMatrix. See :ref:`py-data` for a list of supported input\n",
            "     |          types.\n",
            "     |\n",
            "     |          Note that, if passing an iterator, it **will cache data on disk**, and note\n",
            "     |          that fields like ``label`` will be concatenated in-memory from multiple\n",
            "     |          calls to the iterator.\n",
            "     |      label :\n",
            "     |          Label of the training data.\n",
            "     |      weight :\n",
            "     |          Weight for each instance.\n",
            "     |\n",
            "     |           .. note::\n",
            "     |\n",
            "     |               For ranking task, weights are per-group.  In ranking task, one weight\n",
            "     |               is assigned to each group (not each data point). This is because we\n",
            "     |               only care about the relative ordering of data points within each group,\n",
            "     |               so it doesn't make sense to assign weights to individual data points.\n",
            "     |\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      missing :\n",
            "     |          Value in the input data which needs to be present as a missing value. If\n",
            "     |          None, defaults to np.nan.\n",
            "     |      silent :\n",
            "     |          Whether print messages during construction\n",
            "     |      feature_names :\n",
            "     |          Set names for features.\n",
            "     |      feature_types :\n",
            "     |\n",
            "     |          Set types for features. If `data` is a DataFrame type and passing\n",
            "     |          `enable_categorical=True`, the types will be deduced automatically from the\n",
            "     |          column types.\n",
            "     |\n",
            "     |          Otherwise, one can pass a list-like input with the same length as number of\n",
            "     |          columns in `data`, with the following possible values:\n",
            "     |\n",
            "     |          - \"c\", which represents categorical columns.\n",
            "     |          - \"q\", which represents numeric columns.\n",
            "     |          - \"int\", which represents integer columns.\n",
            "     |          - \"i\", which represents boolean columns.\n",
            "     |\n",
            "     |          Note that, while categorical types are treated differently from the rest for\n",
            "     |          model fitting purposes, the other types do not influence the generated\n",
            "     |          model, but have effects in other functionalities such as feature\n",
            "     |          importances.\n",
            "     |\n",
            "     |          For categorical features, the input is assumed to be preprocessed and\n",
            "     |          encoded by the users. The encoding can be done via\n",
            "     |          :py:class:`sklearn.preprocessing.OrdinalEncoder` or pandas dataframe\n",
            "     |          `.cat.codes` method. This is useful when users want to specify categorical\n",
            "     |          features without having to construct a dataframe as input.\n",
            "     |\n",
            "     |          .. versionadded:: 3.1.0\n",
            "     |\n",
            "     |          Alternatively, user can pass a :py:class:`~xgboost.core.Categories` object\n",
            "     |          returned from previous training as a reference for re-coding. One can obtain\n",
            "     |          the reference with the :py:meth:`.get_categories` from the previous training\n",
            "     |          DMatrix or the Booster. This feature is experimental.\n",
            "     |\n",
            "     |      nthread :\n",
            "     |          Number of threads to use for loading data when parallelization is\n",
            "     |          applicable. If -1, uses maximum threads available on the system.\n",
            "     |      group :\n",
            "     |          Group size for all ranking group.\n",
            "     |      qid :\n",
            "     |          Query ID for data samples, used for ranking.\n",
            "     |      label_lower_bound :\n",
            "     |          Lower bound for survival training.\n",
            "     |      label_upper_bound :\n",
            "     |          Upper bound for survival training.\n",
            "     |      feature_weights :\n",
            "     |          Set feature weights for column sampling.\n",
            "     |      enable_categorical :\n",
            "     |\n",
            "     |          .. versionadded:: 1.3.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          Experimental support of specializing for categorical features. See\n",
            "     |          :doc:`/tutorials/categorical` for more info.\n",
            "     |\n",
            "     |          If passing `True` and `data` is a data frame (from supported libraries such\n",
            "     |          as Pandas, Modin, polars, and cuDF), The DMatrix recognizes categorical\n",
            "     |          columns and automatically set the `feature_types` parameter. If `data` is\n",
            "     |          not a data frame, this argument is ignored.\n",
            "     |\n",
            "     |          If passing `False` and `data` is a data frame with categorical columns, it\n",
            "     |          will result in an error.\n",
            "     |\n",
            "     |          See notes in the :py:class:`DataIter` for consistency requirement when the\n",
            "     |          input is an iterator.\n",
            "     |\n",
            "     |          .. versionchanged:: 3.1.0\n",
            "     |\n",
            "     |          XGBoost can remember the encoding of categories when the input is a\n",
            "     |          dataframe.\n",
            "     |\n",
            "     |  data_split_mode(self) -> xgboost.core.DataSplitMode\n",
            "     |      Get the data split mode of the DMatrix.\n",
            "     |\n",
            "     |      .. versionadded:: 2.1.0\n",
            "     |\n",
            "     |  get_base_margin(self) -> numpy.ndarray\n",
            "     |      Get the base margin of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      base_margin\n",
            "     |\n",
            "     |  get_categories(self, export_to_arrow: bool = False) -> xgboost._data_utils.Categories\n",
            "     |      Get the categories in the dataset.\n",
            "     |\n",
            "     |      .. versionadded:: 3.1.0\n",
            "     |\n",
            "     |      .. warning::\n",
            "     |\n",
            "     |          This function is experimental.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      export_to_arrow :\n",
            "     |          The returned container will contain a list of ``pyarrow`` arrays for the\n",
            "     |          categories. See the :py:meth:`~Categories.to_arrow` for more info.\n",
            "     |\n",
            "     |  get_data(self) -> scipy.sparse._csr.csr_matrix\n",
            "     |      Get the predictors from DMatrix as a CSR matrix. This getter is mostly for\n",
            "     |      testing purposes. If this is a quantized DMatrix then quantized values are\n",
            "     |      returned instead of input values.\n",
            "     |\n",
            "     |      .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |  get_float_info(self, field: str) -> numpy.ndarray\n",
            "     |      Get float property from the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      info : array\n",
            "     |          a numpy array of float information of the data\n",
            "     |\n",
            "     |  get_group(self) -> numpy.ndarray\n",
            "     |      Get the group of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      group\n",
            "     |\n",
            "     |  get_label(self) -> numpy.ndarray\n",
            "     |      Get the label of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      label : array\n",
            "     |\n",
            "     |  get_quantile_cut(self) -> Tuple[numpy.ndarray, numpy.ndarray]\n",
            "     |      Get quantile cuts for quantization.\n",
            "     |\n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |  get_uint_info(self, field: str) -> numpy.ndarray\n",
            "     |      Get unsigned integer property from the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      info : array\n",
            "     |          a numpy array of unsigned integer information of the data\n",
            "     |\n",
            "     |  get_weight(self) -> numpy.ndarray\n",
            "     |      Get the weight of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      weight : array\n",
            "     |\n",
            "     |  num_col(self) -> int\n",
            "     |      Get the number of columns (features) in the DMatrix.\n",
            "     |\n",
            "     |  num_nonmissing(self) -> int\n",
            "     |      Get the number of non-missing values in the DMatrix.\n",
            "     |\n",
            "     |      .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |  num_row(self) -> int\n",
            "     |      Get the number of rows in the DMatrix.\n",
            "     |\n",
            "     |  save_binary(self, fname: Union[str, os.PathLike], silent: bool = True) -> None\n",
            "     |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
            "     |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname : string or os.PathLike\n",
            "     |          Name of the output buffer file.\n",
            "     |      silent : bool (optional; default: True)\n",
            "     |          If set, the output is suppressed.\n",
            "     |\n",
            "     |  set_base_margin(self, margin: Any) -> None\n",
            "     |      Set base margin of booster to start from.\n",
            "     |\n",
            "     |      This can be used to specify a prediction value of existing model to be\n",
            "     |      base_margin However, remember margin is needed, instead of transformed\n",
            "     |      prediction e.g. for logistic regression: need to put in value before\n",
            "     |      logistic transformation see also example/demo.py\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      margin: array like\n",
            "     |          Prediction margin of each datapoint\n",
            "     |\n",
            "     |  set_float_info(self, field: str, data: Any) -> None\n",
            "     |      Set float type property into the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_float_info_npy2d(self, field: str, data: Any) -> None\n",
            "     |      Set float type property into the DMatrix\n",
            "     |         for numpy 2d array input\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_group(self, group: Any) -> None\n",
            "     |      Set group size of DMatrix (used for ranking).\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      group : array like\n",
            "     |          Group size of each group\n",
            "     |\n",
            "     |  set_info(self, *, label: Optional[Any] = None, weight: Optional[Any] = None, base_margin: Optional[Any] = None, group: Optional[Any] = None, qid: Optional[Any] = None, label_lower_bound: Optional[Any] = None, label_upper_bound: Optional[Any] = None, feature_names: Optional[Sequence[str]] = None, feature_types: Optional[Sequence[str]] = None, feature_weights: Optional[Any] = None) -> None\n",
            "     |      Set meta info for DMatrix.  See doc string for :py:obj:`xgboost.DMatrix`.\n",
            "     |\n",
            "     |  set_label(self, label: Any) -> None\n",
            "     |      Set label of dmatrix\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      label: array like\n",
            "     |          The label information to be set into DMatrix\n",
            "     |\n",
            "     |  set_uint_info(self, field: str, data: Any) -> None\n",
            "     |      Set uint type property into the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_weight(self, weight: Any) -> None\n",
            "     |      Set weight of each instance.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      weight : array like\n",
            "     |          Weight for each data point\n",
            "     |\n",
            "     |          .. note:: For ranking task, weights are per-group.\n",
            "     |\n",
            "     |              In ranking task, one weight is assigned to each group (not each\n",
            "     |              data point). This is because we only care about the relative\n",
            "     |              ordering of data points within each group, so it doesn't make\n",
            "     |              sense to assign weights to individual data points.\n",
            "     |\n",
            "     |  slice(self, rindex: Union[List[int], numpy.ndarray], allow_groups: bool = False) -> 'DMatrix'\n",
            "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      rindex\n",
            "     |          List of indices to be selected.\n",
            "     |      allow_groups\n",
            "     |          Allow slicing of a matrix with a groups attribute\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      res\n",
            "     |          A new DMatrix containing only selected indices.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  feature_names\n",
            "     |      Labels for features (column labels).\n",
            "     |\n",
            "     |      Setting it to ``None`` resets existing feature names.\n",
            "     |\n",
            "     |  feature_types\n",
            "     |      Type of features (column types).\n",
            "     |\n",
            "     |      This is for displaying the results and categorical data support. See\n",
            "     |      :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      Setting it to ``None`` resets existing feature types.\n",
            "\n",
            "    class DataIter(abc.ABC)\n",
            "     |  DataIter(cache_prefix: Optional[str] = None, release_data: bool = True, *, on_host: bool = True, min_cache_page_bytes: Optional[int] = None) -> None\n",
            "     |\n",
            "     |  The interface for user defined data iterator. The iterator facilitates\n",
            "     |  distributed training, :py:class:`QuantileDMatrix`, and external memory support using\n",
            "     |  :py:class:`DMatrix` or :py:class:`ExtMemQuantileDMatrix`. Most of time, users don't\n",
            "     |  need to interact with this class directly.\n",
            "     |\n",
            "     |  .. note::\n",
            "     |\n",
            "     |      The class caches some intermediate results using the `data` input (predictor\n",
            "     |      `X`) as key. Don't repeat the `X` for multiple batches with different meta data\n",
            "     |      (like `label`), make a copy if necessary.\n",
            "     |\n",
            "     |  .. note::\n",
            "     |\n",
            "     |      When the input for each batch is a DataFrame, we assume categories are\n",
            "     |      consistently encoded for all batches. For example, given two dataframes for two\n",
            "     |      batches, this is invalid:\n",
            "     |\n",
            "     |      .. code-block::\n",
            "     |\n",
            "     |          import pandas as pd\n",
            "     |\n",
            "     |          x0 = pd.DataFrame({\"a\": [0, 1]}, dtype=\"category\")\n",
            "     |          x1 = pd.DataFrame({\"a\": [1, 2]}, dtype=\"category\")\n",
            "     |\n",
            "     |      This is invalid because the `x0` has `[0, 1]` as categories while `x2` has `[1,\n",
            "     |      2]`. They should share the same set of categories and encoding:\n",
            "     |\n",
            "     |      .. code-block::\n",
            "     |\n",
            "     |          import numpy as np\n",
            "     |\n",
            "     |          categories = np.array([0, 1, 2])\n",
            "     |          x0[\"a\"] = pd.Categorical.from_codes(\n",
            "     |              codes=np.array([0, 1]), categories=categories\n",
            "     |          )\n",
            "     |          x1[\"a\"] = pd.Categorical.from_codes(\n",
            "     |              codes=np.array([1, 2]), categories=categories\n",
            "     |          )\n",
            "     |\n",
            "     |      You can make sure the consistent encoding in your preprocessing step be careful\n",
            "     |      that the data is stored in formats that preserve the encoding when chunking the\n",
            "     |      data.\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  cache_prefix :\n",
            "     |      Prefix to the cache files, only used in external memory.\n",
            "     |\n",
            "     |      Note that using this class for external memory **will cache data\n",
            "     |      on disk** under the path passed here.\n",
            "     |\n",
            "     |  release_data :\n",
            "     |      Whether the iterator should release the data during iteration. Set it to True if\n",
            "     |      the data transformation (converting data to np.float32 type) is memory\n",
            "     |      intensive. Otherwise, if the transformation is computation intensive then we can\n",
            "     |      keep the cache.\n",
            "     |\n",
            "     |  on_host :\n",
            "     |      Whether the data should be cached on the host memory instead of the file system\n",
            "     |      when using GPU with external memory. When set to true (the default), the\n",
            "     |      \"external memory\" is the CPU (host) memory. See\n",
            "     |      :doc:`/tutorials/external_memory` for more info.\n",
            "     |\n",
            "     |      .. versionadded:: 3.0.0\n",
            "     |\n",
            "     |      .. warning::\n",
            "     |\n",
            "     |          This is an experimental parameter and subject to change.\n",
            "     |\n",
            "     |  min_cache_page_bytes :\n",
            "     |      The minimum number of bytes of each cached pages. Only used for on-host cache\n",
            "     |      with GPU-based :py:class:`ExtMemQuantileDMatrix`. When using GPU-based external\n",
            "     |      memory with the data cached in the host memory, XGBoost can concatenate the\n",
            "     |      pages internally to increase the batch size for the GPU. The default page size\n",
            "     |      is about 1/16 of the total device memory. Users can manually set the value based\n",
            "     |      on the actual hardware and datasets. Set this to 0 to disable page\n",
            "     |      concatenation.\n",
            "     |\n",
            "     |      .. versionadded:: 3.0.0\n",
            "     |\n",
            "     |      .. warning::\n",
            "     |\n",
            "     |          This is an experimental parameter and subject to change.\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      DataIter\n",
            "     |      abc.ABC\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __del__(self) -> None\n",
            "     |\n",
            "     |  __init__(self, cache_prefix: Optional[str] = None, release_data: bool = True, *, on_host: bool = True, min_cache_page_bytes: Optional[int] = None) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |\n",
            "     |  get_callbacks(self, enable_categorical: bool) -> Tuple[Callable, Callable]\n",
            "     |      Get callback functions for iterating in C. This is an internal function.\n",
            "     |\n",
            "     |  next(self, input_data: Callable) -> bool\n",
            "     |      Set the next batch of data.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |\n",
            "     |      input_data:\n",
            "     |          A function with same data fields like `data`, `label` with\n",
            "     |          `xgboost.DMatrix`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      False if there's no more batch, otherwise True.\n",
            "     |\n",
            "     |  reraise(self) -> None\n",
            "     |      Reraise the exception thrown during iteration.\n",
            "     |\n",
            "     |  reset(self) -> None\n",
            "     |      Reset the data iterator.  Prototype for user defined function.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties defined here:\n",
            "     |\n",
            "     |  proxy\n",
            "     |      Handle of DMatrix proxy.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |\n",
            "     |  __abstractmethods__ = frozenset({'next', 'reset'})\n",
            "\n",
            "    class ExtMemQuantileDMatrix(DMatrix, _RefMixIn)\n",
            "     |  ExtMemQuantileDMatrix(data: xgboost.core.DataIter, *, missing: Optional[float] = None, nthread: Optional[int] = None, max_bin: Optional[int] = None, ref: Optional[xgboost.core.DMatrix] = None, enable_categorical: bool = False, max_quantile_batches: Optional[int] = None, cache_host_ratio: Optional[float] = None) -> None\n",
            "     |\n",
            "     |  The external memory version of the :py:class:`QuantileDMatrix`.\n",
            "     |\n",
            "     |  See :doc:`/tutorials/external_memory` for explanation and usage examples, and\n",
            "     |  :py:class:`QuantileDMatrix` for parameter document.\n",
            "     |\n",
            "     |  .. warning::\n",
            "     |\n",
            "     |      This is an experimental feature and subject to change.\n",
            "     |\n",
            "     |  .. versionadded:: 3.0.0\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      ExtMemQuantileDMatrix\n",
            "     |      DMatrix\n",
            "     |      _RefMixIn\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __init__(self, data: xgboost.core.DataIter, *, missing: Optional[float] = None, nthread: Optional[int] = None, max_bin: Optional[int] = None, ref: Optional[xgboost.core.DMatrix] = None, enable_categorical: bool = False, max_quantile_batches: Optional[int] = None, cache_host_ratio: Optional[float] = None) -> None\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data :\n",
            "     |          A user-defined :py:class:`DataIter` for loading data.\n",
            "     |\n",
            "     |      max_quantile_batches :\n",
            "     |          See :py:class:`QuantileDMatrix`.\n",
            "     |\n",
            "     |      cache_host_ratio :\n",
            "     |\n",
            "     |          .. versionadded:: 3.1.0\n",
            "     |\n",
            "     |          Used by the GPU implementation. For GPU-based inputs, XGBoost can split the\n",
            "     |          cache into host and device caches to reduce the data transfer overhead. This\n",
            "     |          parameter specifies the size of host cache compared to the size of the\n",
            "     |          entire cache: :math:`host / (host + device)`.\n",
            "     |\n",
            "     |          See :ref:`extmem-adaptive-cache` for more info.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from DMatrix:\n",
            "     |\n",
            "     |  __del__(self) -> None\n",
            "     |\n",
            "     |  data_split_mode(self) -> xgboost.core.DataSplitMode\n",
            "     |      Get the data split mode of the DMatrix.\n",
            "     |\n",
            "     |      .. versionadded:: 2.1.0\n",
            "     |\n",
            "     |  get_base_margin(self) -> numpy.ndarray\n",
            "     |      Get the base margin of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      base_margin\n",
            "     |\n",
            "     |  get_categories(self, export_to_arrow: bool = False) -> xgboost._data_utils.Categories\n",
            "     |      Get the categories in the dataset.\n",
            "     |\n",
            "     |      .. versionadded:: 3.1.0\n",
            "     |\n",
            "     |      .. warning::\n",
            "     |\n",
            "     |          This function is experimental.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      export_to_arrow :\n",
            "     |          The returned container will contain a list of ``pyarrow`` arrays for the\n",
            "     |          categories. See the :py:meth:`~Categories.to_arrow` for more info.\n",
            "     |\n",
            "     |  get_data(self) -> scipy.sparse._csr.csr_matrix\n",
            "     |      Get the predictors from DMatrix as a CSR matrix. This getter is mostly for\n",
            "     |      testing purposes. If this is a quantized DMatrix then quantized values are\n",
            "     |      returned instead of input values.\n",
            "     |\n",
            "     |      .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |  get_float_info(self, field: str) -> numpy.ndarray\n",
            "     |      Get float property from the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      info : array\n",
            "     |          a numpy array of float information of the data\n",
            "     |\n",
            "     |  get_group(self) -> numpy.ndarray\n",
            "     |      Get the group of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      group\n",
            "     |\n",
            "     |  get_label(self) -> numpy.ndarray\n",
            "     |      Get the label of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      label : array\n",
            "     |\n",
            "     |  get_quantile_cut(self) -> Tuple[numpy.ndarray, numpy.ndarray]\n",
            "     |      Get quantile cuts for quantization.\n",
            "     |\n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |  get_uint_info(self, field: str) -> numpy.ndarray\n",
            "     |      Get unsigned integer property from the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      info : array\n",
            "     |          a numpy array of unsigned integer information of the data\n",
            "     |\n",
            "     |  get_weight(self) -> numpy.ndarray\n",
            "     |      Get the weight of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      weight : array\n",
            "     |\n",
            "     |  num_col(self) -> int\n",
            "     |      Get the number of columns (features) in the DMatrix.\n",
            "     |\n",
            "     |  num_nonmissing(self) -> int\n",
            "     |      Get the number of non-missing values in the DMatrix.\n",
            "     |\n",
            "     |      .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |  num_row(self) -> int\n",
            "     |      Get the number of rows in the DMatrix.\n",
            "     |\n",
            "     |  save_binary(self, fname: Union[str, os.PathLike], silent: bool = True) -> None\n",
            "     |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
            "     |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname : string or os.PathLike\n",
            "     |          Name of the output buffer file.\n",
            "     |      silent : bool (optional; default: True)\n",
            "     |          If set, the output is suppressed.\n",
            "     |\n",
            "     |  set_base_margin(self, margin: Any) -> None\n",
            "     |      Set base margin of booster to start from.\n",
            "     |\n",
            "     |      This can be used to specify a prediction value of existing model to be\n",
            "     |      base_margin However, remember margin is needed, instead of transformed\n",
            "     |      prediction e.g. for logistic regression: need to put in value before\n",
            "     |      logistic transformation see also example/demo.py\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      margin: array like\n",
            "     |          Prediction margin of each datapoint\n",
            "     |\n",
            "     |  set_float_info(self, field: str, data: Any) -> None\n",
            "     |      Set float type property into the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_float_info_npy2d(self, field: str, data: Any) -> None\n",
            "     |      Set float type property into the DMatrix\n",
            "     |         for numpy 2d array input\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_group(self, group: Any) -> None\n",
            "     |      Set group size of DMatrix (used for ranking).\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      group : array like\n",
            "     |          Group size of each group\n",
            "     |\n",
            "     |  set_info(self, *, label: Optional[Any] = None, weight: Optional[Any] = None, base_margin: Optional[Any] = None, group: Optional[Any] = None, qid: Optional[Any] = None, label_lower_bound: Optional[Any] = None, label_upper_bound: Optional[Any] = None, feature_names: Optional[Sequence[str]] = None, feature_types: Optional[Sequence[str]] = None, feature_weights: Optional[Any] = None) -> None\n",
            "     |      Set meta info for DMatrix.  See doc string for :py:obj:`xgboost.DMatrix`.\n",
            "     |\n",
            "     |  set_label(self, label: Any) -> None\n",
            "     |      Set label of dmatrix\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      label: array like\n",
            "     |          The label information to be set into DMatrix\n",
            "     |\n",
            "     |  set_uint_info(self, field: str, data: Any) -> None\n",
            "     |      Set uint type property into the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_weight(self, weight: Any) -> None\n",
            "     |      Set weight of each instance.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      weight : array like\n",
            "     |          Weight for each data point\n",
            "     |\n",
            "     |          .. note:: For ranking task, weights are per-group.\n",
            "     |\n",
            "     |              In ranking task, one weight is assigned to each group (not each\n",
            "     |              data point). This is because we only care about the relative\n",
            "     |              ordering of data points within each group, so it doesn't make\n",
            "     |              sense to assign weights to individual data points.\n",
            "     |\n",
            "     |  slice(self, rindex: Union[List[int], numpy.ndarray], allow_groups: bool = False) -> 'DMatrix'\n",
            "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      rindex\n",
            "     |          List of indices to be selected.\n",
            "     |      allow_groups\n",
            "     |          Allow slicing of a matrix with a groups attribute\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      res\n",
            "     |          A new DMatrix containing only selected indices.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from DMatrix:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  feature_names\n",
            "     |      Labels for features (column labels).\n",
            "     |\n",
            "     |      Setting it to ``None`` resets existing feature names.\n",
            "     |\n",
            "     |  feature_types\n",
            "     |      Type of features (column types).\n",
            "     |\n",
            "     |      This is for displaying the results and categorical data support. See\n",
            "     |      :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      Setting it to ``None`` resets existing feature types.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from _RefMixIn:\n",
            "     |\n",
            "     |  ref\n",
            "     |      Internal method for retrieving a reference to the training DMatrix.\n",
            "\n",
            "    class QuantileDMatrix(DMatrix, _RefMixIn)\n",
            "     |  QuantileDMatrix(data: Any, label: Optional[Any] = None, *, weight: Optional[Any] = None, base_margin: Optional[Any] = None, missing: Optional[float] = None, silent: bool = False, feature_names: Optional[Sequence[str]] = None, feature_types: Optional[Sequence[str]] = None, nthread: Optional[int] = None, max_bin: Optional[int] = None, ref: Optional[xgboost.core.DMatrix] = None, group: Optional[Any] = None, qid: Optional[Any] = None, label_lower_bound: Optional[Any] = None, label_upper_bound: Optional[Any] = None, feature_weights: Optional[Any] = None, enable_categorical: bool = False, max_quantile_batches: Optional[int] = None, data_split_mode: xgboost.core.DataSplitMode = <DataSplitMode.ROW: 0>) -> None\n",
            "     |\n",
            "     |  A DMatrix variant that generates quantilized data directly from input for the\n",
            "     |  ``hist`` tree method. This DMatrix is primarily designed to save memory in training\n",
            "     |  by avoiding intermediate storage. Set ``max_bin`` to control the number of bins\n",
            "     |  during quantisation, which should be consistent with the training parameter\n",
            "     |  ``max_bin``. When ``QuantileDMatrix`` is used for validation/test dataset, ``ref``\n",
            "     |  should be another ``QuantileDMatrix`` or ``DMatrix``, but not recommended as it\n",
            "     |  defeats the purpose of saving memory) constructed from training dataset.  See\n",
            "     |  :py:obj:`xgboost.DMatrix` for documents on meta info.\n",
            "     |\n",
            "     |  .. note::\n",
            "     |\n",
            "     |      Do not use ``QuantileDMatrix`` as validation/test dataset without supplying a\n",
            "     |      reference (the training dataset) ``QuantileDMatrix`` using ``ref`` as some\n",
            "     |      information may be lost in quantisation.\n",
            "     |\n",
            "     |  .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |\n",
            "     |  .. code-block::\n",
            "     |\n",
            "     |      from sklearn.datasets import make_regression\n",
            "     |      from sklearn.model_selection import train_test_split\n",
            "     |\n",
            "     |      X, y = make_regression()\n",
            "     |      X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
            "     |      Xy_train = xgb.QuantileDMatrix(X_train, y_train)\n",
            "     |      # It's necessary to have the training DMatrix as a reference for valid\n",
            "     |      # quantiles.\n",
            "     |      Xy_test = xgb.QuantileDMatrix(X_test, y_test, ref=Xy_train)\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |  max_bin :\n",
            "     |      The number of histogram bin, should be consistent with the training parameter\n",
            "     |      ``max_bin``.\n",
            "     |\n",
            "     |  ref :\n",
            "     |      The training dataset that provides quantile information, needed when creating\n",
            "     |      validation/test dataset with ``QuantileDMatrix``. Supplying the training DMatrix\n",
            "     |      as a reference means that the same quantisation applied to the training data is\n",
            "     |      applied to the validation/test data\n",
            "     |\n",
            "     |  max_quantile_batches :\n",
            "     |      For GPU-based inputs from an iterator, XGBoost handles incoming batches with\n",
            "     |      multiple growing sub-streams. This parameter sets the maximum number of batches\n",
            "     |      before XGBoost can cut a sub-stream and create a new one. This can help bound\n",
            "     |      the memory usage. By default, XGBoost grows a sub-stream exponentially until\n",
            "     |      batches are exhausted. This option is only used for the training dataset and the\n",
            "     |      default is None (unbounded). Lastly, if the `data` is a single batch instead of\n",
            "     |      an iterator, this parameter has no effect.\n",
            "     |\n",
            "     |      .. versionadded:: 3.0.0\n",
            "     |\n",
            "     |      .. warning::\n",
            "     |\n",
            "     |          This is an experimental parameter and subject to change.\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      QuantileDMatrix\n",
            "     |      DMatrix\n",
            "     |      _RefMixIn\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __init__(self, data: Any, label: Optional[Any] = None, *, weight: Optional[Any] = None, base_margin: Optional[Any] = None, missing: Optional[float] = None, silent: bool = False, feature_names: Optional[Sequence[str]] = None, feature_types: Optional[Sequence[str]] = None, nthread: Optional[int] = None, max_bin: Optional[int] = None, ref: Optional[xgboost.core.DMatrix] = None, group: Optional[Any] = None, qid: Optional[Any] = None, label_lower_bound: Optional[Any] = None, label_upper_bound: Optional[Any] = None, feature_weights: Optional[Any] = None, enable_categorical: bool = False, max_quantile_batches: Optional[int] = None, data_split_mode: xgboost.core.DataSplitMode = <DataSplitMode.ROW: 0>) -> None\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      data :\n",
            "     |          Data source of DMatrix. See :ref:`py-data` for a list of supported input\n",
            "     |          types.\n",
            "     |\n",
            "     |          Note that, if passing an iterator, it **will cache data on disk**, and note\n",
            "     |          that fields like ``label`` will be concatenated in-memory from multiple\n",
            "     |          calls to the iterator.\n",
            "     |      label :\n",
            "     |          Label of the training data.\n",
            "     |      weight :\n",
            "     |          Weight for each instance.\n",
            "     |\n",
            "     |           .. note::\n",
            "     |\n",
            "     |               For ranking task, weights are per-group.  In ranking task, one weight\n",
            "     |               is assigned to each group (not each data point). This is because we\n",
            "     |               only care about the relative ordering of data points within each group,\n",
            "     |               so it doesn't make sense to assign weights to individual data points.\n",
            "     |\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      missing :\n",
            "     |          Value in the input data which needs to be present as a missing value. If\n",
            "     |          None, defaults to np.nan.\n",
            "     |      silent :\n",
            "     |          Whether print messages during construction\n",
            "     |      feature_names :\n",
            "     |          Set names for features.\n",
            "     |      feature_types :\n",
            "     |\n",
            "     |          Set types for features. If `data` is a DataFrame type and passing\n",
            "     |          `enable_categorical=True`, the types will be deduced automatically from the\n",
            "     |          column types.\n",
            "     |\n",
            "     |          Otherwise, one can pass a list-like input with the same length as number of\n",
            "     |          columns in `data`, with the following possible values:\n",
            "     |\n",
            "     |          - \"c\", which represents categorical columns.\n",
            "     |          - \"q\", which represents numeric columns.\n",
            "     |          - \"int\", which represents integer columns.\n",
            "     |          - \"i\", which represents boolean columns.\n",
            "     |\n",
            "     |          Note that, while categorical types are treated differently from the rest for\n",
            "     |          model fitting purposes, the other types do not influence the generated\n",
            "     |          model, but have effects in other functionalities such as feature\n",
            "     |          importances.\n",
            "     |\n",
            "     |          For categorical features, the input is assumed to be preprocessed and\n",
            "     |          encoded by the users. The encoding can be done via\n",
            "     |          :py:class:`sklearn.preprocessing.OrdinalEncoder` or pandas dataframe\n",
            "     |          `.cat.codes` method. This is useful when users want to specify categorical\n",
            "     |          features without having to construct a dataframe as input.\n",
            "     |\n",
            "     |          .. versionadded:: 3.1.0\n",
            "     |\n",
            "     |          Alternatively, user can pass a :py:class:`~xgboost.core.Categories` object\n",
            "     |          returned from previous training as a reference for re-coding. One can obtain\n",
            "     |          the reference with the :py:meth:`.get_categories` from the previous training\n",
            "     |          DMatrix or the Booster. This feature is experimental.\n",
            "     |\n",
            "     |      nthread :\n",
            "     |          Number of threads to use for loading data when parallelization is\n",
            "     |          applicable. If -1, uses maximum threads available on the system.\n",
            "     |      group :\n",
            "     |          Group size for all ranking group.\n",
            "     |      qid :\n",
            "     |          Query ID for data samples, used for ranking.\n",
            "     |      label_lower_bound :\n",
            "     |          Lower bound for survival training.\n",
            "     |      label_upper_bound :\n",
            "     |          Upper bound for survival training.\n",
            "     |      feature_weights :\n",
            "     |          Set feature weights for column sampling.\n",
            "     |      enable_categorical :\n",
            "     |\n",
            "     |          .. versionadded:: 1.3.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          Experimental support of specializing for categorical features. See\n",
            "     |          :doc:`/tutorials/categorical` for more info.\n",
            "     |\n",
            "     |          If passing `True` and `data` is a data frame (from supported libraries such\n",
            "     |          as Pandas, Modin, polars, and cuDF), The DMatrix recognizes categorical\n",
            "     |          columns and automatically set the `feature_types` parameter. If `data` is\n",
            "     |          not a data frame, this argument is ignored.\n",
            "     |\n",
            "     |          If passing `False` and `data` is a data frame with categorical columns, it\n",
            "     |          will result in an error.\n",
            "     |\n",
            "     |          See notes in the :py:class:`DataIter` for consistency requirement when the\n",
            "     |          input is an iterator.\n",
            "     |\n",
            "     |          .. versionchanged:: 3.1.0\n",
            "     |\n",
            "     |          XGBoost can remember the encoding of categories when the input is a\n",
            "     |          dataframe.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from DMatrix:\n",
            "     |\n",
            "     |  __del__(self) -> None\n",
            "     |\n",
            "     |  data_split_mode(self) -> xgboost.core.DataSplitMode\n",
            "     |      Get the data split mode of the DMatrix.\n",
            "     |\n",
            "     |      .. versionadded:: 2.1.0\n",
            "     |\n",
            "     |  get_base_margin(self) -> numpy.ndarray\n",
            "     |      Get the base margin of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      base_margin\n",
            "     |\n",
            "     |  get_categories(self, export_to_arrow: bool = False) -> xgboost._data_utils.Categories\n",
            "     |      Get the categories in the dataset.\n",
            "     |\n",
            "     |      .. versionadded:: 3.1.0\n",
            "     |\n",
            "     |      .. warning::\n",
            "     |\n",
            "     |          This function is experimental.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      export_to_arrow :\n",
            "     |          The returned container will contain a list of ``pyarrow`` arrays for the\n",
            "     |          categories. See the :py:meth:`~Categories.to_arrow` for more info.\n",
            "     |\n",
            "     |  get_data(self) -> scipy.sparse._csr.csr_matrix\n",
            "     |      Get the predictors from DMatrix as a CSR matrix. This getter is mostly for\n",
            "     |      testing purposes. If this is a quantized DMatrix then quantized values are\n",
            "     |      returned instead of input values.\n",
            "     |\n",
            "     |      .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |  get_float_info(self, field: str) -> numpy.ndarray\n",
            "     |      Get float property from the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      info : array\n",
            "     |          a numpy array of float information of the data\n",
            "     |\n",
            "     |  get_group(self) -> numpy.ndarray\n",
            "     |      Get the group of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      group\n",
            "     |\n",
            "     |  get_label(self) -> numpy.ndarray\n",
            "     |      Get the label of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      label : array\n",
            "     |\n",
            "     |  get_quantile_cut(self) -> Tuple[numpy.ndarray, numpy.ndarray]\n",
            "     |      Get quantile cuts for quantization.\n",
            "     |\n",
            "     |      .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |  get_uint_info(self, field: str) -> numpy.ndarray\n",
            "     |      Get unsigned integer property from the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      info : array\n",
            "     |          a numpy array of unsigned integer information of the data\n",
            "     |\n",
            "     |  get_weight(self) -> numpy.ndarray\n",
            "     |      Get the weight of the DMatrix.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      weight : array\n",
            "     |\n",
            "     |  num_col(self) -> int\n",
            "     |      Get the number of columns (features) in the DMatrix.\n",
            "     |\n",
            "     |  num_nonmissing(self) -> int\n",
            "     |      Get the number of non-missing values in the DMatrix.\n",
            "     |\n",
            "     |      .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |  num_row(self) -> int\n",
            "     |      Get the number of rows in the DMatrix.\n",
            "     |\n",
            "     |  save_binary(self, fname: Union[str, os.PathLike], silent: bool = True) -> None\n",
            "     |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
            "     |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname : string or os.PathLike\n",
            "     |          Name of the output buffer file.\n",
            "     |      silent : bool (optional; default: True)\n",
            "     |          If set, the output is suppressed.\n",
            "     |\n",
            "     |  set_base_margin(self, margin: Any) -> None\n",
            "     |      Set base margin of booster to start from.\n",
            "     |\n",
            "     |      This can be used to specify a prediction value of existing model to be\n",
            "     |      base_margin However, remember margin is needed, instead of transformed\n",
            "     |      prediction e.g. for logistic regression: need to put in value before\n",
            "     |      logistic transformation see also example/demo.py\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      margin: array like\n",
            "     |          Prediction margin of each datapoint\n",
            "     |\n",
            "     |  set_float_info(self, field: str, data: Any) -> None\n",
            "     |      Set float type property into the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_float_info_npy2d(self, field: str, data: Any) -> None\n",
            "     |      Set float type property into the DMatrix\n",
            "     |         for numpy 2d array input\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_group(self, group: Any) -> None\n",
            "     |      Set group size of DMatrix (used for ranking).\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      group : array like\n",
            "     |          Group size of each group\n",
            "     |\n",
            "     |  set_info(self, *, label: Optional[Any] = None, weight: Optional[Any] = None, base_margin: Optional[Any] = None, group: Optional[Any] = None, qid: Optional[Any] = None, label_lower_bound: Optional[Any] = None, label_upper_bound: Optional[Any] = None, feature_names: Optional[Sequence[str]] = None, feature_types: Optional[Sequence[str]] = None, feature_weights: Optional[Any] = None) -> None\n",
            "     |      Set meta info for DMatrix.  See doc string for :py:obj:`xgboost.DMatrix`.\n",
            "     |\n",
            "     |  set_label(self, label: Any) -> None\n",
            "     |      Set label of dmatrix\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      label: array like\n",
            "     |          The label information to be set into DMatrix\n",
            "     |\n",
            "     |  set_uint_info(self, field: str, data: Any) -> None\n",
            "     |      Set uint type property into the DMatrix.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      field: str\n",
            "     |          The field name of the information\n",
            "     |\n",
            "     |      data: numpy array\n",
            "     |          The array of data to be set\n",
            "     |\n",
            "     |  set_weight(self, weight: Any) -> None\n",
            "     |      Set weight of each instance.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      weight : array like\n",
            "     |          Weight for each data point\n",
            "     |\n",
            "     |          .. note:: For ranking task, weights are per-group.\n",
            "     |\n",
            "     |              In ranking task, one weight is assigned to each group (not each\n",
            "     |              data point). This is because we only care about the relative\n",
            "     |              ordering of data points within each group, so it doesn't make\n",
            "     |              sense to assign weights to individual data points.\n",
            "     |\n",
            "     |  slice(self, rindex: Union[List[int], numpy.ndarray], allow_groups: bool = False) -> 'DMatrix'\n",
            "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      rindex\n",
            "     |          List of indices to be selected.\n",
            "     |      allow_groups\n",
            "     |          Allow slicing of a matrix with a groups attribute\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      res\n",
            "     |          A new DMatrix containing only selected indices.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from DMatrix:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  feature_names\n",
            "     |      Labels for features (column labels).\n",
            "     |\n",
            "     |      Setting it to ``None`` resets existing feature names.\n",
            "     |\n",
            "     |  feature_types\n",
            "     |      Type of features (column types).\n",
            "     |\n",
            "     |      This is for displaying the results and categorical data support. See\n",
            "     |      :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      Setting it to ``None`` resets existing feature types.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from _RefMixIn:\n",
            "     |\n",
            "     |  ref\n",
            "     |      Internal method for retrieving a reference to the training DMatrix.\n",
            "\n",
            "    class RabitTracker(builtins.object)\n",
            "     |  RabitTracker(n_workers: int, host_ip: Optional[str], port: int = 0, *, sortby: str = 'host', timeout: int = 0) -> None\n",
            "     |\n",
            "     |  Tracker for the collective used in XGBoost, acting as a coordinator between\n",
            "     |  workers.\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |\n",
            "     |  n_workers:\n",
            "     |\n",
            "     |      The total number of workers in the communication group.\n",
            "     |\n",
            "     |  host_ip:\n",
            "     |\n",
            "     |      The IP address of the tracker node. XGBoost can try to guess one by probing with\n",
            "     |      sockets. But it's best to explicitly pass an address.\n",
            "     |\n",
            "     |  port:\n",
            "     |\n",
            "     |      The port this tracker should listen to. XGBoost can query an available port from\n",
            "     |      the OS, this configuration is useful for restricted network environments.\n",
            "     |\n",
            "     |  sortby:\n",
            "     |\n",
            "     |      How to sort the workers for rank assignment. The default is host, but users can\n",
            "     |      set the `DMLC_TASK_ID` via arguments of :py:meth:`~xgboost.collective.init` and\n",
            "     |      obtain deterministic rank assignment through sorting by task name. Available\n",
            "     |      options are:\n",
            "     |\n",
            "     |        - host\n",
            "     |        - task\n",
            "     |\n",
            "     |  timeout :\n",
            "     |\n",
            "     |      Timeout for constructing (bootstrap) and shutting down the communication group,\n",
            "     |      doesn't apply to communication when the group is up and running.\n",
            "     |\n",
            "     |      The timeout value should take the time of data loading and pre-processing into\n",
            "     |      account, due to potential lazy execution. By default the Tracker doesn't have\n",
            "     |      any timeout to avoid pre-mature aborting.\n",
            "     |\n",
            "     |      The :py:meth:`.wait_for` method has a different timeout parameter that can stop\n",
            "     |      the tracker even if the tracker is still being used. A value error is raised\n",
            "     |      when timeout is reached.\n",
            "     |\n",
            "     |  Examples\n",
            "     |  --------\n",
            "     |\n",
            "     |  .. code-block:: python\n",
            "     |\n",
            "     |      from xgboost.tracker import RabitTracker\n",
            "     |      from xgboost import collective as coll\n",
            "     |\n",
            "     |      tracker = RabitTracker(host_ip=\"127.0.0.1\", n_workers=2)\n",
            "     |      tracker.start()\n",
            "     |\n",
            "     |      with coll.CommunicatorContext(**tracker.worker_args()):\n",
            "     |          ret = coll.broadcast(\"msg\", 0)\n",
            "     |          assert str(ret) == \"msg\"\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __del__(self) -> None\n",
            "     |\n",
            "     |  __init__(self, n_workers: int, host_ip: Optional[str], port: int = 0, *, sortby: str = 'host', timeout: int = 0) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |\n",
            "     |  free(self) -> None\n",
            "     |      Internal function for testing.\n",
            "     |\n",
            "     |  start(self) -> None\n",
            "     |      Start the tracker. Once started, the client still need to call the\n",
            "     |      :py:meth:`wait_for` method in order to wait for it to finish (think of it as a\n",
            "     |      thread).\n",
            "     |\n",
            "     |  wait_for(self, timeout: Optional[int] = None) -> None\n",
            "     |      Wait for the tracker to finish all the work and shutdown. When timeout is\n",
            "     |      reached, a value error is raised. By default we don't have timeout since we\n",
            "     |      don't know how long it takes for the model to finish training.\n",
            "     |\n",
            "     |  worker_args(self) -> Dict[str, Union[str, int]]\n",
            "     |      Get arguments for workers.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors defined here:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "\n",
            "    class XGBClassifier(sklearn.base.ClassifierMixin, XGBModel)\n",
            "     |  XGBClassifier(*, objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', **kwargs: Any) -> None\n",
            "     |\n",
            "     |  Implementation of the scikit-learn API for XGBoost classification.\n",
            "     |  See :doc:`/python/sklearn_estimator` for more information.\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |\n",
            "     |      n_estimators : Optional[int]\n",
            "     |          Number of boosting rounds.\n",
            "     |\n",
            "     |      max_depth :  typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum tree depth for base learners.\n",
            "     |\n",
            "     |      max_leaves : typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum number of leaves; 0 indicates no limit.\n",
            "     |\n",
            "     |      max_bin : typing.Optional[int]\n",
            "     |\n",
            "     |          If using histogram-based algorithm, maximum number of bins per feature\n",
            "     |\n",
            "     |      grow_policy : typing.Optional[str]\n",
            "     |\n",
            "     |          Tree growing policy.\n",
            "     |\n",
            "     |          - depthwise: Favors splitting at nodes closest to the node,\n",
            "     |          - lossguide: Favors splitting at nodes with highest loss change.\n",
            "     |\n",
            "     |      learning_rate : typing.Optional[float]\n",
            "     |\n",
            "     |          Boosting learning rate (xgb's \"eta\")\n",
            "     |\n",
            "     |      verbosity : typing.Optional[int]\n",
            "     |\n",
            "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
            "     |\n",
            "     |      objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
            "     |\n",
            "     |          Specify the learning task and the corresponding learning objective or a custom\n",
            "     |          objective function to be used.\n",
            "     |\n",
            "     |          For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
            "     |          :ref:`custom-obj-metric` for more information, along with the end note for\n",
            "     |          function signatures.\n",
            "     |\n",
            "     |      booster: typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
            "     |\n",
            "     |      tree_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
            "     |          default, XGBoost will choose the most conservative option available.  It's\n",
            "     |          recommended to study this option from the parameters document :doc:`tree method\n",
            "     |          </treemethod>`\n",
            "     |\n",
            "     |      n_jobs : typing.Optional[int]\n",
            "     |\n",
            "     |          Number of parallel threads used to run xgboost.  When used with other\n",
            "     |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
            "     |          parallelize and balance the threads.  Creating thread contention will\n",
            "     |          significantly slow down both algorithms.\n",
            "     |\n",
            "     |      gamma : typing.Optional[float]\n",
            "     |\n",
            "     |          (min_split_loss) Minimum loss reduction required to make a further partition on\n",
            "     |          a leaf node of the tree.\n",
            "     |\n",
            "     |      min_child_weight : typing.Optional[float]\n",
            "     |\n",
            "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
            "     |\n",
            "     |      max_delta_step : typing.Optional[float]\n",
            "     |\n",
            "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
            "     |\n",
            "     |      subsample : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of the training instance.\n",
            "     |\n",
            "     |      sampling_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
            "     |\n",
            "     |          - ``uniform``: Select random training instances uniformly.\n",
            "     |          - ``gradient_based``: Select random training instances with higher probability\n",
            "     |              when the gradient and hessian are larger. (cf. CatBoost)\n",
            "     |\n",
            "     |      colsample_bytree : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns when constructing each tree.\n",
            "     |\n",
            "     |      colsample_bylevel : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each level.\n",
            "     |\n",
            "     |      colsample_bynode : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each split.\n",
            "     |\n",
            "     |      reg_alpha : typing.Optional[float]\n",
            "     |\n",
            "     |          L1 regularization term on weights (xgb's alpha).\n",
            "     |\n",
            "     |      reg_lambda : typing.Optional[float]\n",
            "     |\n",
            "     |          L2 regularization term on weights (xgb's lambda).\n",
            "     |\n",
            "     |      scale_pos_weight : typing.Optional[float]\n",
            "     |          Balancing of positive and negative weights.\n",
            "     |\n",
            "     |      base_score : typing.Union[float, typing.List[float], NoneType]\n",
            "     |\n",
            "     |          The initial prediction score of all instances, global bias.\n",
            "     |\n",
            "     |      random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
            "     |\n",
            "     |          Random number seed.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
            "     |             it uses Hogwild algorithm.\n",
            "     |\n",
            "     |      missing : float\n",
            "     |\n",
            "     |          Value in the data which needs to be present as a missing value. Default to\n",
            "     |          :py:data:`numpy.nan`.\n",
            "     |\n",
            "     |      num_parallel_tree: typing.Optional[int]\n",
            "     |\n",
            "     |          Used for boosting random forest.\n",
            "     |\n",
            "     |      monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
            "     |\n",
            "     |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
            "     |          for more information.\n",
            "     |\n",
            "     |      interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
            "     |\n",
            "     |          Constraints for interaction representing permitted interactions.  The\n",
            "     |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
            "     |          3, 4]]``, where each inner list is a group of indices of features that are\n",
            "     |          allowed to interact with each other.  See :doc:`tutorial\n",
            "     |          </tutorials/feature_interaction_constraint>` for more information\n",
            "     |\n",
            "     |      importance_type: typing.Optional[str]\n",
            "     |\n",
            "     |          The feature importance type for the feature_importances\\_ property:\n",
            "     |\n",
            "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
            "     |            \"total_cover\".\n",
            "     |          * For linear model, only \"weight\" is defined and it's the normalized\n",
            "     |            coefficients without bias.\n",
            "     |\n",
            "     |      device : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
            "     |\n",
            "     |      validate_parameters : typing.Optional[bool]\n",
            "     |\n",
            "     |          Give warnings for unknown parameter.\n",
            "     |\n",
            "     |      enable_categorical : bool\n",
            "     |\n",
            "     |          See the same parameter of :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_types : typing.Optional[typing.Sequence[str]]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          Used for specifying feature types without constructing a dataframe. See\n",
            "     |          the :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_weights : Optional[ArrayLike]\n",
            "     |\n",
            "     |          Weight for each feature, defines the probability of each feature being selected\n",
            "     |          when colsample is being used.  All values must be greater than 0, otherwise a\n",
            "     |          `ValueError` is thrown.\n",
            "     |\n",
            "     |      max_cat_to_onehot : Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
            "     |          for categorical data.  When number of categories is lesser than the threshold\n",
            "     |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
            "     |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
            "     |          categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      max_cat_threshold : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          Maximum number of categories considered for each split. Used only by\n",
            "     |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
            "     |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      multi_strategy : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          .. note:: This parameter is working-in-progress.\n",
            "     |\n",
            "     |          The strategy used for training multi-target models, including multi-target\n",
            "     |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
            "     |          more information.\n",
            "     |\n",
            "     |          - ``one_output_per_tree``: One model for each target.\n",
            "     |          - ``multi_output_tree``:  Use multi-target trees.\n",
            "     |\n",
            "     |      eval_metric : typing.Union[str, typing.List[typing.Union[str, typing.Callable]], typing.Callable, NoneType]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          Metric used for monitoring the training result and early stopping.  It can be a\n",
            "     |          string or list of strings as names of predefined metric in XGBoost (See\n",
            "     |          :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
            "     |          other user defined metric that looks like `sklearn.metrics`.\n",
            "     |\n",
            "     |          If custom objective is also provided, then custom metric should implement the\n",
            "     |          corresponding reverse link function.\n",
            "     |\n",
            "     |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
            "     |          object is provided, it's assumed to be a cost function and by default XGBoost\n",
            "     |          will minimize the result during early stopping.\n",
            "     |\n",
            "     |          For advanced usage on Early stopping like directly choosing to maximize instead\n",
            "     |          of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
            "     |          information.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              from sklearn.datasets import load_diabetes\n",
            "     |              from sklearn.metrics import mean_absolute_error\n",
            "     |              X, y = load_diabetes(return_X_y=True)\n",
            "     |              reg = xgb.XGBRegressor(\n",
            "     |                  tree_method=\"hist\",\n",
            "     |                  eval_metric=mean_absolute_error,\n",
            "     |              )\n",
            "     |              reg.fit(X, y, eval_set=[(X, y)])\n",
            "     |\n",
            "     |      early_stopping_rounds : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          - Activates early stopping. Validation metric needs to improve at least once in\n",
            "     |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
            "     |            least one item in **eval_set** in :py:meth:`fit`.\n",
            "     |\n",
            "     |          - If early stopping occurs, the model will have two additional attributes:\n",
            "     |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
            "     |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
            "     |            number of trees during inference. If users want to access the full model\n",
            "     |            (including trees built after early stopping), they can specify the\n",
            "     |            `iteration_range` in these inference methods. In addition, other utilities\n",
            "     |            like model plotting can also use the entire model.\n",
            "     |\n",
            "     |          - If you prefer to discard the trees after `best_iteration`, consider using the\n",
            "     |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          - If there's more than one item in **eval_set**, the last entry will be used for\n",
            "     |            early stopping.  If there's more than one metric in **eval_metric**, the last\n",
            "     |            metric will be used for early stopping.\n",
            "     |\n",
            "     |      callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
            "     |\n",
            "     |          List of callback functions that are applied at end of each iteration.\n",
            "     |          It is possible to use predefined callbacks by using\n",
            "     |          :ref:`Callback API <callback_api>`.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             States in callback are not preserved during training, which means callback\n",
            "     |             objects can not be reused for multiple training sessions without\n",
            "     |             reinitialization or deepcopy.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              for params in parameters_grid:\n",
            "     |                  # be sure to (re)initialize the callbacks before each run\n",
            "     |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            "     |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
            "     |                  reg.fit(X, y)\n",
            "     |\n",
            "     |      kwargs : typing.Optional[typing.Any]\n",
            "     |\n",
            "     |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
            "     |          can be found :doc:`here </parameter>`.\n",
            "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
            "     |          dict simultaneously will result in a TypeError.\n",
            "     |\n",
            "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
            "     |\n",
            "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
            "     |              that parameters passed via this argument will interact properly\n",
            "     |              with scikit-learn.\n",
            "     |\n",
            "     |          .. note::  Custom objective function\n",
            "     |\n",
            "     |              A custom objective function can be provided for the ``objective``\n",
            "     |              parameter. In this case, it should have the signature ``objective(y_true,\n",
            "     |              y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
            "     |              -> [grad, hess]``:\n",
            "     |\n",
            "     |              y_true: array_like of shape [n_samples]\n",
            "     |                  The target values\n",
            "     |              y_pred: array_like of shape [n_samples]\n",
            "     |                  The predicted values\n",
            "     |              sample_weight :\n",
            "     |                  Optional sample weights.\n",
            "     |\n",
            "     |              grad: array_like of shape [n_samples]\n",
            "     |                  The value of the gradient for each sample point.\n",
            "     |              hess: array_like of shape [n_samples]\n",
            "     |                  The value of the second derivative for each sample point\n",
            "     |\n",
            "     |              Note that, if the custom objective produces negative values for\n",
            "     |              the Hessian, these will be clipped. If the objective is non-convex,\n",
            "     |              one might also consider using the expected Hessian (Fisher\n",
            "     |              information) instead.\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      XGBClassifier\n",
            "     |      sklearn.base.ClassifierMixin\n",
            "     |      XGBModel\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __init__(self, *, objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'binary:logistic', **kwargs: Any) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |\n",
            "     |  __sklearn_tags__(self) -> sklearn.utils._tags.Tags\n",
            "     |\n",
            "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None) -> 'XGBClassifier'\n",
            "     |      Fit gradient boosting classifier.\n",
            "     |\n",
            "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
            "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
            "     |      pass ``xgb_model`` argument.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |          When the ``tree_method`` is set to ``hist``, internally, the\n",
            "     |          :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n",
            "     |          for conserving memory. However, this has performance implications when the\n",
            "     |          device of input data is not matched with algorithm. For instance, if the\n",
            "     |          input is a numpy array on CPU but ``cuda`` is used for training, then the\n",
            "     |          data is first processed on CPU then transferred to GPU.\n",
            "     |      y :\n",
            "     |          Labels\n",
            "     |      sample_weight :\n",
            "     |          instance weights\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      eval_set :\n",
            "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
            "     |          metrics will be computed.\n",
            "     |          Validation metrics will help us track the performance of the model.\n",
            "     |\n",
            "     |      verbose :\n",
            "     |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
            "     |          measured on the validation set is printed to stdout at each boosting stage.\n",
            "     |          If `verbose` is an integer, the evaluation metric is printed at each\n",
            "     |          `verbose` boosting stage. The last boosting stage / the boosting stage found\n",
            "     |          by using `early_stopping_rounds` is also printed.\n",
            "     |      xgb_model :\n",
            "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
            "     |          loaded before training (allows training continuation).\n",
            "     |      sample_weight_eval_set :\n",
            "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
            "     |          object storing instance weights for the i-th validation set.\n",
            "     |      base_margin_eval_set :\n",
            "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
            "     |          object storing base margin for the i-th validation set.\n",
            "     |      feature_weights :\n",
            "     |\n",
            "     |          .. deprecated:: 3.0.0\n",
            "     |\n",
            "     |          Use `feature_weights` in :py:meth:`__init__` or :py:meth:`set_params`\n",
            "     |          instead.\n",
            "     |\n",
            "     |  predict(self, X: Any, *, output_margin: bool = False, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> Any\n",
            "     |      Predict with `X`.  If the model is trained with early stopping, then\n",
            "     |      :py:attr:`best_iteration` is used automatically. The estimator uses\n",
            "     |      `inplace_predict` by default and falls back to using :py:class:`DMatrix` if\n",
            "     |      devices between the data and the estimator don't match.\n",
            "     |\n",
            "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Data to predict with. See :ref:`py-data` for a list of supported types.\n",
            "     |      output_margin :\n",
            "     |          Whether to output the raw untransformed margin value.\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's feature_names are\n",
            "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
            "     |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
            "     |          are used in this prediction.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction\n",
            "     |\n",
            "     |  predict_proba(self, X: Any, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n",
            "     |      Predict the probability of each `X` example being of a given class. If the\n",
            "     |      model is trained with early stopping, then :py:attr:`best_iteration` is used\n",
            "     |      automatically. The estimator uses `inplace_predict` by default and falls back to\n",
            "     |      using :py:class:`DMatrix` if devices between the data and the estimator don't\n",
            "     |      match.\n",
            "     |\n",
            "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's feature_names are\n",
            "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
            "     |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
            "     |          used in this prediction.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction :\n",
            "     |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
            "     |          probability of each data example being of a given class.\n",
            "     |\n",
            "     |  set_fit_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', base_margin_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_weights: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', verbose: Union[bool, NoneType, str] = '$UNCHANGED$', xgb_model: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``fit`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``fit``.\n",
            "     |\n",
            "     |      base_margin_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      feature_weights : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``feature_weights`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      verbose : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``verbose`` parameter in ``fit``.\n",
            "     |\n",
            "     |      xgb_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``xgb_model`` parameter in ``fit``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_predict_proba_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``predict_proba`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``predict_proba`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict_proba``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``predict_proba``.\n",
            "     |\n",
            "     |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``iteration_range`` parameter in ``predict_proba``.\n",
            "     |\n",
            "     |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``validate_features`` parameter in ``predict_proba``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_predict_request(self: xgboost.sklearn.XGBClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', output_margin: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``predict`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``iteration_range`` parameter in ``predict``.\n",
            "     |\n",
            "     |      output_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``output_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``validate_features`` parameter in ``predict``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_score_request(self: xgboost.sklearn.XGBClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``score`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties defined here:\n",
            "     |\n",
            "     |  classes_\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |\n",
            "     |  __annotations__ = {}\n",
            "     |\n",
            "     |  __slotnames__ = []\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            "     |\n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Return the mean accuracy on the given test data and labels.\n",
            "     |\n",
            "     |      In multi-label classification, this is the subset accuracy\n",
            "     |      which is a harsh metric since you require for each sample that\n",
            "     |      each label set be correctly predicted.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like of shape (n_samples, n_features)\n",
            "     |          Test samples.\n",
            "     |\n",
            "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            "     |          True labels for `X`.\n",
            "     |\n",
            "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
            "     |          Sample weights.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from XGBModel:\n",
            "     |\n",
            "     |  __sklearn_is_fitted__(self) -> bool\n",
            "     |\n",
            "     |  apply(self, X: Any, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n",
            "     |      Return the predicted leaf every tree for each sample. If the model is trained\n",
            "     |      with early stopping, then :py:attr:`best_iteration` is used automatically.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input features matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |      iteration_range :\n",
            "     |          See :py:meth:`predict`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
            "     |          For each datapoint x in X and for each tree, return the index of the\n",
            "     |          leaf x ends up in. Leaves are numbered within\n",
            "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
            "     |\n",
            "     |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
            "     |      Return the evaluation results.\n",
            "     |\n",
            "     |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
            "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
            "     |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
            "     |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
            "     |      function.\n",
            "     |\n",
            "     |      The returned evaluation result is a dictionary:\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
            "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      evals_result\n",
            "     |\n",
            "     |  get_booster(self) -> xgboost.core.Booster\n",
            "     |      Get the underlying xgboost Booster of this model.\n",
            "     |\n",
            "     |      This will raise an exception when fit was not called\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      booster : a xgboost booster of underlying model\n",
            "     |\n",
            "     |  get_num_boosting_rounds(self) -> int\n",
            "     |      Gets the number of xgboost boosting rounds.\n",
            "     |\n",
            "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
            "     |      Get parameters.\n",
            "     |\n",
            "     |  get_xgb_params(self) -> Dict[str, Any]\n",
            "     |      Get xgboost specific parameters.\n",
            "     |\n",
            "     |  load_model(self, fname: Union[os.PathLike[~AnyStr], bytearray, str]) -> None\n",
            "     |      Load the model from a file or a bytearray.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        model.load_model(\"model.json\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |        model.load_model(\"model.ubj\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        buf = model.save_raw()\n",
            "     |        model.load_model(buf)\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Input file name or memory buffer(see also save_raw)\n",
            "     |\n",
            "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
            "     |      Save the model to a file.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Output file name\n",
            "     |\n",
            "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
            "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
            "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
            "     |      parameters that are not defined as member variables in sklearn grid\n",
            "     |      search.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from XGBModel:\n",
            "     |\n",
            "     |  best_iteration\n",
            "     |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
            "     |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
            "     |\n",
            "     |  best_score\n",
            "     |      The best score obtained by early stopping.\n",
            "     |\n",
            "     |  coef_\n",
            "     |      Coefficients property\n",
            "     |\n",
            "     |      .. note:: Coefficients are defined only for linear learners\n",
            "     |\n",
            "     |          Coefficients are only defined when the linear model is chosen as\n",
            "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
            "     |          learner types, such as tree learners (`booster=gbtree`).\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
            "     |\n",
            "     |  feature_importances_\n",
            "     |      Feature importances property, return depends on `importance_type`\n",
            "     |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
            "     |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
            "     |      based on the importance type. For instance, if the importance type is\n",
            "     |      \"total_gain\", then the score is sum of loss change for each split from all\n",
            "     |      trees.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
            "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
            "     |\n",
            "     |  feature_names_in_\n",
            "     |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has\n",
            "     |      feature names that are all strings.\n",
            "     |\n",
            "     |  intercept_\n",
            "     |      Intercept (bias) property\n",
            "     |\n",
            "     |      For tree-based model, the returned value is the `base_score`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
            "     |\n",
            "     |  n_features_in_\n",
            "     |      Number of features seen during :py:meth:`fit`.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |\n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |\n",
            "     |  __repr__(self, N_CHAR_MAX=700)\n",
            "     |      Return repr(self).\n",
            "     |\n",
            "     |  __setstate__(self, state)\n",
            "     |\n",
            "     |  __sklearn_clone__(self)\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  get_metadata_routing(self)\n",
            "     |      Get metadata routing of this object.\n",
            "     |\n",
            "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      routing : MetadataRequest\n",
            "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            "     |          routing information.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  __init_subclass__(**kwargs)\n",
            "     |      Set the ``set_{method}_request`` methods.\n",
            "     |\n",
            "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            "     |      looks for the information available in the set default values which are\n",
            "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            "     |      from method signatures.\n",
            "     |\n",
            "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
            "     |      does not explicitly accept a metadata through its arguments or if the\n",
            "     |      developer would like to specify a request value for those metadata\n",
            "     |      which are different from the default ``None``.\n",
            "     |\n",
            "     |      References\n",
            "     |      ----------\n",
            "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n",
            "    class XGBModel(sklearn.base.BaseEstimator)\n",
            "     |  XGBModel(*, max_depth: Optional[int] = None, max_leaves: Optional[int] = None, max_bin: Optional[int] = None, grow_policy: Optional[str] = None, learning_rate: Optional[float] = None, n_estimators: Optional[int] = None, verbosity: Optional[int] = None, objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = None, booster: Optional[str] = None, tree_method: Optional[str] = None, n_jobs: Optional[int] = None, gamma: Optional[float] = None, min_child_weight: Optional[float] = None, max_delta_step: Optional[float] = None, subsample: Optional[float] = None, sampling_method: Optional[str] = None, colsample_bytree: Optional[float] = None, colsample_bylevel: Optional[float] = None, colsample_bynode: Optional[float] = None, reg_alpha: Optional[float] = None, reg_lambda: Optional[float] = None, scale_pos_weight: Optional[float] = None, base_score: Union[float, List[float], NoneType] = None, random_state: Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType] = None, missing: float = nan, num_parallel_tree: Optional[int] = None, monotone_constraints: Union[Dict[str, int], str, NoneType] = None, interaction_constraints: Union[str, Sequence[Sequence[str]], NoneType] = None, importance_type: Optional[str] = None, device: Optional[str] = None, validate_parameters: Optional[bool] = None, enable_categorical: bool = False, feature_types: Optional[Sequence[str]] = None, feature_weights: Optional[Any] = None, max_cat_to_onehot: Optional[int] = None, max_cat_threshold: Optional[int] = None, multi_strategy: Optional[str] = None, eval_metric: Union[str, List[Union[str, Callable]], Callable, NoneType] = None, early_stopping_rounds: Optional[int] = None, callbacks: Optional[List[xgboost.callback.TrainingCallback]] = None, **kwargs: Any) -> None\n",
            "     |\n",
            "     |  Implementation of the Scikit-Learn API for XGBoost.\n",
            "     |  See :doc:`/python/sklearn_estimator` for more information.\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |\n",
            "     |      n_estimators : typing.Optional[int]\n",
            "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
            "     |          rounds.\n",
            "     |\n",
            "     |      max_depth :  typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum tree depth for base learners.\n",
            "     |\n",
            "     |      max_leaves : typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum number of leaves; 0 indicates no limit.\n",
            "     |\n",
            "     |      max_bin : typing.Optional[int]\n",
            "     |\n",
            "     |          If using histogram-based algorithm, maximum number of bins per feature\n",
            "     |\n",
            "     |      grow_policy : typing.Optional[str]\n",
            "     |\n",
            "     |          Tree growing policy.\n",
            "     |\n",
            "     |          - depthwise: Favors splitting at nodes closest to the node,\n",
            "     |          - lossguide: Favors splitting at nodes with highest loss change.\n",
            "     |\n",
            "     |      learning_rate : typing.Optional[float]\n",
            "     |\n",
            "     |          Boosting learning rate (xgb's \"eta\")\n",
            "     |\n",
            "     |      verbosity : typing.Optional[int]\n",
            "     |\n",
            "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
            "     |\n",
            "     |      objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
            "     |\n",
            "     |          Specify the learning task and the corresponding learning objective or a custom\n",
            "     |          objective function to be used.\n",
            "     |\n",
            "     |          For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
            "     |          :ref:`custom-obj-metric` for more information, along with the end note for\n",
            "     |          function signatures.\n",
            "     |\n",
            "     |      booster: typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
            "     |\n",
            "     |      tree_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
            "     |          default, XGBoost will choose the most conservative option available.  It's\n",
            "     |          recommended to study this option from the parameters document :doc:`tree method\n",
            "     |          </treemethod>`\n",
            "     |\n",
            "     |      n_jobs : typing.Optional[int]\n",
            "     |\n",
            "     |          Number of parallel threads used to run xgboost.  When used with other\n",
            "     |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
            "     |          parallelize and balance the threads.  Creating thread contention will\n",
            "     |          significantly slow down both algorithms.\n",
            "     |\n",
            "     |      gamma : typing.Optional[float]\n",
            "     |\n",
            "     |          (min_split_loss) Minimum loss reduction required to make a further partition on\n",
            "     |          a leaf node of the tree.\n",
            "     |\n",
            "     |      min_child_weight : typing.Optional[float]\n",
            "     |\n",
            "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
            "     |\n",
            "     |      max_delta_step : typing.Optional[float]\n",
            "     |\n",
            "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
            "     |\n",
            "     |      subsample : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of the training instance.\n",
            "     |\n",
            "     |      sampling_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
            "     |\n",
            "     |          - ``uniform``: Select random training instances uniformly.\n",
            "     |          - ``gradient_based``: Select random training instances with higher probability\n",
            "     |              when the gradient and hessian are larger. (cf. CatBoost)\n",
            "     |\n",
            "     |      colsample_bytree : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns when constructing each tree.\n",
            "     |\n",
            "     |      colsample_bylevel : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each level.\n",
            "     |\n",
            "     |      colsample_bynode : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each split.\n",
            "     |\n",
            "     |      reg_alpha : typing.Optional[float]\n",
            "     |\n",
            "     |          L1 regularization term on weights (xgb's alpha).\n",
            "     |\n",
            "     |      reg_lambda : typing.Optional[float]\n",
            "     |\n",
            "     |          L2 regularization term on weights (xgb's lambda).\n",
            "     |\n",
            "     |      scale_pos_weight : typing.Optional[float]\n",
            "     |          Balancing of positive and negative weights.\n",
            "     |\n",
            "     |      base_score : typing.Union[float, typing.List[float], NoneType]\n",
            "     |\n",
            "     |          The initial prediction score of all instances, global bias.\n",
            "     |\n",
            "     |      random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
            "     |\n",
            "     |          Random number seed.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
            "     |             it uses Hogwild algorithm.\n",
            "     |\n",
            "     |      missing : float\n",
            "     |\n",
            "     |          Value in the data which needs to be present as a missing value. Default to\n",
            "     |          :py:data:`numpy.nan`.\n",
            "     |\n",
            "     |      num_parallel_tree: typing.Optional[int]\n",
            "     |\n",
            "     |          Used for boosting random forest.\n",
            "     |\n",
            "     |      monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
            "     |\n",
            "     |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
            "     |          for more information.\n",
            "     |\n",
            "     |      interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
            "     |\n",
            "     |          Constraints for interaction representing permitted interactions.  The\n",
            "     |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
            "     |          3, 4]]``, where each inner list is a group of indices of features that are\n",
            "     |          allowed to interact with each other.  See :doc:`tutorial\n",
            "     |          </tutorials/feature_interaction_constraint>` for more information\n",
            "     |\n",
            "     |      importance_type: typing.Optional[str]\n",
            "     |\n",
            "     |          The feature importance type for the feature_importances\\_ property:\n",
            "     |\n",
            "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
            "     |            \"total_cover\".\n",
            "     |          * For linear model, only \"weight\" is defined and it's the normalized\n",
            "     |            coefficients without bias.\n",
            "     |\n",
            "     |      device : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
            "     |\n",
            "     |      validate_parameters : typing.Optional[bool]\n",
            "     |\n",
            "     |          Give warnings for unknown parameter.\n",
            "     |\n",
            "     |      enable_categorical : bool\n",
            "     |\n",
            "     |          See the same parameter of :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_types : typing.Optional[typing.Sequence[str]]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          Used for specifying feature types without constructing a dataframe. See\n",
            "     |          the :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_weights : Optional[ArrayLike]\n",
            "     |\n",
            "     |          Weight for each feature, defines the probability of each feature being selected\n",
            "     |          when colsample is being used.  All values must be greater than 0, otherwise a\n",
            "     |          `ValueError` is thrown.\n",
            "     |\n",
            "     |      max_cat_to_onehot : Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
            "     |          for categorical data.  When number of categories is lesser than the threshold\n",
            "     |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
            "     |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
            "     |          categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      max_cat_threshold : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          Maximum number of categories considered for each split. Used only by\n",
            "     |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
            "     |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      multi_strategy : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          .. note:: This parameter is working-in-progress.\n",
            "     |\n",
            "     |          The strategy used for training multi-target models, including multi-target\n",
            "     |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
            "     |          more information.\n",
            "     |\n",
            "     |          - ``one_output_per_tree``: One model for each target.\n",
            "     |          - ``multi_output_tree``:  Use multi-target trees.\n",
            "     |\n",
            "     |      eval_metric : typing.Union[str, typing.List[typing.Union[str, typing.Callable]], typing.Callable, NoneType]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          Metric used for monitoring the training result and early stopping.  It can be a\n",
            "     |          string or list of strings as names of predefined metric in XGBoost (See\n",
            "     |          :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
            "     |          other user defined metric that looks like `sklearn.metrics`.\n",
            "     |\n",
            "     |          If custom objective is also provided, then custom metric should implement the\n",
            "     |          corresponding reverse link function.\n",
            "     |\n",
            "     |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
            "     |          object is provided, it's assumed to be a cost function and by default XGBoost\n",
            "     |          will minimize the result during early stopping.\n",
            "     |\n",
            "     |          For advanced usage on Early stopping like directly choosing to maximize instead\n",
            "     |          of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
            "     |          information.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              from sklearn.datasets import load_diabetes\n",
            "     |              from sklearn.metrics import mean_absolute_error\n",
            "     |              X, y = load_diabetes(return_X_y=True)\n",
            "     |              reg = xgb.XGBRegressor(\n",
            "     |                  tree_method=\"hist\",\n",
            "     |                  eval_metric=mean_absolute_error,\n",
            "     |              )\n",
            "     |              reg.fit(X, y, eval_set=[(X, y)])\n",
            "     |\n",
            "     |      early_stopping_rounds : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          - Activates early stopping. Validation metric needs to improve at least once in\n",
            "     |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
            "     |            least one item in **eval_set** in :py:meth:`fit`.\n",
            "     |\n",
            "     |          - If early stopping occurs, the model will have two additional attributes:\n",
            "     |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
            "     |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
            "     |            number of trees during inference. If users want to access the full model\n",
            "     |            (including trees built after early stopping), they can specify the\n",
            "     |            `iteration_range` in these inference methods. In addition, other utilities\n",
            "     |            like model plotting can also use the entire model.\n",
            "     |\n",
            "     |          - If you prefer to discard the trees after `best_iteration`, consider using the\n",
            "     |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          - If there's more than one item in **eval_set**, the last entry will be used for\n",
            "     |            early stopping.  If there's more than one metric in **eval_metric**, the last\n",
            "     |            metric will be used for early stopping.\n",
            "     |\n",
            "     |      callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
            "     |\n",
            "     |          List of callback functions that are applied at end of each iteration.\n",
            "     |          It is possible to use predefined callbacks by using\n",
            "     |          :ref:`Callback API <callback_api>`.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             States in callback are not preserved during training, which means callback\n",
            "     |             objects can not be reused for multiple training sessions without\n",
            "     |             reinitialization or deepcopy.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              for params in parameters_grid:\n",
            "     |                  # be sure to (re)initialize the callbacks before each run\n",
            "     |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            "     |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
            "     |                  reg.fit(X, y)\n",
            "     |\n",
            "     |      kwargs : typing.Optional[typing.Any]\n",
            "     |\n",
            "     |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
            "     |          can be found :doc:`here </parameter>`.\n",
            "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
            "     |          dict simultaneously will result in a TypeError.\n",
            "     |\n",
            "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
            "     |\n",
            "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
            "     |              that parameters passed via this argument will interact properly\n",
            "     |              with scikit-learn.\n",
            "     |\n",
            "     |          .. note::  Custom objective function\n",
            "     |\n",
            "     |              A custom objective function can be provided for the ``objective``\n",
            "     |              parameter. In this case, it should have the signature ``objective(y_true,\n",
            "     |              y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
            "     |              -> [grad, hess]``:\n",
            "     |\n",
            "     |              y_true: array_like of shape [n_samples]\n",
            "     |                  The target values\n",
            "     |              y_pred: array_like of shape [n_samples]\n",
            "     |                  The predicted values\n",
            "     |              sample_weight :\n",
            "     |                  Optional sample weights.\n",
            "     |\n",
            "     |              grad: array_like of shape [n_samples]\n",
            "     |                  The value of the gradient for each sample point.\n",
            "     |              hess: array_like of shape [n_samples]\n",
            "     |                  The value of the second derivative for each sample point\n",
            "     |\n",
            "     |              Note that, if the custom objective produces negative values for\n",
            "     |              the Hessian, these will be clipped. If the objective is non-convex,\n",
            "     |              one might also consider using the expected Hessian (Fisher\n",
            "     |              information) instead.\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      XGBModel\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __init__(self, *, max_depth: Optional[int] = None, max_leaves: Optional[int] = None, max_bin: Optional[int] = None, grow_policy: Optional[str] = None, learning_rate: Optional[float] = None, n_estimators: Optional[int] = None, verbosity: Optional[int] = None, objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = None, booster: Optional[str] = None, tree_method: Optional[str] = None, n_jobs: Optional[int] = None, gamma: Optional[float] = None, min_child_weight: Optional[float] = None, max_delta_step: Optional[float] = None, subsample: Optional[float] = None, sampling_method: Optional[str] = None, colsample_bytree: Optional[float] = None, colsample_bylevel: Optional[float] = None, colsample_bynode: Optional[float] = None, reg_alpha: Optional[float] = None, reg_lambda: Optional[float] = None, scale_pos_weight: Optional[float] = None, base_score: Union[float, List[float], NoneType] = None, random_state: Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType] = None, missing: float = nan, num_parallel_tree: Optional[int] = None, monotone_constraints: Union[Dict[str, int], str, NoneType] = None, interaction_constraints: Union[str, Sequence[Sequence[str]], NoneType] = None, importance_type: Optional[str] = None, device: Optional[str] = None, validate_parameters: Optional[bool] = None, enable_categorical: bool = False, feature_types: Optional[Sequence[str]] = None, feature_weights: Optional[Any] = None, max_cat_to_onehot: Optional[int] = None, max_cat_threshold: Optional[int] = None, multi_strategy: Optional[str] = None, eval_metric: Union[str, List[Union[str, Callable]], Callable, NoneType] = None, early_stopping_rounds: Optional[int] = None, callbacks: Optional[List[xgboost.callback.TrainingCallback]] = None, **kwargs: Any) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |\n",
            "     |  __sklearn_is_fitted__(self) -> bool\n",
            "     |\n",
            "     |  __sklearn_tags__(self) -> sklearn.utils._tags.Tags\n",
            "     |\n",
            "     |  apply(self, X: Any, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n",
            "     |      Return the predicted leaf every tree for each sample. If the model is trained\n",
            "     |      with early stopping, then :py:attr:`best_iteration` is used automatically.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input features matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |      iteration_range :\n",
            "     |          See :py:meth:`predict`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
            "     |          For each datapoint x in X and for each tree, return the index of the\n",
            "     |          leaf x ends up in. Leaves are numbered within\n",
            "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
            "     |\n",
            "     |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
            "     |      Return the evaluation results.\n",
            "     |\n",
            "     |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
            "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
            "     |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
            "     |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
            "     |      function.\n",
            "     |\n",
            "     |      The returned evaluation result is a dictionary:\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
            "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      evals_result\n",
            "     |\n",
            "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, ForwardRef('XGBModel'), str, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None) -> 'XGBModel'\n",
            "     |      Fit gradient boosting model.\n",
            "     |\n",
            "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
            "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
            "     |      pass ``xgb_model`` argument.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |          When the ``tree_method`` is set to ``hist``, internally, the\n",
            "     |          :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n",
            "     |          for conserving memory. However, this has performance implications when the\n",
            "     |          device of input data is not matched with algorithm. For instance, if the\n",
            "     |          input is a numpy array on CPU but ``cuda`` is used for training, then the\n",
            "     |          data is first processed on CPU then transferred to GPU.\n",
            "     |      y :\n",
            "     |          Labels\n",
            "     |      sample_weight :\n",
            "     |          instance weights\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      eval_set :\n",
            "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
            "     |          metrics will be computed.\n",
            "     |          Validation metrics will help us track the performance of the model.\n",
            "     |\n",
            "     |      verbose :\n",
            "     |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
            "     |          measured on the validation set is printed to stdout at each boosting stage.\n",
            "     |          If `verbose` is an integer, the evaluation metric is printed at each\n",
            "     |          `verbose` boosting stage. The last boosting stage / the boosting stage found\n",
            "     |          by using `early_stopping_rounds` is also printed.\n",
            "     |      xgb_model :\n",
            "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
            "     |          loaded before training (allows training continuation).\n",
            "     |      sample_weight_eval_set :\n",
            "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
            "     |          object storing instance weights for the i-th validation set.\n",
            "     |      base_margin_eval_set :\n",
            "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
            "     |          object storing base margin for the i-th validation set.\n",
            "     |      feature_weights :\n",
            "     |\n",
            "     |          .. deprecated:: 3.0.0\n",
            "     |\n",
            "     |          Use `feature_weights` in :py:meth:`__init__` or :py:meth:`set_params`\n",
            "     |          instead.\n",
            "     |\n",
            "     |  get_booster(self) -> xgboost.core.Booster\n",
            "     |      Get the underlying xgboost Booster of this model.\n",
            "     |\n",
            "     |      This will raise an exception when fit was not called\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      booster : a xgboost booster of underlying model\n",
            "     |\n",
            "     |  get_num_boosting_rounds(self) -> int\n",
            "     |      Gets the number of xgboost boosting rounds.\n",
            "     |\n",
            "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
            "     |      Get parameters.\n",
            "     |\n",
            "     |  get_xgb_params(self) -> Dict[str, Any]\n",
            "     |      Get xgboost specific parameters.\n",
            "     |\n",
            "     |  load_model(self, fname: Union[os.PathLike[~AnyStr], bytearray, str]) -> None\n",
            "     |      Load the model from a file or a bytearray.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        model.load_model(\"model.json\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |        model.load_model(\"model.ubj\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        buf = model.save_raw()\n",
            "     |        model.load_model(buf)\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Input file name or memory buffer(see also save_raw)\n",
            "     |\n",
            "     |  predict(self, X: Any, *, output_margin: bool = False, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> Any\n",
            "     |      Predict with `X`.  If the model is trained with early stopping, then\n",
            "     |      :py:attr:`best_iteration` is used automatically. The estimator uses\n",
            "     |      `inplace_predict` by default and falls back to using :py:class:`DMatrix` if\n",
            "     |      devices between the data and the estimator don't match.\n",
            "     |\n",
            "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Data to predict with. See :ref:`py-data` for a list of supported types.\n",
            "     |      output_margin :\n",
            "     |          Whether to output the raw untransformed margin value.\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's feature_names are\n",
            "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
            "     |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
            "     |          are used in this prediction.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction\n",
            "     |\n",
            "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
            "     |      Save the model to a file.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Output file name\n",
            "     |\n",
            "     |  set_fit_request(self: xgboost.sklearn.XGBModel, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', base_margin_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_weights: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', verbose: Union[bool, NoneType, str] = '$UNCHANGED$', xgb_model: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBModel from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``fit`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``fit``.\n",
            "     |\n",
            "     |      base_margin_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      feature_weights : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``feature_weights`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      verbose : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``verbose`` parameter in ``fit``.\n",
            "     |\n",
            "     |      xgb_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``xgb_model`` parameter in ``fit``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
            "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
            "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
            "     |      parameters that are not defined as member variables in sklearn grid\n",
            "     |      search.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |\n",
            "     |  set_predict_request(self: xgboost.sklearn.XGBModel, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', output_margin: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBModel from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``predict`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``iteration_range`` parameter in ``predict``.\n",
            "     |\n",
            "     |      output_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``output_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``validate_features`` parameter in ``predict``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties defined here:\n",
            "     |\n",
            "     |  best_iteration\n",
            "     |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
            "     |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
            "     |\n",
            "     |  best_score\n",
            "     |      The best score obtained by early stopping.\n",
            "     |\n",
            "     |  coef_\n",
            "     |      Coefficients property\n",
            "     |\n",
            "     |      .. note:: Coefficients are defined only for linear learners\n",
            "     |\n",
            "     |          Coefficients are only defined when the linear model is chosen as\n",
            "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
            "     |          learner types, such as tree learners (`booster=gbtree`).\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
            "     |\n",
            "     |  feature_importances_\n",
            "     |      Feature importances property, return depends on `importance_type`\n",
            "     |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
            "     |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
            "     |      based on the importance type. For instance, if the importance type is\n",
            "     |      \"total_gain\", then the score is sum of loss change for each split from all\n",
            "     |      trees.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
            "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
            "     |\n",
            "     |  feature_names_in_\n",
            "     |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has\n",
            "     |      feature names that are all strings.\n",
            "     |\n",
            "     |  intercept_\n",
            "     |      Intercept (bias) property\n",
            "     |\n",
            "     |      For tree-based model, the returned value is the `base_score`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
            "     |\n",
            "     |  n_features_in_\n",
            "     |      Number of features seen during :py:meth:`fit`.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |\n",
            "     |  __annotations__ = {}\n",
            "     |\n",
            "     |  __slotnames__ = []\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |\n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |\n",
            "     |  __repr__(self, N_CHAR_MAX=700)\n",
            "     |      Return repr(self).\n",
            "     |\n",
            "     |  __setstate__(self, state)\n",
            "     |\n",
            "     |  __sklearn_clone__(self)\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  get_metadata_routing(self)\n",
            "     |      Get metadata routing of this object.\n",
            "     |\n",
            "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      routing : MetadataRequest\n",
            "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            "     |          routing information.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  __init_subclass__(**kwargs)\n",
            "     |      Set the ``set_{method}_request`` methods.\n",
            "     |\n",
            "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            "     |      looks for the information available in the set default values which are\n",
            "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            "     |      from method signatures.\n",
            "     |\n",
            "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
            "     |      does not explicitly accept a metadata through its arguments or if the\n",
            "     |      developer would like to specify a request value for those metadata\n",
            "     |      which are different from the default ``None``.\n",
            "     |\n",
            "     |      References\n",
            "     |      ----------\n",
            "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n",
            "    class XGBRFClassifier(XGBClassifier)\n",
            "     |  XGBRFClassifier(*, learning_rate: float = 1.0, subsample: float = 0.8, colsample_bynode: float = 0.8, reg_lambda: float = 1e-05, **kwargs: Any)\n",
            "     |\n",
            "     |  scikit-learn API for XGBoost random forest classification.\n",
            "     |  See :doc:`/python/sklearn_estimator` for more information.\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |\n",
            "     |      n_estimators : Optional[int]\n",
            "     |          Number of trees in random forest to fit.\n",
            "     |\n",
            "     |      max_depth :  typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum tree depth for base learners.\n",
            "     |\n",
            "     |      max_leaves : typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum number of leaves; 0 indicates no limit.\n",
            "     |\n",
            "     |      max_bin : typing.Optional[int]\n",
            "     |\n",
            "     |          If using histogram-based algorithm, maximum number of bins per feature\n",
            "     |\n",
            "     |      grow_policy : typing.Optional[str]\n",
            "     |\n",
            "     |          Tree growing policy.\n",
            "     |\n",
            "     |          - depthwise: Favors splitting at nodes closest to the node,\n",
            "     |          - lossguide: Favors splitting at nodes with highest loss change.\n",
            "     |\n",
            "     |      learning_rate : typing.Optional[float]\n",
            "     |\n",
            "     |          Boosting learning rate (xgb's \"eta\")\n",
            "     |\n",
            "     |      verbosity : typing.Optional[int]\n",
            "     |\n",
            "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
            "     |\n",
            "     |      objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
            "     |\n",
            "     |          Specify the learning task and the corresponding learning objective or a custom\n",
            "     |          objective function to be used.\n",
            "     |\n",
            "     |          For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
            "     |          :ref:`custom-obj-metric` for more information, along with the end note for\n",
            "     |          function signatures.\n",
            "     |\n",
            "     |      booster: typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
            "     |\n",
            "     |      tree_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
            "     |          default, XGBoost will choose the most conservative option available.  It's\n",
            "     |          recommended to study this option from the parameters document :doc:`tree method\n",
            "     |          </treemethod>`\n",
            "     |\n",
            "     |      n_jobs : typing.Optional[int]\n",
            "     |\n",
            "     |          Number of parallel threads used to run xgboost.  When used with other\n",
            "     |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
            "     |          parallelize and balance the threads.  Creating thread contention will\n",
            "     |          significantly slow down both algorithms.\n",
            "     |\n",
            "     |      gamma : typing.Optional[float]\n",
            "     |\n",
            "     |          (min_split_loss) Minimum loss reduction required to make a further partition on\n",
            "     |          a leaf node of the tree.\n",
            "     |\n",
            "     |      min_child_weight : typing.Optional[float]\n",
            "     |\n",
            "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
            "     |\n",
            "     |      max_delta_step : typing.Optional[float]\n",
            "     |\n",
            "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
            "     |\n",
            "     |      subsample : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of the training instance.\n",
            "     |\n",
            "     |      sampling_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
            "     |\n",
            "     |          - ``uniform``: Select random training instances uniformly.\n",
            "     |          - ``gradient_based``: Select random training instances with higher probability\n",
            "     |              when the gradient and hessian are larger. (cf. CatBoost)\n",
            "     |\n",
            "     |      colsample_bytree : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns when constructing each tree.\n",
            "     |\n",
            "     |      colsample_bylevel : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each level.\n",
            "     |\n",
            "     |      colsample_bynode : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each split.\n",
            "     |\n",
            "     |      reg_alpha : typing.Optional[float]\n",
            "     |\n",
            "     |          L1 regularization term on weights (xgb's alpha).\n",
            "     |\n",
            "     |      reg_lambda : typing.Optional[float]\n",
            "     |\n",
            "     |          L2 regularization term on weights (xgb's lambda).\n",
            "     |\n",
            "     |      scale_pos_weight : typing.Optional[float]\n",
            "     |          Balancing of positive and negative weights.\n",
            "     |\n",
            "     |      base_score : typing.Union[float, typing.List[float], NoneType]\n",
            "     |\n",
            "     |          The initial prediction score of all instances, global bias.\n",
            "     |\n",
            "     |      random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
            "     |\n",
            "     |          Random number seed.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
            "     |             it uses Hogwild algorithm.\n",
            "     |\n",
            "     |      missing : float\n",
            "     |\n",
            "     |          Value in the data which needs to be present as a missing value. Default to\n",
            "     |          :py:data:`numpy.nan`.\n",
            "     |\n",
            "     |      num_parallel_tree: typing.Optional[int]\n",
            "     |\n",
            "     |          Used for boosting random forest.\n",
            "     |\n",
            "     |      monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
            "     |\n",
            "     |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
            "     |          for more information.\n",
            "     |\n",
            "     |      interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
            "     |\n",
            "     |          Constraints for interaction representing permitted interactions.  The\n",
            "     |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
            "     |          3, 4]]``, where each inner list is a group of indices of features that are\n",
            "     |          allowed to interact with each other.  See :doc:`tutorial\n",
            "     |          </tutorials/feature_interaction_constraint>` for more information\n",
            "     |\n",
            "     |      importance_type: typing.Optional[str]\n",
            "     |\n",
            "     |          The feature importance type for the feature_importances\\_ property:\n",
            "     |\n",
            "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
            "     |            \"total_cover\".\n",
            "     |          * For linear model, only \"weight\" is defined and it's the normalized\n",
            "     |            coefficients without bias.\n",
            "     |\n",
            "     |      device : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
            "     |\n",
            "     |      validate_parameters : typing.Optional[bool]\n",
            "     |\n",
            "     |          Give warnings for unknown parameter.\n",
            "     |\n",
            "     |      enable_categorical : bool\n",
            "     |\n",
            "     |          See the same parameter of :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_types : typing.Optional[typing.Sequence[str]]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          Used for specifying feature types without constructing a dataframe. See\n",
            "     |          the :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_weights : Optional[ArrayLike]\n",
            "     |\n",
            "     |          Weight for each feature, defines the probability of each feature being selected\n",
            "     |          when colsample is being used.  All values must be greater than 0, otherwise a\n",
            "     |          `ValueError` is thrown.\n",
            "     |\n",
            "     |      max_cat_to_onehot : Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
            "     |          for categorical data.  When number of categories is lesser than the threshold\n",
            "     |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
            "     |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
            "     |          categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      max_cat_threshold : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          Maximum number of categories considered for each split. Used only by\n",
            "     |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
            "     |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      multi_strategy : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          .. note:: This parameter is working-in-progress.\n",
            "     |\n",
            "     |          The strategy used for training multi-target models, including multi-target\n",
            "     |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
            "     |          more information.\n",
            "     |\n",
            "     |          - ``one_output_per_tree``: One model for each target.\n",
            "     |          - ``multi_output_tree``:  Use multi-target trees.\n",
            "     |\n",
            "     |      eval_metric : typing.Union[str, typing.List[typing.Union[str, typing.Callable]], typing.Callable, NoneType]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          Metric used for monitoring the training result and early stopping.  It can be a\n",
            "     |          string or list of strings as names of predefined metric in XGBoost (See\n",
            "     |          :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
            "     |          other user defined metric that looks like `sklearn.metrics`.\n",
            "     |\n",
            "     |          If custom objective is also provided, then custom metric should implement the\n",
            "     |          corresponding reverse link function.\n",
            "     |\n",
            "     |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
            "     |          object is provided, it's assumed to be a cost function and by default XGBoost\n",
            "     |          will minimize the result during early stopping.\n",
            "     |\n",
            "     |          For advanced usage on Early stopping like directly choosing to maximize instead\n",
            "     |          of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
            "     |          information.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              from sklearn.datasets import load_diabetes\n",
            "     |              from sklearn.metrics import mean_absolute_error\n",
            "     |              X, y = load_diabetes(return_X_y=True)\n",
            "     |              reg = xgb.XGBRegressor(\n",
            "     |                  tree_method=\"hist\",\n",
            "     |                  eval_metric=mean_absolute_error,\n",
            "     |              )\n",
            "     |              reg.fit(X, y, eval_set=[(X, y)])\n",
            "     |\n",
            "     |      early_stopping_rounds : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          - Activates early stopping. Validation metric needs to improve at least once in\n",
            "     |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
            "     |            least one item in **eval_set** in :py:meth:`fit`.\n",
            "     |\n",
            "     |          - If early stopping occurs, the model will have two additional attributes:\n",
            "     |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
            "     |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
            "     |            number of trees during inference. If users want to access the full model\n",
            "     |            (including trees built after early stopping), they can specify the\n",
            "     |            `iteration_range` in these inference methods. In addition, other utilities\n",
            "     |            like model plotting can also use the entire model.\n",
            "     |\n",
            "     |          - If you prefer to discard the trees after `best_iteration`, consider using the\n",
            "     |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          - If there's more than one item in **eval_set**, the last entry will be used for\n",
            "     |            early stopping.  If there's more than one metric in **eval_metric**, the last\n",
            "     |            metric will be used for early stopping.\n",
            "     |\n",
            "     |      callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
            "     |\n",
            "     |          List of callback functions that are applied at end of each iteration.\n",
            "     |          It is possible to use predefined callbacks by using\n",
            "     |          :ref:`Callback API <callback_api>`.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             States in callback are not preserved during training, which means callback\n",
            "     |             objects can not be reused for multiple training sessions without\n",
            "     |             reinitialization or deepcopy.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              for params in parameters_grid:\n",
            "     |                  # be sure to (re)initialize the callbacks before each run\n",
            "     |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            "     |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
            "     |                  reg.fit(X, y)\n",
            "     |\n",
            "     |      kwargs : typing.Optional[typing.Any]\n",
            "     |\n",
            "     |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
            "     |          can be found :doc:`here </parameter>`.\n",
            "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
            "     |          dict simultaneously will result in a TypeError.\n",
            "     |\n",
            "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
            "     |\n",
            "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
            "     |              that parameters passed via this argument will interact properly\n",
            "     |              with scikit-learn.\n",
            "     |\n",
            "     |          .. note::  Custom objective function\n",
            "     |\n",
            "     |              A custom objective function can be provided for the ``objective``\n",
            "     |              parameter. In this case, it should have the signature ``objective(y_true,\n",
            "     |              y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
            "     |              -> [grad, hess]``:\n",
            "     |\n",
            "     |              y_true: array_like of shape [n_samples]\n",
            "     |                  The target values\n",
            "     |              y_pred: array_like of shape [n_samples]\n",
            "     |                  The predicted values\n",
            "     |              sample_weight :\n",
            "     |                  Optional sample weights.\n",
            "     |\n",
            "     |              grad: array_like of shape [n_samples]\n",
            "     |                  The value of the gradient for each sample point.\n",
            "     |              hess: array_like of shape [n_samples]\n",
            "     |                  The value of the second derivative for each sample point\n",
            "     |\n",
            "     |              Note that, if the custom objective produces negative values for\n",
            "     |              the Hessian, these will be clipped. If the objective is non-convex,\n",
            "     |              one might also consider using the expected Hessian (Fisher\n",
            "     |              information) instead.\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      XGBRFClassifier\n",
            "     |      XGBClassifier\n",
            "     |      sklearn.base.ClassifierMixin\n",
            "     |      XGBModel\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __init__(self, *, learning_rate: float = 1.0, subsample: float = 0.8, colsample_bynode: float = 0.8, reg_lambda: float = 1e-05, **kwargs: Any)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |\n",
            "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None) -> 'XGBRFClassifier'\n",
            "     |      Fit gradient boosting classifier.\n",
            "     |\n",
            "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
            "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
            "     |      pass ``xgb_model`` argument.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |          When the ``tree_method`` is set to ``hist``, internally, the\n",
            "     |          :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n",
            "     |          for conserving memory. However, this has performance implications when the\n",
            "     |          device of input data is not matched with algorithm. For instance, if the\n",
            "     |          input is a numpy array on CPU but ``cuda`` is used for training, then the\n",
            "     |          data is first processed on CPU then transferred to GPU.\n",
            "     |      y :\n",
            "     |          Labels\n",
            "     |      sample_weight :\n",
            "     |          instance weights\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      eval_set :\n",
            "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
            "     |          metrics will be computed.\n",
            "     |          Validation metrics will help us track the performance of the model.\n",
            "     |\n",
            "     |      verbose :\n",
            "     |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
            "     |          measured on the validation set is printed to stdout at each boosting stage.\n",
            "     |          If `verbose` is an integer, the evaluation metric is printed at each\n",
            "     |          `verbose` boosting stage. The last boosting stage / the boosting stage found\n",
            "     |          by using `early_stopping_rounds` is also printed.\n",
            "     |      xgb_model :\n",
            "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
            "     |          loaded before training (allows training continuation).\n",
            "     |      sample_weight_eval_set :\n",
            "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
            "     |          object storing instance weights for the i-th validation set.\n",
            "     |      base_margin_eval_set :\n",
            "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
            "     |          object storing base margin for the i-th validation set.\n",
            "     |      feature_weights :\n",
            "     |\n",
            "     |          .. deprecated:: 3.0.0\n",
            "     |\n",
            "     |          Use `feature_weights` in :py:meth:`__init__` or :py:meth:`set_params`\n",
            "     |          instead.\n",
            "     |\n",
            "     |  get_num_boosting_rounds(self) -> int\n",
            "     |      Gets the number of xgboost boosting rounds.\n",
            "     |\n",
            "     |  get_xgb_params(self) -> Dict[str, Any]\n",
            "     |      Get xgboost specific parameters.\n",
            "     |\n",
            "     |  set_fit_request(self: xgboost.sklearn.XGBRFClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', base_margin_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_weights: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', verbose: Union[bool, NoneType, str] = '$UNCHANGED$', xgb_model: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRFClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``fit`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``fit``.\n",
            "     |\n",
            "     |      base_margin_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      feature_weights : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``feature_weights`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      verbose : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``verbose`` parameter in ``fit``.\n",
            "     |\n",
            "     |      xgb_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``xgb_model`` parameter in ``fit``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_predict_proba_request(self: xgboost.sklearn.XGBRFClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRFClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``predict_proba`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``predict_proba`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict_proba``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``predict_proba``.\n",
            "     |\n",
            "     |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``iteration_range`` parameter in ``predict_proba``.\n",
            "     |\n",
            "     |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``validate_features`` parameter in ``predict_proba``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_predict_request(self: xgboost.sklearn.XGBRFClassifier, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', output_margin: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRFClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``predict`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``iteration_range`` parameter in ``predict``.\n",
            "     |\n",
            "     |      output_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``output_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``validate_features`` parameter in ``predict``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_score_request(self: xgboost.sklearn.XGBRFClassifier, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRFClassifier from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``score`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |\n",
            "     |  __annotations__ = {}\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from XGBClassifier:\n",
            "     |\n",
            "     |  __sklearn_tags__(self) -> sklearn.utils._tags.Tags\n",
            "     |\n",
            "     |  predict(self, X: Any, *, output_margin: bool = False, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> Any\n",
            "     |      Predict with `X`.  If the model is trained with early stopping, then\n",
            "     |      :py:attr:`best_iteration` is used automatically. The estimator uses\n",
            "     |      `inplace_predict` by default and falls back to using :py:class:`DMatrix` if\n",
            "     |      devices between the data and the estimator don't match.\n",
            "     |\n",
            "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Data to predict with. See :ref:`py-data` for a list of supported types.\n",
            "     |      output_margin :\n",
            "     |          Whether to output the raw untransformed margin value.\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's feature_names are\n",
            "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
            "     |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
            "     |          are used in this prediction.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction\n",
            "     |\n",
            "     |  predict_proba(self, X: Any, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n",
            "     |      Predict the probability of each `X` example being of a given class. If the\n",
            "     |      model is trained with early stopping, then :py:attr:`best_iteration` is used\n",
            "     |      automatically. The estimator uses `inplace_predict` by default and falls back to\n",
            "     |      using :py:class:`DMatrix` if devices between the data and the estimator don't\n",
            "     |      match.\n",
            "     |\n",
            "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's feature_names are\n",
            "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying `iteration_range=(10,\n",
            "     |          20)`, then only the forests built during [10, 20) (half open set) rounds are\n",
            "     |          used in this prediction.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction :\n",
            "     |          a numpy array of shape array-like of shape (n_samples, n_classes) with the\n",
            "     |          probability of each data example being of a given class.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from XGBClassifier:\n",
            "     |\n",
            "     |  classes_\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from XGBClassifier:\n",
            "     |\n",
            "     |  __slotnames__ = []\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            "     |\n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Return the mean accuracy on the given test data and labels.\n",
            "     |\n",
            "     |      In multi-label classification, this is the subset accuracy\n",
            "     |      which is a harsh metric since you require for each sample that\n",
            "     |      each label set be correctly predicted.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like of shape (n_samples, n_features)\n",
            "     |          Test samples.\n",
            "     |\n",
            "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            "     |          True labels for `X`.\n",
            "     |\n",
            "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
            "     |          Sample weights.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from XGBModel:\n",
            "     |\n",
            "     |  __sklearn_is_fitted__(self) -> bool\n",
            "     |\n",
            "     |  apply(self, X: Any, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n",
            "     |      Return the predicted leaf every tree for each sample. If the model is trained\n",
            "     |      with early stopping, then :py:attr:`best_iteration` is used automatically.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input features matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |      iteration_range :\n",
            "     |          See :py:meth:`predict`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
            "     |          For each datapoint x in X and for each tree, return the index of the\n",
            "     |          leaf x ends up in. Leaves are numbered within\n",
            "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
            "     |\n",
            "     |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
            "     |      Return the evaluation results.\n",
            "     |\n",
            "     |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
            "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
            "     |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
            "     |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
            "     |      function.\n",
            "     |\n",
            "     |      The returned evaluation result is a dictionary:\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
            "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      evals_result\n",
            "     |\n",
            "     |  get_booster(self) -> xgboost.core.Booster\n",
            "     |      Get the underlying xgboost Booster of this model.\n",
            "     |\n",
            "     |      This will raise an exception when fit was not called\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      booster : a xgboost booster of underlying model\n",
            "     |\n",
            "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
            "     |      Get parameters.\n",
            "     |\n",
            "     |  load_model(self, fname: Union[os.PathLike[~AnyStr], bytearray, str]) -> None\n",
            "     |      Load the model from a file or a bytearray.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        model.load_model(\"model.json\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |        model.load_model(\"model.ubj\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        buf = model.save_raw()\n",
            "     |        model.load_model(buf)\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Input file name or memory buffer(see also save_raw)\n",
            "     |\n",
            "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
            "     |      Save the model to a file.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Output file name\n",
            "     |\n",
            "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
            "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
            "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
            "     |      parameters that are not defined as member variables in sklearn grid\n",
            "     |      search.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from XGBModel:\n",
            "     |\n",
            "     |  best_iteration\n",
            "     |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
            "     |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
            "     |\n",
            "     |  best_score\n",
            "     |      The best score obtained by early stopping.\n",
            "     |\n",
            "     |  coef_\n",
            "     |      Coefficients property\n",
            "     |\n",
            "     |      .. note:: Coefficients are defined only for linear learners\n",
            "     |\n",
            "     |          Coefficients are only defined when the linear model is chosen as\n",
            "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
            "     |          learner types, such as tree learners (`booster=gbtree`).\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
            "     |\n",
            "     |  feature_importances_\n",
            "     |      Feature importances property, return depends on `importance_type`\n",
            "     |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
            "     |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
            "     |      based on the importance type. For instance, if the importance type is\n",
            "     |      \"total_gain\", then the score is sum of loss change for each split from all\n",
            "     |      trees.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
            "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
            "     |\n",
            "     |  feature_names_in_\n",
            "     |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has\n",
            "     |      feature names that are all strings.\n",
            "     |\n",
            "     |  intercept_\n",
            "     |      Intercept (bias) property\n",
            "     |\n",
            "     |      For tree-based model, the returned value is the `base_score`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
            "     |\n",
            "     |  n_features_in_\n",
            "     |      Number of features seen during :py:meth:`fit`.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |\n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |\n",
            "     |  __repr__(self, N_CHAR_MAX=700)\n",
            "     |      Return repr(self).\n",
            "     |\n",
            "     |  __setstate__(self, state)\n",
            "     |\n",
            "     |  __sklearn_clone__(self)\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  get_metadata_routing(self)\n",
            "     |      Get metadata routing of this object.\n",
            "     |\n",
            "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      routing : MetadataRequest\n",
            "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            "     |          routing information.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  __init_subclass__(**kwargs)\n",
            "     |      Set the ``set_{method}_request`` methods.\n",
            "     |\n",
            "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            "     |      looks for the information available in the set default values which are\n",
            "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            "     |      from method signatures.\n",
            "     |\n",
            "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
            "     |      does not explicitly accept a metadata through its arguments or if the\n",
            "     |      developer would like to specify a request value for those metadata\n",
            "     |      which are different from the default ``None``.\n",
            "     |\n",
            "     |      References\n",
            "     |      ----------\n",
            "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n",
            "    class XGBRFRegressor(XGBRegressor)\n",
            "     |  XGBRFRegressor(*, learning_rate: float = 1.0, subsample: float = 0.8, colsample_bynode: float = 0.8, reg_lambda: float = 1e-05, **kwargs: Any) -> None\n",
            "     |\n",
            "     |  scikit-learn API for XGBoost random forest regression.\n",
            "     |  See :doc:`/python/sklearn_estimator` for more information.\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |\n",
            "     |      n_estimators : Optional[int]\n",
            "     |          Number of trees in random forest to fit.\n",
            "     |\n",
            "     |      max_depth :  typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum tree depth for base learners.\n",
            "     |\n",
            "     |      max_leaves : typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum number of leaves; 0 indicates no limit.\n",
            "     |\n",
            "     |      max_bin : typing.Optional[int]\n",
            "     |\n",
            "     |          If using histogram-based algorithm, maximum number of bins per feature\n",
            "     |\n",
            "     |      grow_policy : typing.Optional[str]\n",
            "     |\n",
            "     |          Tree growing policy.\n",
            "     |\n",
            "     |          - depthwise: Favors splitting at nodes closest to the node,\n",
            "     |          - lossguide: Favors splitting at nodes with highest loss change.\n",
            "     |\n",
            "     |      learning_rate : typing.Optional[float]\n",
            "     |\n",
            "     |          Boosting learning rate (xgb's \"eta\")\n",
            "     |\n",
            "     |      verbosity : typing.Optional[int]\n",
            "     |\n",
            "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
            "     |\n",
            "     |      objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
            "     |\n",
            "     |          Specify the learning task and the corresponding learning objective or a custom\n",
            "     |          objective function to be used.\n",
            "     |\n",
            "     |          For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
            "     |          :ref:`custom-obj-metric` for more information, along with the end note for\n",
            "     |          function signatures.\n",
            "     |\n",
            "     |      booster: typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
            "     |\n",
            "     |      tree_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
            "     |          default, XGBoost will choose the most conservative option available.  It's\n",
            "     |          recommended to study this option from the parameters document :doc:`tree method\n",
            "     |          </treemethod>`\n",
            "     |\n",
            "     |      n_jobs : typing.Optional[int]\n",
            "     |\n",
            "     |          Number of parallel threads used to run xgboost.  When used with other\n",
            "     |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
            "     |          parallelize and balance the threads.  Creating thread contention will\n",
            "     |          significantly slow down both algorithms.\n",
            "     |\n",
            "     |      gamma : typing.Optional[float]\n",
            "     |\n",
            "     |          (min_split_loss) Minimum loss reduction required to make a further partition on\n",
            "     |          a leaf node of the tree.\n",
            "     |\n",
            "     |      min_child_weight : typing.Optional[float]\n",
            "     |\n",
            "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
            "     |\n",
            "     |      max_delta_step : typing.Optional[float]\n",
            "     |\n",
            "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
            "     |\n",
            "     |      subsample : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of the training instance.\n",
            "     |\n",
            "     |      sampling_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
            "     |\n",
            "     |          - ``uniform``: Select random training instances uniformly.\n",
            "     |          - ``gradient_based``: Select random training instances with higher probability\n",
            "     |              when the gradient and hessian are larger. (cf. CatBoost)\n",
            "     |\n",
            "     |      colsample_bytree : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns when constructing each tree.\n",
            "     |\n",
            "     |      colsample_bylevel : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each level.\n",
            "     |\n",
            "     |      colsample_bynode : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each split.\n",
            "     |\n",
            "     |      reg_alpha : typing.Optional[float]\n",
            "     |\n",
            "     |          L1 regularization term on weights (xgb's alpha).\n",
            "     |\n",
            "     |      reg_lambda : typing.Optional[float]\n",
            "     |\n",
            "     |          L2 regularization term on weights (xgb's lambda).\n",
            "     |\n",
            "     |      scale_pos_weight : typing.Optional[float]\n",
            "     |          Balancing of positive and negative weights.\n",
            "     |\n",
            "     |      base_score : typing.Union[float, typing.List[float], NoneType]\n",
            "     |\n",
            "     |          The initial prediction score of all instances, global bias.\n",
            "     |\n",
            "     |      random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
            "     |\n",
            "     |          Random number seed.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
            "     |             it uses Hogwild algorithm.\n",
            "     |\n",
            "     |      missing : float\n",
            "     |\n",
            "     |          Value in the data which needs to be present as a missing value. Default to\n",
            "     |          :py:data:`numpy.nan`.\n",
            "     |\n",
            "     |      num_parallel_tree: typing.Optional[int]\n",
            "     |\n",
            "     |          Used for boosting random forest.\n",
            "     |\n",
            "     |      monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
            "     |\n",
            "     |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
            "     |          for more information.\n",
            "     |\n",
            "     |      interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
            "     |\n",
            "     |          Constraints for interaction representing permitted interactions.  The\n",
            "     |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
            "     |          3, 4]]``, where each inner list is a group of indices of features that are\n",
            "     |          allowed to interact with each other.  See :doc:`tutorial\n",
            "     |          </tutorials/feature_interaction_constraint>` for more information\n",
            "     |\n",
            "     |      importance_type: typing.Optional[str]\n",
            "     |\n",
            "     |          The feature importance type for the feature_importances\\_ property:\n",
            "     |\n",
            "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
            "     |            \"total_cover\".\n",
            "     |          * For linear model, only \"weight\" is defined and it's the normalized\n",
            "     |            coefficients without bias.\n",
            "     |\n",
            "     |      device : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
            "     |\n",
            "     |      validate_parameters : typing.Optional[bool]\n",
            "     |\n",
            "     |          Give warnings for unknown parameter.\n",
            "     |\n",
            "     |      enable_categorical : bool\n",
            "     |\n",
            "     |          See the same parameter of :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_types : typing.Optional[typing.Sequence[str]]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          Used for specifying feature types without constructing a dataframe. See\n",
            "     |          the :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_weights : Optional[ArrayLike]\n",
            "     |\n",
            "     |          Weight for each feature, defines the probability of each feature being selected\n",
            "     |          when colsample is being used.  All values must be greater than 0, otherwise a\n",
            "     |          `ValueError` is thrown.\n",
            "     |\n",
            "     |      max_cat_to_onehot : Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
            "     |          for categorical data.  When number of categories is lesser than the threshold\n",
            "     |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
            "     |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
            "     |          categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      max_cat_threshold : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          Maximum number of categories considered for each split. Used only by\n",
            "     |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
            "     |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      multi_strategy : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          .. note:: This parameter is working-in-progress.\n",
            "     |\n",
            "     |          The strategy used for training multi-target models, including multi-target\n",
            "     |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
            "     |          more information.\n",
            "     |\n",
            "     |          - ``one_output_per_tree``: One model for each target.\n",
            "     |          - ``multi_output_tree``:  Use multi-target trees.\n",
            "     |\n",
            "     |      eval_metric : typing.Union[str, typing.List[typing.Union[str, typing.Callable]], typing.Callable, NoneType]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          Metric used for monitoring the training result and early stopping.  It can be a\n",
            "     |          string or list of strings as names of predefined metric in XGBoost (See\n",
            "     |          :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
            "     |          other user defined metric that looks like `sklearn.metrics`.\n",
            "     |\n",
            "     |          If custom objective is also provided, then custom metric should implement the\n",
            "     |          corresponding reverse link function.\n",
            "     |\n",
            "     |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
            "     |          object is provided, it's assumed to be a cost function and by default XGBoost\n",
            "     |          will minimize the result during early stopping.\n",
            "     |\n",
            "     |          For advanced usage on Early stopping like directly choosing to maximize instead\n",
            "     |          of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
            "     |          information.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              from sklearn.datasets import load_diabetes\n",
            "     |              from sklearn.metrics import mean_absolute_error\n",
            "     |              X, y = load_diabetes(return_X_y=True)\n",
            "     |              reg = xgb.XGBRegressor(\n",
            "     |                  tree_method=\"hist\",\n",
            "     |                  eval_metric=mean_absolute_error,\n",
            "     |              )\n",
            "     |              reg.fit(X, y, eval_set=[(X, y)])\n",
            "     |\n",
            "     |      early_stopping_rounds : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          - Activates early stopping. Validation metric needs to improve at least once in\n",
            "     |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
            "     |            least one item in **eval_set** in :py:meth:`fit`.\n",
            "     |\n",
            "     |          - If early stopping occurs, the model will have two additional attributes:\n",
            "     |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
            "     |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
            "     |            number of trees during inference. If users want to access the full model\n",
            "     |            (including trees built after early stopping), they can specify the\n",
            "     |            `iteration_range` in these inference methods. In addition, other utilities\n",
            "     |            like model plotting can also use the entire model.\n",
            "     |\n",
            "     |          - If you prefer to discard the trees after `best_iteration`, consider using the\n",
            "     |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          - If there's more than one item in **eval_set**, the last entry will be used for\n",
            "     |            early stopping.  If there's more than one metric in **eval_metric**, the last\n",
            "     |            metric will be used for early stopping.\n",
            "     |\n",
            "     |      callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
            "     |\n",
            "     |          List of callback functions that are applied at end of each iteration.\n",
            "     |          It is possible to use predefined callbacks by using\n",
            "     |          :ref:`Callback API <callback_api>`.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             States in callback are not preserved during training, which means callback\n",
            "     |             objects can not be reused for multiple training sessions without\n",
            "     |             reinitialization or deepcopy.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              for params in parameters_grid:\n",
            "     |                  # be sure to (re)initialize the callbacks before each run\n",
            "     |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            "     |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
            "     |                  reg.fit(X, y)\n",
            "     |\n",
            "     |      kwargs : typing.Optional[typing.Any]\n",
            "     |\n",
            "     |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
            "     |          can be found :doc:`here </parameter>`.\n",
            "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
            "     |          dict simultaneously will result in a TypeError.\n",
            "     |\n",
            "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
            "     |\n",
            "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
            "     |              that parameters passed via this argument will interact properly\n",
            "     |              with scikit-learn.\n",
            "     |\n",
            "     |          .. note::  Custom objective function\n",
            "     |\n",
            "     |              A custom objective function can be provided for the ``objective``\n",
            "     |              parameter. In this case, it should have the signature ``objective(y_true,\n",
            "     |              y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
            "     |              -> [grad, hess]``:\n",
            "     |\n",
            "     |              y_true: array_like of shape [n_samples]\n",
            "     |                  The target values\n",
            "     |              y_pred: array_like of shape [n_samples]\n",
            "     |                  The predicted values\n",
            "     |              sample_weight :\n",
            "     |                  Optional sample weights.\n",
            "     |\n",
            "     |              grad: array_like of shape [n_samples]\n",
            "     |                  The value of the gradient for each sample point.\n",
            "     |              hess: array_like of shape [n_samples]\n",
            "     |                  The value of the second derivative for each sample point\n",
            "     |\n",
            "     |              Note that, if the custom objective produces negative values for\n",
            "     |              the Hessian, these will be clipped. If the objective is non-convex,\n",
            "     |              one might also consider using the expected Hessian (Fisher\n",
            "     |              information) instead.\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      XGBRFRegressor\n",
            "     |      XGBRegressor\n",
            "     |      sklearn.base.RegressorMixin\n",
            "     |      XGBModel\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __init__(self, *, learning_rate: float = 1.0, subsample: float = 0.8, colsample_bynode: float = 0.8, reg_lambda: float = 1e-05, **kwargs: Any) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |\n",
            "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None) -> 'XGBRFRegressor'\n",
            "     |      Fit gradient boosting model.\n",
            "     |\n",
            "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
            "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
            "     |      pass ``xgb_model`` argument.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |          When the ``tree_method`` is set to ``hist``, internally, the\n",
            "     |          :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n",
            "     |          for conserving memory. However, this has performance implications when the\n",
            "     |          device of input data is not matched with algorithm. For instance, if the\n",
            "     |          input is a numpy array on CPU but ``cuda`` is used for training, then the\n",
            "     |          data is first processed on CPU then transferred to GPU.\n",
            "     |      y :\n",
            "     |          Labels\n",
            "     |      sample_weight :\n",
            "     |          instance weights\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      eval_set :\n",
            "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
            "     |          metrics will be computed.\n",
            "     |          Validation metrics will help us track the performance of the model.\n",
            "     |\n",
            "     |      verbose :\n",
            "     |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
            "     |          measured on the validation set is printed to stdout at each boosting stage.\n",
            "     |          If `verbose` is an integer, the evaluation metric is printed at each\n",
            "     |          `verbose` boosting stage. The last boosting stage / the boosting stage found\n",
            "     |          by using `early_stopping_rounds` is also printed.\n",
            "     |      xgb_model :\n",
            "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
            "     |          loaded before training (allows training continuation).\n",
            "     |      sample_weight_eval_set :\n",
            "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
            "     |          object storing instance weights for the i-th validation set.\n",
            "     |      base_margin_eval_set :\n",
            "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
            "     |          object storing base margin for the i-th validation set.\n",
            "     |      feature_weights :\n",
            "     |\n",
            "     |          .. deprecated:: 3.0.0\n",
            "     |\n",
            "     |          Use `feature_weights` in :py:meth:`__init__` or :py:meth:`set_params`\n",
            "     |          instead.\n",
            "     |\n",
            "     |  get_num_boosting_rounds(self) -> int\n",
            "     |      Gets the number of xgboost boosting rounds.\n",
            "     |\n",
            "     |  get_xgb_params(self) -> Dict[str, Any]\n",
            "     |      Get xgboost specific parameters.\n",
            "     |\n",
            "     |  set_fit_request(self: xgboost.sklearn.XGBRFRegressor, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', base_margin_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_weights: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', verbose: Union[bool, NoneType, str] = '$UNCHANGED$', xgb_model: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRFRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``fit`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``fit``.\n",
            "     |\n",
            "     |      base_margin_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      feature_weights : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``feature_weights`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      verbose : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``verbose`` parameter in ``fit``.\n",
            "     |\n",
            "     |      xgb_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``xgb_model`` parameter in ``fit``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_predict_request(self: xgboost.sklearn.XGBRFRegressor, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', output_margin: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRFRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``predict`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``iteration_range`` parameter in ``predict``.\n",
            "     |\n",
            "     |      output_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``output_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``validate_features`` parameter in ``predict``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_score_request(self: xgboost.sklearn.XGBRFRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRFRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``score`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |\n",
            "     |  __annotations__ = {}\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from XGBRegressor:\n",
            "     |\n",
            "     |  __sklearn_tags__(self) -> sklearn.utils._tags.Tags\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
            "     |\n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Return the coefficient of determination of the prediction.\n",
            "     |\n",
            "     |      The coefficient of determination :math:`R^2` is defined as\n",
            "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
            "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
            "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
            "     |      The best possible score is 1.0 and it can be negative (because the\n",
            "     |      model can be arbitrarily worse). A constant model that always predicts\n",
            "     |      the expected value of `y`, disregarding the input features, would get\n",
            "     |      a :math:`R^2` score of 0.0.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like of shape (n_samples, n_features)\n",
            "     |          Test samples. For some estimators this may be a precomputed\n",
            "     |          kernel matrix or a list of generic objects instead with shape\n",
            "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
            "     |          is the number of samples used in the fitting for the estimator.\n",
            "     |\n",
            "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            "     |          True values for `X`.\n",
            "     |\n",
            "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
            "     |          Sample weights.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
            "     |\n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
            "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
            "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
            "     |      This influences the ``score`` method of all the multioutput\n",
            "     |      regressors (except for\n",
            "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from XGBModel:\n",
            "     |\n",
            "     |  __sklearn_is_fitted__(self) -> bool\n",
            "     |\n",
            "     |  apply(self, X: Any, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n",
            "     |      Return the predicted leaf every tree for each sample. If the model is trained\n",
            "     |      with early stopping, then :py:attr:`best_iteration` is used automatically.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input features matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |      iteration_range :\n",
            "     |          See :py:meth:`predict`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
            "     |          For each datapoint x in X and for each tree, return the index of the\n",
            "     |          leaf x ends up in. Leaves are numbered within\n",
            "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
            "     |\n",
            "     |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
            "     |      Return the evaluation results.\n",
            "     |\n",
            "     |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
            "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
            "     |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
            "     |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
            "     |      function.\n",
            "     |\n",
            "     |      The returned evaluation result is a dictionary:\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
            "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      evals_result\n",
            "     |\n",
            "     |  get_booster(self) -> xgboost.core.Booster\n",
            "     |      Get the underlying xgboost Booster of this model.\n",
            "     |\n",
            "     |      This will raise an exception when fit was not called\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      booster : a xgboost booster of underlying model\n",
            "     |\n",
            "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
            "     |      Get parameters.\n",
            "     |\n",
            "     |  load_model(self, fname: Union[os.PathLike[~AnyStr], bytearray, str]) -> None\n",
            "     |      Load the model from a file or a bytearray.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        model.load_model(\"model.json\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |        model.load_model(\"model.ubj\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        buf = model.save_raw()\n",
            "     |        model.load_model(buf)\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Input file name or memory buffer(see also save_raw)\n",
            "     |\n",
            "     |  predict(self, X: Any, *, output_margin: bool = False, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> Any\n",
            "     |      Predict with `X`.  If the model is trained with early stopping, then\n",
            "     |      :py:attr:`best_iteration` is used automatically. The estimator uses\n",
            "     |      `inplace_predict` by default and falls back to using :py:class:`DMatrix` if\n",
            "     |      devices between the data and the estimator don't match.\n",
            "     |\n",
            "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Data to predict with. See :ref:`py-data` for a list of supported types.\n",
            "     |      output_margin :\n",
            "     |          Whether to output the raw untransformed margin value.\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's feature_names are\n",
            "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
            "     |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
            "     |          are used in this prediction.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction\n",
            "     |\n",
            "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
            "     |      Save the model to a file.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Output file name\n",
            "     |\n",
            "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
            "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
            "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
            "     |      parameters that are not defined as member variables in sklearn grid\n",
            "     |      search.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from XGBModel:\n",
            "     |\n",
            "     |  best_iteration\n",
            "     |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
            "     |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
            "     |\n",
            "     |  best_score\n",
            "     |      The best score obtained by early stopping.\n",
            "     |\n",
            "     |  coef_\n",
            "     |      Coefficients property\n",
            "     |\n",
            "     |      .. note:: Coefficients are defined only for linear learners\n",
            "     |\n",
            "     |          Coefficients are only defined when the linear model is chosen as\n",
            "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
            "     |          learner types, such as tree learners (`booster=gbtree`).\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
            "     |\n",
            "     |  feature_importances_\n",
            "     |      Feature importances property, return depends on `importance_type`\n",
            "     |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
            "     |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
            "     |      based on the importance type. For instance, if the importance type is\n",
            "     |      \"total_gain\", then the score is sum of loss change for each split from all\n",
            "     |      trees.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
            "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
            "     |\n",
            "     |  feature_names_in_\n",
            "     |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has\n",
            "     |      feature names that are all strings.\n",
            "     |\n",
            "     |  intercept_\n",
            "     |      Intercept (bias) property\n",
            "     |\n",
            "     |      For tree-based model, the returned value is the `base_score`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
            "     |\n",
            "     |  n_features_in_\n",
            "     |      Number of features seen during :py:meth:`fit`.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from XGBModel:\n",
            "     |\n",
            "     |  __slotnames__ = []\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |\n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |\n",
            "     |  __repr__(self, N_CHAR_MAX=700)\n",
            "     |      Return repr(self).\n",
            "     |\n",
            "     |  __setstate__(self, state)\n",
            "     |\n",
            "     |  __sklearn_clone__(self)\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  get_metadata_routing(self)\n",
            "     |      Get metadata routing of this object.\n",
            "     |\n",
            "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      routing : MetadataRequest\n",
            "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            "     |          routing information.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  __init_subclass__(**kwargs)\n",
            "     |      Set the ``set_{method}_request`` methods.\n",
            "     |\n",
            "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            "     |      looks for the information available in the set default values which are\n",
            "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            "     |      from method signatures.\n",
            "     |\n",
            "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
            "     |      does not explicitly accept a metadata through its arguments or if the\n",
            "     |      developer would like to specify a request value for those metadata\n",
            "     |      which are different from the default ``None``.\n",
            "     |\n",
            "     |      References\n",
            "     |      ----------\n",
            "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n",
            "    class XGBRanker(XGBRankerMixIn, XGBModel)\n",
            "     |  XGBRanker(*, objective: str = 'rank:ndcg', **kwargs: Any)\n",
            "     |\n",
            "     |  Implementation of the Scikit-Learn API for XGBoost Ranking.\n",
            "     |\n",
            "     |  See :doc:`Learning to Rank </tutorials/learning_to_rank>` for an introducion.\n",
            "     |\n",
            "     |\n",
            "     |  See :doc:`/python/sklearn_estimator` for more information.\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |\n",
            "     |      n_estimators : typing.Optional[int]\n",
            "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
            "     |          rounds.\n",
            "     |\n",
            "     |      max_depth :  typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum tree depth for base learners.\n",
            "     |\n",
            "     |      max_leaves : typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum number of leaves; 0 indicates no limit.\n",
            "     |\n",
            "     |      max_bin : typing.Optional[int]\n",
            "     |\n",
            "     |          If using histogram-based algorithm, maximum number of bins per feature\n",
            "     |\n",
            "     |      grow_policy : typing.Optional[str]\n",
            "     |\n",
            "     |          Tree growing policy.\n",
            "     |\n",
            "     |          - depthwise: Favors splitting at nodes closest to the node,\n",
            "     |          - lossguide: Favors splitting at nodes with highest loss change.\n",
            "     |\n",
            "     |      learning_rate : typing.Optional[float]\n",
            "     |\n",
            "     |          Boosting learning rate (xgb's \"eta\")\n",
            "     |\n",
            "     |      verbosity : typing.Optional[int]\n",
            "     |\n",
            "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
            "     |\n",
            "     |      objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
            "     |\n",
            "     |          Specify the learning task and the corresponding learning objective or a custom\n",
            "     |          objective function to be used.\n",
            "     |\n",
            "     |          For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
            "     |          :ref:`custom-obj-metric` for more information, along with the end note for\n",
            "     |          function signatures.\n",
            "     |\n",
            "     |      booster: typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
            "     |\n",
            "     |      tree_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
            "     |          default, XGBoost will choose the most conservative option available.  It's\n",
            "     |          recommended to study this option from the parameters document :doc:`tree method\n",
            "     |          </treemethod>`\n",
            "     |\n",
            "     |      n_jobs : typing.Optional[int]\n",
            "     |\n",
            "     |          Number of parallel threads used to run xgboost.  When used with other\n",
            "     |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
            "     |          parallelize and balance the threads.  Creating thread contention will\n",
            "     |          significantly slow down both algorithms.\n",
            "     |\n",
            "     |      gamma : typing.Optional[float]\n",
            "     |\n",
            "     |          (min_split_loss) Minimum loss reduction required to make a further partition on\n",
            "     |          a leaf node of the tree.\n",
            "     |\n",
            "     |      min_child_weight : typing.Optional[float]\n",
            "     |\n",
            "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
            "     |\n",
            "     |      max_delta_step : typing.Optional[float]\n",
            "     |\n",
            "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
            "     |\n",
            "     |      subsample : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of the training instance.\n",
            "     |\n",
            "     |      sampling_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
            "     |\n",
            "     |          - ``uniform``: Select random training instances uniformly.\n",
            "     |          - ``gradient_based``: Select random training instances with higher probability\n",
            "     |              when the gradient and hessian are larger. (cf. CatBoost)\n",
            "     |\n",
            "     |      colsample_bytree : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns when constructing each tree.\n",
            "     |\n",
            "     |      colsample_bylevel : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each level.\n",
            "     |\n",
            "     |      colsample_bynode : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each split.\n",
            "     |\n",
            "     |      reg_alpha : typing.Optional[float]\n",
            "     |\n",
            "     |          L1 regularization term on weights (xgb's alpha).\n",
            "     |\n",
            "     |      reg_lambda : typing.Optional[float]\n",
            "     |\n",
            "     |          L2 regularization term on weights (xgb's lambda).\n",
            "     |\n",
            "     |      scale_pos_weight : typing.Optional[float]\n",
            "     |          Balancing of positive and negative weights.\n",
            "     |\n",
            "     |      base_score : typing.Union[float, typing.List[float], NoneType]\n",
            "     |\n",
            "     |          The initial prediction score of all instances, global bias.\n",
            "     |\n",
            "     |      random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
            "     |\n",
            "     |          Random number seed.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
            "     |             it uses Hogwild algorithm.\n",
            "     |\n",
            "     |      missing : float\n",
            "     |\n",
            "     |          Value in the data which needs to be present as a missing value. Default to\n",
            "     |          :py:data:`numpy.nan`.\n",
            "     |\n",
            "     |      num_parallel_tree: typing.Optional[int]\n",
            "     |\n",
            "     |          Used for boosting random forest.\n",
            "     |\n",
            "     |      monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
            "     |\n",
            "     |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
            "     |          for more information.\n",
            "     |\n",
            "     |      interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
            "     |\n",
            "     |          Constraints for interaction representing permitted interactions.  The\n",
            "     |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
            "     |          3, 4]]``, where each inner list is a group of indices of features that are\n",
            "     |          allowed to interact with each other.  See :doc:`tutorial\n",
            "     |          </tutorials/feature_interaction_constraint>` for more information\n",
            "     |\n",
            "     |      importance_type: typing.Optional[str]\n",
            "     |\n",
            "     |          The feature importance type for the feature_importances\\_ property:\n",
            "     |\n",
            "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
            "     |            \"total_cover\".\n",
            "     |          * For linear model, only \"weight\" is defined and it's the normalized\n",
            "     |            coefficients without bias.\n",
            "     |\n",
            "     |      device : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
            "     |\n",
            "     |      validate_parameters : typing.Optional[bool]\n",
            "     |\n",
            "     |          Give warnings for unknown parameter.\n",
            "     |\n",
            "     |      enable_categorical : bool\n",
            "     |\n",
            "     |          See the same parameter of :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_types : typing.Optional[typing.Sequence[str]]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          Used for specifying feature types without constructing a dataframe. See\n",
            "     |          the :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_weights : Optional[ArrayLike]\n",
            "     |\n",
            "     |          Weight for each feature, defines the probability of each feature being selected\n",
            "     |          when colsample is being used.  All values must be greater than 0, otherwise a\n",
            "     |          `ValueError` is thrown.\n",
            "     |\n",
            "     |      max_cat_to_onehot : Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
            "     |          for categorical data.  When number of categories is lesser than the threshold\n",
            "     |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
            "     |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
            "     |          categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      max_cat_threshold : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          Maximum number of categories considered for each split. Used only by\n",
            "     |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
            "     |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      multi_strategy : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          .. note:: This parameter is working-in-progress.\n",
            "     |\n",
            "     |          The strategy used for training multi-target models, including multi-target\n",
            "     |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
            "     |          more information.\n",
            "     |\n",
            "     |          - ``one_output_per_tree``: One model for each target.\n",
            "     |          - ``multi_output_tree``:  Use multi-target trees.\n",
            "     |\n",
            "     |      eval_metric : typing.Union[str, typing.List[typing.Union[str, typing.Callable]], typing.Callable, NoneType]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          Metric used for monitoring the training result and early stopping.  It can be a\n",
            "     |          string or list of strings as names of predefined metric in XGBoost (See\n",
            "     |          :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
            "     |          other user defined metric that looks like `sklearn.metrics`.\n",
            "     |\n",
            "     |          If custom objective is also provided, then custom metric should implement the\n",
            "     |          corresponding reverse link function.\n",
            "     |\n",
            "     |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
            "     |          object is provided, it's assumed to be a cost function and by default XGBoost\n",
            "     |          will minimize the result during early stopping.\n",
            "     |\n",
            "     |          For advanced usage on Early stopping like directly choosing to maximize instead\n",
            "     |          of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
            "     |          information.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              from sklearn.datasets import load_diabetes\n",
            "     |              from sklearn.metrics import mean_absolute_error\n",
            "     |              X, y = load_diabetes(return_X_y=True)\n",
            "     |              reg = xgb.XGBRegressor(\n",
            "     |                  tree_method=\"hist\",\n",
            "     |                  eval_metric=mean_absolute_error,\n",
            "     |              )\n",
            "     |              reg.fit(X, y, eval_set=[(X, y)])\n",
            "     |\n",
            "     |      early_stopping_rounds : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          - Activates early stopping. Validation metric needs to improve at least once in\n",
            "     |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
            "     |            least one item in **eval_set** in :py:meth:`fit`.\n",
            "     |\n",
            "     |          - If early stopping occurs, the model will have two additional attributes:\n",
            "     |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
            "     |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
            "     |            number of trees during inference. If users want to access the full model\n",
            "     |            (including trees built after early stopping), they can specify the\n",
            "     |            `iteration_range` in these inference methods. In addition, other utilities\n",
            "     |            like model plotting can also use the entire model.\n",
            "     |\n",
            "     |          - If you prefer to discard the trees after `best_iteration`, consider using the\n",
            "     |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          - If there's more than one item in **eval_set**, the last entry will be used for\n",
            "     |            early stopping.  If there's more than one metric in **eval_metric**, the last\n",
            "     |            metric will be used for early stopping.\n",
            "     |\n",
            "     |      callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
            "     |\n",
            "     |          List of callback functions that are applied at end of each iteration.\n",
            "     |          It is possible to use predefined callbacks by using\n",
            "     |          :ref:`Callback API <callback_api>`.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             States in callback are not preserved during training, which means callback\n",
            "     |             objects can not be reused for multiple training sessions without\n",
            "     |             reinitialization or deepcopy.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              for params in parameters_grid:\n",
            "     |                  # be sure to (re)initialize the callbacks before each run\n",
            "     |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            "     |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
            "     |                  reg.fit(X, y)\n",
            "     |\n",
            "     |      kwargs : typing.Optional[typing.Any]\n",
            "     |\n",
            "     |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
            "     |          can be found :doc:`here </parameter>`.\n",
            "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
            "     |          dict simultaneously will result in a TypeError.\n",
            "     |\n",
            "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
            "     |\n",
            "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
            "     |              that parameters passed via this argument will interact properly\n",
            "     |              with scikit-learn.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |              A custom objective function is currently not supported by XGBRanker.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |              Query group information is only required for ranking training but not\n",
            "     |              prediction. Multiple groups can be predicted on a single call to\n",
            "     |              :py:meth:`predict`.\n",
            "     |\n",
            "     |          When fitting the model with the `group` parameter, your data need to be sorted\n",
            "     |          by the query group first. `group` is an array that contains the size of each\n",
            "     |          query group.\n",
            "     |\n",
            "     |          Similarly, when fitting the model with the `qid` parameter, the data should be\n",
            "     |          sorted according to query index and `qid` is an array that contains the query\n",
            "     |          index for each training sample.\n",
            "     |\n",
            "     |          For example, if your original data look like:\n",
            "     |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |          |   qid |   label   |   features    |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |          |   1   |   0       |   x_1         |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |          |   1   |   1       |   x_2         |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |          |   1   |   0       |   x_3         |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |          |   2   |   0       |   x_4         |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |          |   2   |   1       |   x_5         |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |          |   2   |   1       |   x_6         |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |          |   2   |   1       |   x_7         |\n",
            "     |          +-------+-----------+---------------+\n",
            "     |\n",
            "     |          then :py:meth:`fit` method can be called with either `group` array as ``[3, 4]``\n",
            "     |          or with `qid` as ``[1, 1, 1, 2, 2, 2, 2]``, that is the qid column.  Also, the\n",
            "     |          `qid` can be a special column of input `X` instead of a separated parameter, see\n",
            "     |          :py:meth:`fit` for more info.\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      XGBRanker\n",
            "     |      XGBRankerMixIn\n",
            "     |      XGBModel\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __init__(self, *, objective: str = 'rank:ndcg', **kwargs: Any)\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |\n",
            "     |  apply(self, X: Any, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> Any\n",
            "     |      Return the predicted leaf every tree for each sample. If the model is trained\n",
            "     |      with early stopping, then :py:attr:`best_iteration` is used automatically.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input features matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |      iteration_range :\n",
            "     |          See :py:meth:`predict`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
            "     |          For each datapoint x in X and for each tree, return the index of the\n",
            "     |          leaf x ends up in. Leaves are numbered within\n",
            "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
            "     |\n",
            "     |  fit(self, X: Any, y: Any, *, group: Optional[Any] = None, qid: Optional[Any] = None, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, eval_group: Optional[Sequence[Any]] = None, eval_qid: Optional[Sequence[Any]] = None, verbose: Union[bool, int, NoneType] = False, xgb_model: Union[xgboost.core.Booster, str, xgboost.sklearn.XGBModel, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None) -> 'XGBRanker'\n",
            "     |      Fit gradient boosting ranker\n",
            "     |\n",
            "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
            "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
            "     |      pass ``xgb_model`` argument.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |          When this is a :py:class:`pandas.DataFrame` or a :py:class:`cudf.DataFrame`,\n",
            "     |          it may contain a special column called ``qid`` for specifying the query\n",
            "     |          index. Using a special column is the same as using the `qid` parameter,\n",
            "     |          except for being compatible with sklearn utility functions like\n",
            "     |          :py:func:`sklearn.model_selection.cross_validation`. The same convention\n",
            "     |          applies to the :py:meth:`XGBRanker.score` and :py:meth:`XGBRanker.predict`.\n",
            "     |\n",
            "     |          +-----+----------------+----------------+\n",
            "     |          | qid | feat_0         | feat_1         |\n",
            "     |          +-----+----------------+----------------+\n",
            "     |          | 0   | :math:`x_{00}` | :math:`x_{01}` |\n",
            "     |          +-----+----------------+----------------+\n",
            "     |          | 1   | :math:`x_{10}` | :math:`x_{11}` |\n",
            "     |          +-----+----------------+----------------+\n",
            "     |          | 1   | :math:`x_{20}` | :math:`x_{21}` |\n",
            "     |          +-----+----------------+----------------+\n",
            "     |\n",
            "     |          When the ``tree_method`` is set to ``hist``, internally, the\n",
            "     |          :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n",
            "     |          for conserving memory. However, this has performance implications when the\n",
            "     |          device of input data is not matched with algorithm. For instance, if the\n",
            "     |          input is a numpy array on CPU but ``cuda`` is used for training, then the\n",
            "     |          data is first processed on CPU then transferred to GPU.\n",
            "     |      y :\n",
            "     |          Labels\n",
            "     |      group :\n",
            "     |          Size of each query group of training data. Should have as many elements as\n",
            "     |          the query groups in the training data.  If this is set to None, then user\n",
            "     |          must provide qid.\n",
            "     |      qid :\n",
            "     |          Query ID for each training sample.  Should have the size of n_samples.  If\n",
            "     |          this is set to None, then user must provide group or a special column in X.\n",
            "     |      sample_weight :\n",
            "     |          Query group weights\n",
            "     |\n",
            "     |          .. note:: Weights are per-group for ranking tasks\n",
            "     |\n",
            "     |              In ranking task, one weight is assigned to each query group/id (not each\n",
            "     |              data point). This is because we only care about the relative ordering of\n",
            "     |              data points within each group, so it doesn't make sense to assign\n",
            "     |              weights to individual data points.\n",
            "     |\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      eval_set :\n",
            "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
            "     |          metrics will be computed.\n",
            "     |          Validation metrics will help us track the performance of the model.\n",
            "     |      eval_group :\n",
            "     |          A list in which ``eval_group[i]`` is the list containing the sizes of all\n",
            "     |          query groups in the ``i``-th pair in **eval_set**.\n",
            "     |      eval_qid :\n",
            "     |          A list in which ``eval_qid[i]`` is the array containing query ID of ``i``-th\n",
            "     |          pair in **eval_set**. The special column convention in `X` applies to\n",
            "     |          validation datasets as well.\n",
            "     |\n",
            "     |      verbose :\n",
            "     |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
            "     |          measured on the validation set is printed to stdout at each boosting stage.\n",
            "     |          If `verbose` is an integer, the evaluation metric is printed at each\n",
            "     |          `verbose` boosting stage. The last boosting stage / the boosting stage found\n",
            "     |          by using `early_stopping_rounds` is also printed.\n",
            "     |      xgb_model :\n",
            "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
            "     |          loaded before training (allows training continuation).\n",
            "     |      sample_weight_eval_set :\n",
            "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
            "     |          group weights on the i-th validation set.\n",
            "     |\n",
            "     |          .. note:: Weights are per-group for ranking tasks\n",
            "     |\n",
            "     |              In ranking task, one weight is assigned to each query group (not each\n",
            "     |              data point). This is because we only care about the relative ordering of\n",
            "     |              data points within each group, so it doesn't make sense to assign\n",
            "     |              weights to individual data points.\n",
            "     |      base_margin_eval_set :\n",
            "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
            "     |          object storing base margin for the i-th validation set.\n",
            "     |      feature_weights :\n",
            "     |          Weight for each feature, defines the probability of each feature being\n",
            "     |          selected when colsample is being used.  All values must be greater than 0,\n",
            "     |          otherwise a `ValueError` is thrown.\n",
            "     |\n",
            "     |  predict(self, X: Any, *, output_margin: bool = False, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> Any\n",
            "     |      Predict with `X`.  If the model is trained with early stopping, then\n",
            "     |      :py:attr:`best_iteration` is used automatically. The estimator uses\n",
            "     |      `inplace_predict` by default and falls back to using :py:class:`DMatrix` if\n",
            "     |      devices between the data and the estimator don't match.\n",
            "     |\n",
            "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Data to predict with. See :ref:`py-data` for a list of supported types.\n",
            "     |      output_margin :\n",
            "     |          Whether to output the raw untransformed margin value.\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's feature_names are\n",
            "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
            "     |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
            "     |          are used in this prediction.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction\n",
            "     |\n",
            "     |  score(self, X: Any, y: Any) -> float\n",
            "     |      Evaluate score for data using the last evaluation metric. If the model is\n",
            "     |      trained with early stopping, then :py:attr:`best_iteration` is used\n",
            "     |      automatically.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : Union[pd.DataFrame, cudf.DataFrame]\n",
            "     |        Feature matrix. A DataFrame with a special `qid` column.\n",
            "     |\n",
            "     |      y :\n",
            "     |        Labels\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score :\n",
            "     |        The result of the first evaluation metric for the ranker.\n",
            "     |\n",
            "     |  set_fit_request(self: xgboost.sklearn.XGBRanker, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', base_margin_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', eval_group: Union[bool, NoneType, str] = '$UNCHANGED$', eval_qid: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_weights: Union[bool, NoneType, str] = '$UNCHANGED$', group: Union[bool, NoneType, str] = '$UNCHANGED$', qid: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', verbose: Union[bool, NoneType, str] = '$UNCHANGED$', xgb_model: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRanker from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``fit`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``fit``.\n",
            "     |\n",
            "     |      base_margin_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      eval_group : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``eval_group`` parameter in ``fit``.\n",
            "     |\n",
            "     |      eval_qid : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``eval_qid`` parameter in ``fit``.\n",
            "     |\n",
            "     |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      feature_weights : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``feature_weights`` parameter in ``fit``.\n",
            "     |\n",
            "     |      group : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``group`` parameter in ``fit``.\n",
            "     |\n",
            "     |      qid : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``qid`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      verbose : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``verbose`` parameter in ``fit``.\n",
            "     |\n",
            "     |      xgb_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``xgb_model`` parameter in ``fit``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_predict_request(self: xgboost.sklearn.XGBRanker, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', output_margin: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRanker from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``predict`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``iteration_range`` parameter in ``predict``.\n",
            "     |\n",
            "     |      output_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``output_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``validate_features`` parameter in ``predict``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |\n",
            "     |  __annotations__ = {}\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from XGBRankerMixIn:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from XGBModel:\n",
            "     |\n",
            "     |  __sklearn_is_fitted__(self) -> bool\n",
            "     |\n",
            "     |  __sklearn_tags__(self) -> sklearn.utils._tags.Tags\n",
            "     |\n",
            "     |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
            "     |      Return the evaluation results.\n",
            "     |\n",
            "     |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
            "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
            "     |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
            "     |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
            "     |      function.\n",
            "     |\n",
            "     |      The returned evaluation result is a dictionary:\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
            "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      evals_result\n",
            "     |\n",
            "     |  get_booster(self) -> xgboost.core.Booster\n",
            "     |      Get the underlying xgboost Booster of this model.\n",
            "     |\n",
            "     |      This will raise an exception when fit was not called\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      booster : a xgboost booster of underlying model\n",
            "     |\n",
            "     |  get_num_boosting_rounds(self) -> int\n",
            "     |      Gets the number of xgboost boosting rounds.\n",
            "     |\n",
            "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
            "     |      Get parameters.\n",
            "     |\n",
            "     |  get_xgb_params(self) -> Dict[str, Any]\n",
            "     |      Get xgboost specific parameters.\n",
            "     |\n",
            "     |  load_model(self, fname: Union[os.PathLike[~AnyStr], bytearray, str]) -> None\n",
            "     |      Load the model from a file or a bytearray.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        model.load_model(\"model.json\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |        model.load_model(\"model.ubj\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        buf = model.save_raw()\n",
            "     |        model.load_model(buf)\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Input file name or memory buffer(see also save_raw)\n",
            "     |\n",
            "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
            "     |      Save the model to a file.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Output file name\n",
            "     |\n",
            "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
            "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
            "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
            "     |      parameters that are not defined as member variables in sklearn grid\n",
            "     |      search.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from XGBModel:\n",
            "     |\n",
            "     |  best_iteration\n",
            "     |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
            "     |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
            "     |\n",
            "     |  best_score\n",
            "     |      The best score obtained by early stopping.\n",
            "     |\n",
            "     |  coef_\n",
            "     |      Coefficients property\n",
            "     |\n",
            "     |      .. note:: Coefficients are defined only for linear learners\n",
            "     |\n",
            "     |          Coefficients are only defined when the linear model is chosen as\n",
            "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
            "     |          learner types, such as tree learners (`booster=gbtree`).\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
            "     |\n",
            "     |  feature_importances_\n",
            "     |      Feature importances property, return depends on `importance_type`\n",
            "     |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
            "     |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
            "     |      based on the importance type. For instance, if the importance type is\n",
            "     |      \"total_gain\", then the score is sum of loss change for each split from all\n",
            "     |      trees.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
            "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
            "     |\n",
            "     |  feature_names_in_\n",
            "     |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has\n",
            "     |      feature names that are all strings.\n",
            "     |\n",
            "     |  intercept_\n",
            "     |      Intercept (bias) property\n",
            "     |\n",
            "     |      For tree-based model, the returned value is the `base_score`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
            "     |\n",
            "     |  n_features_in_\n",
            "     |      Number of features seen during :py:meth:`fit`.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from XGBModel:\n",
            "     |\n",
            "     |  __slotnames__ = []\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |\n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |\n",
            "     |  __repr__(self, N_CHAR_MAX=700)\n",
            "     |      Return repr(self).\n",
            "     |\n",
            "     |  __setstate__(self, state)\n",
            "     |\n",
            "     |  __sklearn_clone__(self)\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  get_metadata_routing(self)\n",
            "     |      Get metadata routing of this object.\n",
            "     |\n",
            "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      routing : MetadataRequest\n",
            "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            "     |          routing information.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  __init_subclass__(**kwargs)\n",
            "     |      Set the ``set_{method}_request`` methods.\n",
            "     |\n",
            "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            "     |      looks for the information available in the set default values which are\n",
            "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            "     |      from method signatures.\n",
            "     |\n",
            "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
            "     |      does not explicitly accept a metadata through its arguments or if the\n",
            "     |      developer would like to specify a request value for those metadata\n",
            "     |      which are different from the default ``None``.\n",
            "     |\n",
            "     |      References\n",
            "     |      ----------\n",
            "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n",
            "    class XGBRegressor(sklearn.base.RegressorMixin, XGBModel)\n",
            "     |  XGBRegressor(*, objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'reg:squarederror', **kwargs: Any) -> None\n",
            "     |\n",
            "     |  Implementation of the scikit-learn API for XGBoost regression.\n",
            "     |  See :doc:`/python/sklearn_estimator` for more information.\n",
            "     |\n",
            "     |  Parameters\n",
            "     |  ----------\n",
            "     |\n",
            "     |      n_estimators : typing.Optional[int]\n",
            "     |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
            "     |          rounds.\n",
            "     |\n",
            "     |      max_depth :  typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum tree depth for base learners.\n",
            "     |\n",
            "     |      max_leaves : typing.Optional[int]\n",
            "     |\n",
            "     |          Maximum number of leaves; 0 indicates no limit.\n",
            "     |\n",
            "     |      max_bin : typing.Optional[int]\n",
            "     |\n",
            "     |          If using histogram-based algorithm, maximum number of bins per feature\n",
            "     |\n",
            "     |      grow_policy : typing.Optional[str]\n",
            "     |\n",
            "     |          Tree growing policy.\n",
            "     |\n",
            "     |          - depthwise: Favors splitting at nodes closest to the node,\n",
            "     |          - lossguide: Favors splitting at nodes with highest loss change.\n",
            "     |\n",
            "     |      learning_rate : typing.Optional[float]\n",
            "     |\n",
            "     |          Boosting learning rate (xgb's \"eta\")\n",
            "     |\n",
            "     |      verbosity : typing.Optional[int]\n",
            "     |\n",
            "     |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
            "     |\n",
            "     |      objective : typing.Union[str, xgboost.sklearn._SklObjWProto, typing.Callable[[typing.Any, typing.Any], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
            "     |\n",
            "     |          Specify the learning task and the corresponding learning objective or a custom\n",
            "     |          objective function to be used.\n",
            "     |\n",
            "     |          For custom objective, see :doc:`/tutorials/custom_metric_obj` and\n",
            "     |          :ref:`custom-obj-metric` for more information, along with the end note for\n",
            "     |          function signatures.\n",
            "     |\n",
            "     |      booster: typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which booster to use: ``gbtree``, ``gblinear`` or ``dart``.\n",
            "     |\n",
            "     |      tree_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
            "     |          default, XGBoost will choose the most conservative option available.  It's\n",
            "     |          recommended to study this option from the parameters document :doc:`tree method\n",
            "     |          </treemethod>`\n",
            "     |\n",
            "     |      n_jobs : typing.Optional[int]\n",
            "     |\n",
            "     |          Number of parallel threads used to run xgboost.  When used with other\n",
            "     |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
            "     |          parallelize and balance the threads.  Creating thread contention will\n",
            "     |          significantly slow down both algorithms.\n",
            "     |\n",
            "     |      gamma : typing.Optional[float]\n",
            "     |\n",
            "     |          (min_split_loss) Minimum loss reduction required to make a further partition on\n",
            "     |          a leaf node of the tree.\n",
            "     |\n",
            "     |      min_child_weight : typing.Optional[float]\n",
            "     |\n",
            "     |          Minimum sum of instance weight(hessian) needed in a child.\n",
            "     |\n",
            "     |      max_delta_step : typing.Optional[float]\n",
            "     |\n",
            "     |          Maximum delta step we allow each tree's weight estimation to be.\n",
            "     |\n",
            "     |      subsample : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of the training instance.\n",
            "     |\n",
            "     |      sampling_method : typing.Optional[str]\n",
            "     |\n",
            "     |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n",
            "     |\n",
            "     |          - ``uniform``: Select random training instances uniformly.\n",
            "     |          - ``gradient_based``: Select random training instances with higher probability\n",
            "     |              when the gradient and hessian are larger. (cf. CatBoost)\n",
            "     |\n",
            "     |      colsample_bytree : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns when constructing each tree.\n",
            "     |\n",
            "     |      colsample_bylevel : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each level.\n",
            "     |\n",
            "     |      colsample_bynode : typing.Optional[float]\n",
            "     |\n",
            "     |          Subsample ratio of columns for each split.\n",
            "     |\n",
            "     |      reg_alpha : typing.Optional[float]\n",
            "     |\n",
            "     |          L1 regularization term on weights (xgb's alpha).\n",
            "     |\n",
            "     |      reg_lambda : typing.Optional[float]\n",
            "     |\n",
            "     |          L2 regularization term on weights (xgb's lambda).\n",
            "     |\n",
            "     |      scale_pos_weight : typing.Optional[float]\n",
            "     |          Balancing of positive and negative weights.\n",
            "     |\n",
            "     |      base_score : typing.Union[float, typing.List[float], NoneType]\n",
            "     |\n",
            "     |          The initial prediction score of all instances, global bias.\n",
            "     |\n",
            "     |      random_state : typing.Union[numpy.random.mtrand.RandomState, numpy.random._generator.Generator, int, NoneType]\n",
            "     |\n",
            "     |          Random number seed.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             Using gblinear booster with shotgun updater is nondeterministic as\n",
            "     |             it uses Hogwild algorithm.\n",
            "     |\n",
            "     |      missing : float\n",
            "     |\n",
            "     |          Value in the data which needs to be present as a missing value. Default to\n",
            "     |          :py:data:`numpy.nan`.\n",
            "     |\n",
            "     |      num_parallel_tree: typing.Optional[int]\n",
            "     |\n",
            "     |          Used for boosting random forest.\n",
            "     |\n",
            "     |      monotone_constraints : typing.Union[typing.Dict[str, int], str, NoneType]\n",
            "     |\n",
            "     |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
            "     |          for more information.\n",
            "     |\n",
            "     |      interaction_constraints : typing.Union[str, typing.List[typing.Tuple[str]], NoneType]\n",
            "     |\n",
            "     |          Constraints for interaction representing permitted interactions.  The\n",
            "     |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
            "     |          3, 4]]``, where each inner list is a group of indices of features that are\n",
            "     |          allowed to interact with each other.  See :doc:`tutorial\n",
            "     |          </tutorials/feature_interaction_constraint>` for more information\n",
            "     |\n",
            "     |      importance_type: typing.Optional[str]\n",
            "     |\n",
            "     |          The feature importance type for the feature_importances\\_ property:\n",
            "     |\n",
            "     |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
            "     |            \"total_cover\".\n",
            "     |          * For linear model, only \"weight\" is defined and it's the normalized\n",
            "     |            coefficients without bias.\n",
            "     |\n",
            "     |      device : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n",
            "     |\n",
            "     |      validate_parameters : typing.Optional[bool]\n",
            "     |\n",
            "     |          Give warnings for unknown parameter.\n",
            "     |\n",
            "     |      enable_categorical : bool\n",
            "     |\n",
            "     |          See the same parameter of :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_types : typing.Optional[typing.Sequence[str]]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          Used for specifying feature types without constructing a dataframe. See\n",
            "     |          the :py:class:`DMatrix` for details.\n",
            "     |\n",
            "     |      feature_weights : Optional[ArrayLike]\n",
            "     |\n",
            "     |          Weight for each feature, defines the probability of each feature being selected\n",
            "     |          when colsample is being used.  All values must be greater than 0, otherwise a\n",
            "     |          `ValueError` is thrown.\n",
            "     |\n",
            "     |      max_cat_to_onehot : Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
            "     |          for categorical data.  When number of categories is lesser than the threshold\n",
            "     |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
            "     |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
            "     |          categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      max_cat_threshold : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.7.0\n",
            "     |\n",
            "     |          .. note:: This parameter is experimental\n",
            "     |\n",
            "     |          Maximum number of categories considered for each split. Used only by\n",
            "     |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
            "     |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
            "     |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
            "     |\n",
            "     |      multi_strategy : typing.Optional[str]\n",
            "     |\n",
            "     |          .. versionadded:: 2.0.0\n",
            "     |\n",
            "     |          .. note:: This parameter is working-in-progress.\n",
            "     |\n",
            "     |          The strategy used for training multi-target models, including multi-target\n",
            "     |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n",
            "     |          more information.\n",
            "     |\n",
            "     |          - ``one_output_per_tree``: One model for each target.\n",
            "     |          - ``multi_output_tree``:  Use multi-target trees.\n",
            "     |\n",
            "     |      eval_metric : typing.Union[str, typing.List[typing.Union[str, typing.Callable]], typing.Callable, NoneType]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          Metric used for monitoring the training result and early stopping.  It can be a\n",
            "     |          string or list of strings as names of predefined metric in XGBoost (See\n",
            "     |          :doc:`/parameter`), one of the metrics in :py:mod:`sklearn.metrics`, or any\n",
            "     |          other user defined metric that looks like `sklearn.metrics`.\n",
            "     |\n",
            "     |          If custom objective is also provided, then custom metric should implement the\n",
            "     |          corresponding reverse link function.\n",
            "     |\n",
            "     |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
            "     |          object is provided, it's assumed to be a cost function and by default XGBoost\n",
            "     |          will minimize the result during early stopping.\n",
            "     |\n",
            "     |          For advanced usage on Early stopping like directly choosing to maximize instead\n",
            "     |          of minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          See :doc:`/tutorials/custom_metric_obj` and :ref:`custom-obj-metric` for more\n",
            "     |          information.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              from sklearn.datasets import load_diabetes\n",
            "     |              from sklearn.metrics import mean_absolute_error\n",
            "     |              X, y = load_diabetes(return_X_y=True)\n",
            "     |              reg = xgb.XGBRegressor(\n",
            "     |                  tree_method=\"hist\",\n",
            "     |                  eval_metric=mean_absolute_error,\n",
            "     |              )\n",
            "     |              reg.fit(X, y, eval_set=[(X, y)])\n",
            "     |\n",
            "     |      early_stopping_rounds : typing.Optional[int]\n",
            "     |\n",
            "     |          .. versionadded:: 1.6.0\n",
            "     |\n",
            "     |          - Activates early stopping. Validation metric needs to improve at least once in\n",
            "     |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n",
            "     |            least one item in **eval_set** in :py:meth:`fit`.\n",
            "     |\n",
            "     |          - If early stopping occurs, the model will have two additional attributes:\n",
            "     |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n",
            "     |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n",
            "     |            number of trees during inference. If users want to access the full model\n",
            "     |            (including trees built after early stopping), they can specify the\n",
            "     |            `iteration_range` in these inference methods. In addition, other utilities\n",
            "     |            like model plotting can also use the entire model.\n",
            "     |\n",
            "     |          - If you prefer to discard the trees after `best_iteration`, consider using the\n",
            "     |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n",
            "     |\n",
            "     |          - If there's more than one item in **eval_set**, the last entry will be used for\n",
            "     |            early stopping.  If there's more than one metric in **eval_metric**, the last\n",
            "     |            metric will be used for early stopping.\n",
            "     |\n",
            "     |      callbacks : typing.Optional[typing.List[xgboost.callback.TrainingCallback]]\n",
            "     |\n",
            "     |          List of callback functions that are applied at end of each iteration.\n",
            "     |          It is possible to use predefined callbacks by using\n",
            "     |          :ref:`Callback API <callback_api>`.\n",
            "     |\n",
            "     |          .. note::\n",
            "     |\n",
            "     |             States in callback are not preserved during training, which means callback\n",
            "     |             objects can not be reused for multiple training sessions without\n",
            "     |             reinitialization or deepcopy.\n",
            "     |\n",
            "     |          .. code-block:: python\n",
            "     |\n",
            "     |              for params in parameters_grid:\n",
            "     |                  # be sure to (re)initialize the callbacks before each run\n",
            "     |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            "     |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n",
            "     |                  reg.fit(X, y)\n",
            "     |\n",
            "     |      kwargs : typing.Optional[typing.Any]\n",
            "     |\n",
            "     |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
            "     |          can be found :doc:`here </parameter>`.\n",
            "     |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
            "     |          dict simultaneously will result in a TypeError.\n",
            "     |\n",
            "     |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
            "     |\n",
            "     |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
            "     |              that parameters passed via this argument will interact properly\n",
            "     |              with scikit-learn.\n",
            "     |\n",
            "     |          .. note::  Custom objective function\n",
            "     |\n",
            "     |              A custom objective function can be provided for the ``objective``\n",
            "     |              parameter. In this case, it should have the signature ``objective(y_true,\n",
            "     |              y_pred) -> [grad, hess]`` or ``objective(y_true, y_pred, *, sample_weight)\n",
            "     |              -> [grad, hess]``:\n",
            "     |\n",
            "     |              y_true: array_like of shape [n_samples]\n",
            "     |                  The target values\n",
            "     |              y_pred: array_like of shape [n_samples]\n",
            "     |                  The predicted values\n",
            "     |              sample_weight :\n",
            "     |                  Optional sample weights.\n",
            "     |\n",
            "     |              grad: array_like of shape [n_samples]\n",
            "     |                  The value of the gradient for each sample point.\n",
            "     |              hess: array_like of shape [n_samples]\n",
            "     |                  The value of the second derivative for each sample point\n",
            "     |\n",
            "     |              Note that, if the custom objective produces negative values for\n",
            "     |              the Hessian, these will be clipped. If the objective is non-convex,\n",
            "     |              one might also consider using the expected Hessian (Fisher\n",
            "     |              information) instead.\n",
            "     |\n",
            "     |  Method resolution order:\n",
            "     |      XGBRegressor\n",
            "     |      sklearn.base.RegressorMixin\n",
            "     |      XGBModel\n",
            "     |      sklearn.base.BaseEstimator\n",
            "     |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            "     |      sklearn.utils._metadata_requests._MetadataRequester\n",
            "     |      builtins.object\n",
            "     |\n",
            "     |  Methods defined here:\n",
            "     |\n",
            "     |  __init__(self, *, objective: Union[str, xgboost.sklearn._SklObjWProto, Callable[[Any, Any], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'reg:squarederror', **kwargs: Any) -> None\n",
            "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
            "     |\n",
            "     |  __sklearn_tags__(self) -> sklearn.utils._tags.Tags\n",
            "     |\n",
            "     |  set_fit_request(self: xgboost.sklearn.XGBRegressor, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', base_margin_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', feature_weights: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight_eval_set: Union[bool, NoneType, str] = '$UNCHANGED$', verbose: Union[bool, NoneType, str] = '$UNCHANGED$', xgb_model: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``fit`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``fit``.\n",
            "     |\n",
            "     |      base_margin_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      feature_weights : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``feature_weights`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
            "     |\n",
            "     |      sample_weight_eval_set : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight_eval_set`` parameter in ``fit``.\n",
            "     |\n",
            "     |      verbose : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``verbose`` parameter in ``fit``.\n",
            "     |\n",
            "     |      xgb_model : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``xgb_model`` parameter in ``fit``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_predict_request(self: xgboost.sklearn.XGBRegressor, *, base_margin: Union[bool, NoneType, str] = '$UNCHANGED$', iteration_range: Union[bool, NoneType, str] = '$UNCHANGED$', output_margin: Union[bool, NoneType, str] = '$UNCHANGED$', validate_features: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``predict`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``predict`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``predict``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      base_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``base_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      iteration_range : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``iteration_range`` parameter in ``predict``.\n",
            "     |\n",
            "     |      output_margin : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``output_margin`` parameter in ``predict``.\n",
            "     |\n",
            "     |      validate_features : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``validate_features`` parameter in ``predict``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  set_score_request(self: xgboost.sklearn.XGBRegressor, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> xgboost.sklearn.XGBRegressor from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            "     |      Request metadata passed to the ``score`` method.\n",
            "     |\n",
            "     |      Note that this method is only relevant if\n",
            "     |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            "     |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      The options for each parameter are:\n",
            "     |\n",
            "     |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
            "     |\n",
            "     |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
            "     |\n",
            "     |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            "     |\n",
            "     |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            "     |\n",
            "     |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            "     |      existing request. This allows you to change the request for some\n",
            "     |      parameters and not others.\n",
            "     |\n",
            "     |      .. versionadded:: 1.3\n",
            "     |\n",
            "     |      .. note::\n",
            "     |          This method is only relevant if this estimator is used as a\n",
            "     |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            "     |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            "     |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self : object\n",
            "     |          The updated object.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes defined here:\n",
            "     |\n",
            "     |  __annotations__ = {}\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
            "     |\n",
            "     |  score(self, X, y, sample_weight=None)\n",
            "     |      Return the coefficient of determination of the prediction.\n",
            "     |\n",
            "     |      The coefficient of determination :math:`R^2` is defined as\n",
            "     |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
            "     |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
            "     |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
            "     |      The best possible score is 1.0 and it can be negative (because the\n",
            "     |      model can be arbitrarily worse). A constant model that always predicts\n",
            "     |      the expected value of `y`, disregarding the input features, would get\n",
            "     |      a :math:`R^2` score of 0.0.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X : array-like of shape (n_samples, n_features)\n",
            "     |          Test samples. For some estimators this may be a precomputed\n",
            "     |          kernel matrix or a list of generic objects instead with shape\n",
            "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
            "     |          is the number of samples used in the fitting for the estimator.\n",
            "     |\n",
            "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            "     |          True values for `X`.\n",
            "     |\n",
            "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
            "     |          Sample weights.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      score : float\n",
            "     |          :math:`R^2` of ``self.predict(X)`` w.r.t. `y`.\n",
            "     |\n",
            "     |      Notes\n",
            "     |      -----\n",
            "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
            "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
            "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
            "     |      This influences the ``score`` method of all the multioutput\n",
            "     |      regressors (except for\n",
            "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
            "     |\n",
            "     |  __dict__\n",
            "     |      dictionary for instance variables\n",
            "     |\n",
            "     |  __weakref__\n",
            "     |      list of weak references to the object\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from XGBModel:\n",
            "     |\n",
            "     |  __sklearn_is_fitted__(self) -> bool\n",
            "     |\n",
            "     |  apply(self, X: Any, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> numpy.ndarray\n",
            "     |      Return the predicted leaf every tree for each sample. If the model is trained\n",
            "     |      with early stopping, then :py:attr:`best_iteration` is used automatically.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input features matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |      iteration_range :\n",
            "     |          See :py:meth:`predict`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
            "     |          For each datapoint x in X and for each tree, return the index of the\n",
            "     |          leaf x ends up in. Leaves are numbered within\n",
            "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
            "     |\n",
            "     |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
            "     |      Return the evaluation results.\n",
            "     |\n",
            "     |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
            "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
            "     |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
            "     |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
            "     |      function.\n",
            "     |\n",
            "     |      The returned evaluation result is a dictionary:\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
            "     |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      evals_result\n",
            "     |\n",
            "     |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, ForwardRef('XGBModel'), str, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None) -> 'XGBModel'\n",
            "     |      Fit gradient boosting model.\n",
            "     |\n",
            "     |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
            "     |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
            "     |      pass ``xgb_model`` argument.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Input feature matrix. See :ref:`py-data` for a list of supported types.\n",
            "     |\n",
            "     |          When the ``tree_method`` is set to ``hist``, internally, the\n",
            "     |          :py:class:`QuantileDMatrix` will be used instead of the :py:class:`DMatrix`\n",
            "     |          for conserving memory. However, this has performance implications when the\n",
            "     |          device of input data is not matched with algorithm. For instance, if the\n",
            "     |          input is a numpy array on CPU but ``cuda`` is used for training, then the\n",
            "     |          data is first processed on CPU then transferred to GPU.\n",
            "     |      y :\n",
            "     |          Labels\n",
            "     |      sample_weight :\n",
            "     |          instance weights\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      eval_set :\n",
            "     |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
            "     |          metrics will be computed.\n",
            "     |          Validation metrics will help us track the performance of the model.\n",
            "     |\n",
            "     |      verbose :\n",
            "     |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
            "     |          measured on the validation set is printed to stdout at each boosting stage.\n",
            "     |          If `verbose` is an integer, the evaluation metric is printed at each\n",
            "     |          `verbose` boosting stage. The last boosting stage / the boosting stage found\n",
            "     |          by using `early_stopping_rounds` is also printed.\n",
            "     |      xgb_model :\n",
            "     |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
            "     |          loaded before training (allows training continuation).\n",
            "     |      sample_weight_eval_set :\n",
            "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
            "     |          object storing instance weights for the i-th validation set.\n",
            "     |      base_margin_eval_set :\n",
            "     |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
            "     |          object storing base margin for the i-th validation set.\n",
            "     |      feature_weights :\n",
            "     |\n",
            "     |          .. deprecated:: 3.0.0\n",
            "     |\n",
            "     |          Use `feature_weights` in :py:meth:`__init__` or :py:meth:`set_params`\n",
            "     |          instead.\n",
            "     |\n",
            "     |  get_booster(self) -> xgboost.core.Booster\n",
            "     |      Get the underlying xgboost Booster of this model.\n",
            "     |\n",
            "     |      This will raise an exception when fit was not called\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      booster : a xgboost booster of underlying model\n",
            "     |\n",
            "     |  get_num_boosting_rounds(self) -> int\n",
            "     |      Gets the number of xgboost boosting rounds.\n",
            "     |\n",
            "     |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
            "     |      Get parameters.\n",
            "     |\n",
            "     |  get_xgb_params(self) -> Dict[str, Any]\n",
            "     |      Get xgboost specific parameters.\n",
            "     |\n",
            "     |  load_model(self, fname: Union[os.PathLike[~AnyStr], bytearray, str]) -> None\n",
            "     |      Load the model from a file or a bytearray.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        model.load_model(\"model.json\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |        model.load_model(\"model.ubj\")\n",
            "     |\n",
            "     |        # or\n",
            "     |        buf = model.save_raw()\n",
            "     |        model.load_model(buf)\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Input file name or memory buffer(see also save_raw)\n",
            "     |\n",
            "     |  predict(self, X: Any, *, output_margin: bool = False, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[Union[int, numpy.integer], Union[int, numpy.integer]]] = None) -> Any\n",
            "     |      Predict with `X`.  If the model is trained with early stopping, then\n",
            "     |      :py:attr:`best_iteration` is used automatically. The estimator uses\n",
            "     |      `inplace_predict` by default and falls back to using :py:class:`DMatrix` if\n",
            "     |      devices between the data and the estimator don't match.\n",
            "     |\n",
            "     |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      X :\n",
            "     |          Data to predict with. See :ref:`py-data` for a list of supported types.\n",
            "     |      output_margin :\n",
            "     |          Whether to output the raw untransformed margin value.\n",
            "     |      validate_features :\n",
            "     |          When this is True, validate that the Booster's and data's feature_names are\n",
            "     |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
            "     |      base_margin :\n",
            "     |          Global bias for each instance. See :doc:`/tutorials/intercept` for details.\n",
            "     |      iteration_range :\n",
            "     |          Specifies which layer of trees are used in prediction.  For example, if a\n",
            "     |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
            "     |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
            "     |          are used in this prediction.\n",
            "     |\n",
            "     |          .. versionadded:: 1.4.0\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      prediction\n",
            "     |\n",
            "     |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
            "     |      Save the model to a file.\n",
            "     |\n",
            "     |      The model is saved in an XGBoost internal format which is universal among the\n",
            "     |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
            "     |      (such as feature_names) are only saved when using JSON or UBJSON (default)\n",
            "     |      format. Also, parameters that are not part of the model (like metrics,\n",
            "     |      `max_depth`, etc) are not saved, see :doc:`Model IO </tutorials/saving_model>`\n",
            "     |      for more info.\n",
            "     |\n",
            "     |      .. code-block:: python\n",
            "     |\n",
            "     |        model.save_model(\"model.json\")\n",
            "     |        # or\n",
            "     |        model.save_model(\"model.ubj\")\n",
            "     |\n",
            "     |      Parameters\n",
            "     |      ----------\n",
            "     |      fname :\n",
            "     |          Output file name\n",
            "     |\n",
            "     |  set_params(self, **params: Any) -> 'XGBModel'\n",
            "     |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
            "     |      allow unknown kwargs. This allows using the full range of xgboost\n",
            "     |      parameters that are not defined as member variables in sklearn grid\n",
            "     |      search.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      self\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Readonly properties inherited from XGBModel:\n",
            "     |\n",
            "     |  best_iteration\n",
            "     |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
            "     |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
            "     |\n",
            "     |  best_score\n",
            "     |      The best score obtained by early stopping.\n",
            "     |\n",
            "     |  coef_\n",
            "     |      Coefficients property\n",
            "     |\n",
            "     |      .. note:: Coefficients are defined only for linear learners\n",
            "     |\n",
            "     |          Coefficients are only defined when the linear model is chosen as\n",
            "     |          base learner (`booster=gblinear`). It is not defined for other base\n",
            "     |          learner types, such as tree learners (`booster=gbtree`).\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
            "     |\n",
            "     |  feature_importances_\n",
            "     |      Feature importances property, return depends on `importance_type`\n",
            "     |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
            "     |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
            "     |      based on the importance type. For instance, if the importance type is\n",
            "     |      \"total_gain\", then the score is sum of loss change for each split from all\n",
            "     |      trees.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
            "     |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
            "     |\n",
            "     |  feature_names_in_\n",
            "     |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has\n",
            "     |      feature names that are all strings.\n",
            "     |\n",
            "     |  intercept_\n",
            "     |      Intercept (bias) property\n",
            "     |\n",
            "     |      For tree-based model, the returned value is the `base_score`.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
            "     |\n",
            "     |  n_features_in_\n",
            "     |      Number of features seen during :py:meth:`fit`.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Data and other attributes inherited from XGBModel:\n",
            "     |\n",
            "     |  __slotnames__ = []\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
            "     |\n",
            "     |  __getstate__(self)\n",
            "     |      Helper for pickle.\n",
            "     |\n",
            "     |  __repr__(self, N_CHAR_MAX=700)\n",
            "     |      Return repr(self).\n",
            "     |\n",
            "     |  __setstate__(self, state)\n",
            "     |\n",
            "     |  __sklearn_clone__(self)\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  get_metadata_routing(self)\n",
            "     |      Get metadata routing of this object.\n",
            "     |\n",
            "     |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            "     |      mechanism works.\n",
            "     |\n",
            "     |      Returns\n",
            "     |      -------\n",
            "     |      routing : MetadataRequest\n",
            "     |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            "     |          routing information.\n",
            "     |\n",
            "     |  ----------------------------------------------------------------------\n",
            "     |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            "     |\n",
            "     |  __init_subclass__(**kwargs)\n",
            "     |      Set the ``set_{method}_request`` methods.\n",
            "     |\n",
            "     |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            "     |      looks for the information available in the set default values which are\n",
            "     |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            "     |      from method signatures.\n",
            "     |\n",
            "     |      The ``__metadata_request__*`` class attributes are used when a method\n",
            "     |      does not explicitly accept a metadata through its arguments or if the\n",
            "     |      developer would like to specify a request value for those metadata\n",
            "     |      which are different from the default ``None``.\n",
            "     |\n",
            "     |      References\n",
            "     |      ----------\n",
            "     |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n",
            "FUNCTIONS\n",
            "    build_info() -> dict\n",
            "        Build information of XGBoost.  The returned value format is not stable. Also,\n",
            "        please note that build time dependency is not the same as runtime dependency. For\n",
            "        instance, it's possible to build XGBoost with older CUDA version but run it with the\n",
            "        lastest one.\n",
            "\n",
            "          .. versionadded:: 1.6.0\n",
            "\n",
            "    config_context(**new_config: Any) -> Iterator[NoneType]\n",
            "        Context manager for global XGBoost configuration.\n",
            "\n",
            "\n",
            "        Global configuration consists of a collection of parameters that can be applied in the\n",
            "        global scope. See :ref:`global_config` for the full list of parameters supported in\n",
            "        the global configuration.\n",
            "\n",
            "\n",
            "        .. note::\n",
            "\n",
            "            All settings, not just those presently modified, will be returned to their\n",
            "            previous values when the context manager is exited. This is not thread-safe.\n",
            "\n",
            "\n",
            "        .. versionadded:: 1.4.0\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        new_config: Dict[str, Any]\n",
            "            Keyword arguments representing the parameters and their values\n",
            "\n",
            "        Example\n",
            "        -------\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            import xgboost as xgb\n",
            "\n",
            "            # Show all messages, including ones pertaining to debugging\n",
            "            xgb.set_config(verbosity=2)\n",
            "\n",
            "            # Get current value of global configuration\n",
            "            # This is a dict containing all parameters in the global configuration,\n",
            "            # including 'verbosity'\n",
            "            config = xgb.get_config()\n",
            "            assert config['verbosity'] == 2\n",
            "\n",
            "            # Example of using the context manager xgb.config_context().\n",
            "            # The context manager will restore the previous value of the global\n",
            "            # configuration upon exiting.\n",
            "            with xgb.config_context(verbosity=0):\n",
            "                # Suppress warning caused by model generated with XGBoost version < 1.0.0\n",
            "                bst = xgb.Booster(model_file='./old_model.bin')\n",
            "            assert xgb.get_config()['verbosity'] == 2  # old value restored\n",
            "\n",
            "        Nested configuration context is also supported:\n",
            "\n",
            "        Example\n",
            "        -------\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            with xgb.config_context(verbosity=3):\n",
            "                assert xgb.get_config()[\"verbosity\"] == 3\n",
            "                with xgb.config_context(verbosity=2):\n",
            "                    assert xgb.get_config()[\"verbosity\"] == 2\n",
            "\n",
            "            xgb.set_config(verbosity=2)\n",
            "            assert xgb.get_config()[\"verbosity\"] == 2\n",
            "            with xgb.config_context(verbosity=3):\n",
            "                assert xgb.get_config()[\"verbosity\"] == 3\n",
            "\n",
            "        See Also\n",
            "        --------\n",
            "        set_config: Set global XGBoost configuration\n",
            "        get_config: Get current values of the global configuration\n",
            "\n",
            "    cv(params: Union[List, Dict[str, Any]], dtrain: xgboost.core.DMatrix, num_boost_round: int = 10, *, nfold: int = 3, stratified: bool = False, folds: sklearn.model_selection._split.StratifiedKFold = None, metrics: Sequence[str] = (), obj: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[numpy.ndarray, numpy.ndarray]]] = None, maximize: Optional[bool] = None, early_stopping_rounds: Optional[int] = None, fpreproc: Optional[Callable] = None, as_pandas: bool = True, verbose_eval: Union[bool, int, NoneType] = None, show_stdv: bool = True, seed: int = 0, callbacks: Optional[Sequence[xgboost.callback.TrainingCallback]] = None, shuffle: bool = True, custom_metric: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]]] = None) -> Union[Dict[str, float], ForwardRef('PdDataFrame')]\n",
            "        Cross-validation with given parameters.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        params : dict\n",
            "            Booster params.\n",
            "        dtrain :\n",
            "            Data to be trained. Only the :py:class:`DMatrix` without external memory is\n",
            "            supported.\n",
            "        num_boost_round :\n",
            "            Number of boosting iterations.\n",
            "        nfold : int\n",
            "            Number of folds in CV.\n",
            "        stratified : bool\n",
            "            Perform stratified sampling.\n",
            "        folds : a KFold or StratifiedKFold instance or list of fold indices\n",
            "            Sklearn KFolds or StratifiedKFolds object.\n",
            "            Alternatively may explicitly pass sample indices for each fold.\n",
            "            For ``n`` folds, **folds** should be a length ``n`` list of tuples.\n",
            "            Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\n",
            "            as the training samples for the ``n`` th fold and ``out`` is a list of\n",
            "            indices to be used as the testing samples for the ``n`` th fold.\n",
            "        metrics : string or list of strings\n",
            "            Evaluation metrics to be watched in CV.\n",
            "        obj :\n",
            "\n",
            "            Custom objective function.  See :doc:`Custom Objective\n",
            "            </tutorials/custom_metric_obj>` for details.\n",
            "\n",
            "        maximize : bool\n",
            "            Whether to maximize the evaluataion metric (score or error).\n",
            "\n",
            "        early_stopping_rounds: int\n",
            "            Activates early stopping. Cross-Validation metric (average of validation\n",
            "            metric computed over CV folds) needs to improve at least once in\n",
            "            every **early_stopping_rounds** round(s) to continue training.\n",
            "            The last entry in the evaluation history will represent the best iteration.\n",
            "            If there's more than one metric in the **eval_metric** parameter given in\n",
            "            **params**, the last metric will be used for early stopping.\n",
            "        fpreproc : function\n",
            "            Preprocessing function that takes (dtrain, dtest, param) and returns\n",
            "            transformed versions of those.\n",
            "        as_pandas : bool, default True\n",
            "            Return pd.DataFrame when pandas is installed.\n",
            "            If False or pandas is not installed, return np.ndarray\n",
            "        verbose_eval : bool, int, or None, default None\n",
            "            Whether to display the progress. If None, progress will be displayed\n",
            "            when np.ndarray is returned. If True, progress will be displayed at\n",
            "            boosting stage. If an integer is given, progress will be displayed\n",
            "            at every given `verbose_eval` boosting stage.\n",
            "        show_stdv : bool, default True\n",
            "            Whether to display the standard deviation in progress.\n",
            "            Results are not affected, and always contains std.\n",
            "        seed : int\n",
            "            Seed used to generate the folds (passed to numpy.random.seed).\n",
            "        callbacks :\n",
            "            List of callback functions that are applied at end of each iteration.\n",
            "            It is possible to use predefined callbacks by using\n",
            "            :ref:`Callback API <callback_api>`.\n",
            "\n",
            "            .. note::\n",
            "\n",
            "               States in callback are not preserved during training, which means callback\n",
            "               objects can not be reused for multiple training sessions without\n",
            "               reinitialization or deepcopy.\n",
            "\n",
            "            .. code-block:: python\n",
            "\n",
            "                for params in parameters_grid:\n",
            "                    # be sure to (re)initialize the callbacks before each run\n",
            "                    callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            "                    xgboost.train(params, Xy, callbacks=callbacks)\n",
            "\n",
            "        shuffle : bool\n",
            "            Shuffle data before creating folds.\n",
            "        custom_metric :\n",
            "\n",
            "            .. versionadded 1.6.0\n",
            "\n",
            "            Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`\n",
            "            for details.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        evaluation history : list(string)\n",
            "\n",
            "    get_config() -> Dict[str, Any]\n",
            "        Get current values of the global configuration.\n",
            "\n",
            "\n",
            "        Global configuration consists of a collection of parameters that can be applied in the\n",
            "        global scope. See :ref:`global_config` for the full list of parameters supported in\n",
            "        the global configuration.\n",
            "\n",
            "\n",
            "\n",
            "        .. versionadded:: 1.4.0\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        args: Dict[str, Any]\n",
            "            The list of global parameters and their values\n",
            "\n",
            "        Example\n",
            "        -------\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            import xgboost as xgb\n",
            "\n",
            "            # Show all messages, including ones pertaining to debugging\n",
            "            xgb.set_config(verbosity=2)\n",
            "\n",
            "            # Get current value of global configuration\n",
            "            # This is a dict containing all parameters in the global configuration,\n",
            "            # including 'verbosity'\n",
            "            config = xgb.get_config()\n",
            "            assert config['verbosity'] == 2\n",
            "\n",
            "            # Example of using the context manager xgb.config_context().\n",
            "            # The context manager will restore the previous value of the global\n",
            "            # configuration upon exiting.\n",
            "            with xgb.config_context(verbosity=0):\n",
            "                # Suppress warning caused by model generated with XGBoost version < 1.0.0\n",
            "                bst = xgb.Booster(model_file='./old_model.bin')\n",
            "            assert xgb.get_config()['verbosity'] == 2  # old value restored\n",
            "\n",
            "        Nested configuration context is also supported:\n",
            "\n",
            "        Example\n",
            "        -------\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            with xgb.config_context(verbosity=3):\n",
            "                assert xgb.get_config()[\"verbosity\"] == 3\n",
            "                with xgb.config_context(verbosity=2):\n",
            "                    assert xgb.get_config()[\"verbosity\"] == 2\n",
            "\n",
            "            xgb.set_config(verbosity=2)\n",
            "            assert xgb.get_config()[\"verbosity\"] == 2\n",
            "            with xgb.config_context(verbosity=3):\n",
            "                assert xgb.get_config()[\"verbosity\"] == 3\n",
            "\n",
            "    plot_importance(booster: Union[xgboost.sklearn.XGBModel, xgboost.core.Booster, dict], *, ax: Optional[Any] = None, height: float = 0.2, xlim: Optional[tuple] = None, ylim: Optional[tuple] = None, title: str = 'Feature importance', xlabel: str = 'Importance score', ylabel: str = 'Features', fmap: Union[str, os.PathLike] = '', importance_type: str = 'weight', max_num_features: Optional[int] = None, grid: bool = True, show_values: bool = True, values_format: str = '{v}', **kwargs: Any) -> Any\n",
            "        Plot importance based on fitted trees.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        booster :\n",
            "            Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n",
            "        ax : matplotlib Axes\n",
            "            Target axes instance. If None, new figure and axes will be created.\n",
            "        grid :\n",
            "            Turn the axes grids on or off.  Default is True (On).\n",
            "        importance_type :\n",
            "            How the importance is calculated: either \"weight\", \"gain\", or \"cover\"\n",
            "\n",
            "            * \"weight\" is the number of times a feature appears in a tree\n",
            "            * \"gain\" is the average gain of splits which use the feature\n",
            "            * \"cover\" is the average coverage of splits which use the feature\n",
            "              where coverage is defined as the number of samples affected by the split\n",
            "        max_num_features :\n",
            "            Maximum number of top features displayed on plot. If None, all features will be\n",
            "            displayed.\n",
            "        height :\n",
            "            Bar height, passed to ax.barh()\n",
            "        xlim :\n",
            "            Tuple passed to axes.xlim()\n",
            "        ylim :\n",
            "            Tuple passed to axes.ylim()\n",
            "        title :\n",
            "            Axes title. To disable, pass None.\n",
            "        xlabel :\n",
            "            X axis title label. To disable, pass None.\n",
            "        ylabel :\n",
            "            Y axis title label. To disable, pass None.\n",
            "        fmap :\n",
            "            The name of feature map file.\n",
            "        show_values :\n",
            "            Show values on plot. To disable, pass False.\n",
            "        values_format :\n",
            "            Format string for values. \"v\" will be replaced by the value of the feature\n",
            "            importance.  e.g. Pass \"{v:.2f}\" in order to limit the number of digits after\n",
            "            the decimal point to two, for each value printed on the graph.\n",
            "        kwargs :\n",
            "            Other keywords passed to ax.barh()\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        ax : matplotlib Axes\n",
            "\n",
            "    plot_tree(booster: Union[xgboost.core.Booster, xgboost.sklearn.XGBModel], *, fmap: Union[str, os.PathLike] = '', num_trees: Optional[int] = None, rankdir: Optional[str] = None, ax: Optional[Any] = None, with_stats: bool = False, tree_idx: int = 0, **kwargs: Any) -> Any\n",
            "        Plot specified tree.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        booster :\n",
            "            Booster or XGBModel instance\n",
            "        fmap: str (optional)\n",
            "           The name of feature map file\n",
            "        num_trees :\n",
            "\n",
            "            .. deprecated:: 3.0\n",
            "\n",
            "        rankdir : str, default \"TB\"\n",
            "            Passed to graphviz via graph_attr\n",
            "        ax : matplotlib Axes, default None\n",
            "            Target axes instance. If None, new figure and axes will be created.\n",
            "\n",
            "        with_stats :\n",
            "\n",
            "            .. versionadded:: 3.0\n",
            "\n",
            "            See :py:func:`to_graphviz`.\n",
            "\n",
            "        tree_idx :\n",
            "\n",
            "            .. versionadded:: 3.0\n",
            "\n",
            "            See :py:func:`to_graphviz`.\n",
            "\n",
            "        kwargs :\n",
            "            Other keywords passed to :py:func:`to_graphviz`\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        ax : matplotlib Axes\n",
            "\n",
            "    set_config(**new_config: Any) -> None\n",
            "        Set global configuration.\n",
            "\n",
            "\n",
            "        Global configuration consists of a collection of parameters that can be applied in the\n",
            "        global scope. See :ref:`global_config` for the full list of parameters supported in\n",
            "        the global configuration.\n",
            "\n",
            "\n",
            "\n",
            "        .. versionadded:: 1.4.0\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        new_config: Dict[str, Any]\n",
            "            Keyword arguments representing the parameters and their values\n",
            "\n",
            "        Example\n",
            "        -------\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            import xgboost as xgb\n",
            "\n",
            "            # Show all messages, including ones pertaining to debugging\n",
            "            xgb.set_config(verbosity=2)\n",
            "\n",
            "            # Get current value of global configuration\n",
            "            # This is a dict containing all parameters in the global configuration,\n",
            "            # including 'verbosity'\n",
            "            config = xgb.get_config()\n",
            "            assert config['verbosity'] == 2\n",
            "\n",
            "            # Example of using the context manager xgb.config_context().\n",
            "            # The context manager will restore the previous value of the global\n",
            "            # configuration upon exiting.\n",
            "            with xgb.config_context(verbosity=0):\n",
            "                # Suppress warning caused by model generated with XGBoost version < 1.0.0\n",
            "                bst = xgb.Booster(model_file='./old_model.bin')\n",
            "            assert xgb.get_config()['verbosity'] == 2  # old value restored\n",
            "\n",
            "        Nested configuration context is also supported:\n",
            "\n",
            "        Example\n",
            "        -------\n",
            "\n",
            "        .. code-block:: python\n",
            "\n",
            "            with xgb.config_context(verbosity=3):\n",
            "                assert xgb.get_config()[\"verbosity\"] == 3\n",
            "                with xgb.config_context(verbosity=2):\n",
            "                    assert xgb.get_config()[\"verbosity\"] == 2\n",
            "\n",
            "            xgb.set_config(verbosity=2)\n",
            "            assert xgb.get_config()[\"verbosity\"] == 2\n",
            "            with xgb.config_context(verbosity=3):\n",
            "                assert xgb.get_config()[\"verbosity\"] == 3\n",
            "\n",
            "    to_graphviz(booster: Union[xgboost.core.Booster, xgboost.sklearn.XGBModel], *, fmap: Union[str, os.PathLike] = '', num_trees: Optional[int] = None, rankdir: Optional[str] = None, yes_color: Optional[str] = None, no_color: Optional[str] = None, condition_node_params: Optional[dict] = None, leaf_node_params: Optional[dict] = None, with_stats: bool = False, tree_idx: int = 0, **kwargs: Any) -> Any\n",
            "        Convert specified tree to graphviz instance. IPython can automatically plot\n",
            "        the returned graphviz instance. Otherwise, you should call .render() method\n",
            "        of the returned graphviz instance.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        booster :\n",
            "            Booster or XGBModel instance\n",
            "        fmap :\n",
            "           The name of feature map file\n",
            "        num_trees :\n",
            "\n",
            "            .. deprecated:: 3.0\n",
            "\n",
            "            Specify the ordinal number of target tree\n",
            "\n",
            "        rankdir :\n",
            "            Passed to graphviz via graph_attr\n",
            "        yes_color :\n",
            "            Edge color when meets the node condition.\n",
            "        no_color :\n",
            "            Edge color when doesn't meet the node condition.\n",
            "        condition_node_params :\n",
            "            Condition node configuration for for graphviz.  Example:\n",
            "\n",
            "            .. code-block:: python\n",
            "\n",
            "                {'shape': 'box',\n",
            "                 'style': 'filled,rounded',\n",
            "                 'fillcolor': '#78bceb'}\n",
            "\n",
            "        leaf_node_params :\n",
            "            Leaf node configuration for graphviz. Example:\n",
            "\n",
            "            .. code-block:: python\n",
            "\n",
            "                {'shape': 'box',\n",
            "                 'style': 'filled',\n",
            "                 'fillcolor': '#e48038'}\n",
            "\n",
            "        with_stats :\n",
            "\n",
            "            .. versionadded:: 3.0\n",
            "\n",
            "            Controls whether the split statistics should be included.\n",
            "\n",
            "        tree_idx :\n",
            "\n",
            "            .. versionadded:: 3.0\n",
            "\n",
            "            Specify the ordinal index of target tree.\n",
            "\n",
            "        kwargs :\n",
            "            Other keywords passed to graphviz graph_attr, e.g. ``graph [ {key} = {value} ]``\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        graph: graphviz.Source\n",
            "\n",
            "    train(params: Dict[str, Any], dtrain: xgboost.core.DMatrix, num_boost_round: int = 10, *, evals: Optional[Sequence[Tuple[xgboost.core.DMatrix, str]]] = None, obj: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[numpy.ndarray, numpy.ndarray]]] = None, maximize: Optional[bool] = None, early_stopping_rounds: Optional[int] = None, evals_result: Optional[Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]] = None, verbose_eval: Union[bool, int, NoneType] = True, xgb_model: Union[str, os.PathLike, xgboost.core.Booster, bytearray, NoneType] = None, callbacks: Optional[Sequence[xgboost.callback.TrainingCallback]] = None, custom_metric: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]]] = None) -> xgboost.core.Booster\n",
            "        Train a booster with given parameters.\n",
            "\n",
            "        Parameters\n",
            "        ----------\n",
            "        params :\n",
            "            Booster params.\n",
            "        dtrain :\n",
            "            Data to be trained.\n",
            "        num_boost_round :\n",
            "            Number of boosting iterations.\n",
            "        evals :\n",
            "            List of validation sets for which metrics will evaluated during training.\n",
            "            Validation metrics will help us track the performance of the model.\n",
            "        obj\n",
            "            Custom objective function.  See :doc:`Custom Objective\n",
            "            </tutorials/custom_metric_obj>` for details.\n",
            "        maximize :\n",
            "            Whether to maximize custom_metric.\n",
            "\n",
            "        early_stopping_rounds :\n",
            "\n",
            "            Activates early stopping. Validation metric needs to improve at least once in\n",
            "            every **early_stopping_rounds** round(s) to continue training.\n",
            "\n",
            "            Requires at least one item in **evals**.\n",
            "\n",
            "            The method returns the model from the last iteration (not the best one).  Use\n",
            "            custom callback :py:class:`~xgboost.callback.EarlyStopping` or :py:meth:`model\n",
            "            slicing <xgboost.Booster.__getitem__>` if the best model is desired.  If there's\n",
            "            more than one item in **evals**, the last entry will be used for early stopping.\n",
            "\n",
            "            If there's more than one metric in the **eval_metric** parameter given in\n",
            "            **params**, the last metric will be used for early stopping.\n",
            "\n",
            "            If early stopping occurs, the model will have two additional fields:\n",
            "            ``bst.best_score``, ``bst.best_iteration``.\n",
            "\n",
            "        evals_result :\n",
            "            This dictionary stores the evaluation results of all the items in watchlist.\n",
            "\n",
            "            Example: with a watchlist containing\n",
            "            ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
            "            a parameter containing ``('eval_metric': 'logloss')``,\n",
            "            the **evals_result** returns\n",
            "\n",
            "            .. code-block:: python\n",
            "\n",
            "                {'train': {'logloss': ['0.48253', '0.35953']},\n",
            "                 'eval': {'logloss': ['0.480385', '0.357756']}}\n",
            "\n",
            "        verbose_eval :\n",
            "            Requires at least one item in **evals**.\n",
            "\n",
            "            If **verbose_eval** is True then the evaluation metric on the validation set is\n",
            "            printed at each boosting stage.\n",
            "\n",
            "            If **verbose_eval** is an integer then the evaluation metric on the validation\n",
            "            set is printed at every given **verbose_eval** boosting stage. The last boosting\n",
            "            stage / the boosting stage found by using **early_stopping_rounds** is also\n",
            "            printed.\n",
            "\n",
            "            Example: with ``verbose_eval=4`` and at least one item in **evals**, an\n",
            "            evaluation metric is printed every 4 boosting stages, instead of every boosting\n",
            "            stage.\n",
            "\n",
            "        xgb_model :\n",
            "            Xgb model to be loaded before training (allows training continuation).\n",
            "\n",
            "        callbacks :\n",
            "            List of callback functions that are applied at end of each iteration.\n",
            "            It is possible to use predefined callbacks by using\n",
            "            :ref:`Callback API <callback_api>`.\n",
            "\n",
            "            .. note::\n",
            "\n",
            "               States in callback are not preserved during training, which means callback\n",
            "               objects can not be reused for multiple training sessions without\n",
            "               reinitialization or deepcopy.\n",
            "\n",
            "            .. code-block:: python\n",
            "\n",
            "                for params in parameters_grid:\n",
            "                    # be sure to (re)initialize the callbacks before each run\n",
            "                    callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
            "                    xgboost.train(params, Xy, callbacks=callbacks)\n",
            "\n",
            "        custom_metric:\n",
            "\n",
            "            .. versionadded 1.6.0\n",
            "\n",
            "            Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`\n",
            "            for details. The metric receives transformed prediction (after applying the\n",
            "            reverse link function) when using a builtin objective, and raw output when using\n",
            "            a custom objective.\n",
            "\n",
            "        Returns\n",
            "        -------\n",
            "        Booster : a trained booster model\n",
            "\n",
            "DATA\n",
            "    __all__ = ['DMatrix', 'QuantileDMatrix', 'ExtMemQuantileDMatrix', 'Boo...\n",
            "\n",
            "VERSION\n",
            "    3.1.3\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.12/dist-packages/xgboost/__init__.py\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-enRBzWBGYjm"
      },
      "source": [
        "## （参考）ナイーブベイズ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro71wjsZGYjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59b09e62-d79d-490c-ee18-e5d8191d374d"
      },
      "source": [
        "#ナイーブベイズによる学習と計算結果\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "#ナイーブベイズによる宣言と学習\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_std,y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(gnb.score(X_train_std, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(gnb.score(X_test_std, y_test)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.6934\n",
            "正解率（test):  0.7015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KKYhQmcGYjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3395baae-7e44-4836-8761-93a483966664"
      },
      "source": [
        "help(GaussianNB)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class GaussianNB in module sklearn.naive_bayes:\n",
            "\n",
            "class GaussianNB(_BaseNB)\n",
            " |  GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
            " |\n",
            " |  Gaussian Naive Bayes (GaussianNB).\n",
            " |\n",
            " |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
            " |  For details on algorithm used to update feature means and variance online,\n",
            " |  see `Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque\n",
            " |  <http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf>`_.\n",
            " |\n",
            " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
            " |\n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  priors : array-like of shape (n_classes,), default=None\n",
            " |      Prior probabilities of the classes. If specified, the priors are not\n",
            " |      adjusted according to the data.\n",
            " |\n",
            " |  var_smoothing : float, default=1e-9\n",
            " |      Portion of the largest variance of all features that is added to\n",
            " |      variances for calculation stability.\n",
            " |\n",
            " |      .. versionadded:: 0.20\n",
            " |\n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  class_count_ : ndarray of shape (n_classes,)\n",
            " |      number of training samples observed in each class.\n",
            " |\n",
            " |  class_prior_ : ndarray of shape (n_classes,)\n",
            " |      probability of each class.\n",
            " |\n",
            " |  classes_ : ndarray of shape (n_classes,)\n",
            " |      class labels known to the classifier.\n",
            " |\n",
            " |  epsilon_ : float\n",
            " |      absolute additive value to variances.\n",
            " |\n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |\n",
            " |      .. versionadded:: 0.24\n",
            " |\n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |\n",
            " |      .. versionadded:: 1.0\n",
            " |\n",
            " |  var_ : ndarray of shape (n_classes, n_features)\n",
            " |      Variance of each feature per class.\n",
            " |\n",
            " |      .. versionadded:: 1.0\n",
            " |\n",
            " |  theta_ : ndarray of shape (n_classes, n_features)\n",
            " |      mean of each feature per class.\n",
            " |\n",
            " |  See Also\n",
            " |  --------\n",
            " |  BernoulliNB : Naive Bayes classifier for multivariate Bernoulli models.\n",
            " |  CategoricalNB : Naive Bayes classifier for categorical features.\n",
            " |  ComplementNB : Complement Naive Bayes classifier.\n",
            " |  MultinomialNB : Naive Bayes classifier for multinomial models.\n",
            " |\n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
            " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
            " |  >>> from sklearn.naive_bayes import GaussianNB\n",
            " |  >>> clf = GaussianNB()\n",
            " |  >>> clf.fit(X, Y)\n",
            " |  GaussianNB()\n",
            " |  >>> print(clf.predict([[-0.8, -1]]))\n",
            " |  [1]\n",
            " |  >>> clf_pf = GaussianNB()\n",
            " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
            " |  GaussianNB()\n",
            " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
            " |  [1]\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      GaussianNB\n",
            " |      _BaseNB\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n",
            " |      sklearn.utils._metadata_requests._MetadataRequester\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, *, priors=None, var_smoothing=1e-09)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit Gaussian Naive Bayes according to X, y.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Training vectors, where `n_samples` is the number of samples\n",
            " |          and `n_features` is the number of features.\n",
            " |\n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target values.\n",
            " |\n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |\n",
            " |          .. versionadded:: 0.17\n",
            " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Returns the instance itself.\n",
            " |\n",
            " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
            " |      Incremental fit on a batch of samples.\n",
            " |\n",
            " |      This method is expected to be called several times consecutively\n",
            " |      on different chunks of a dataset so as to implement out-of-core\n",
            " |      or online learning.\n",
            " |\n",
            " |      This is especially useful when the whole dataset is too big to fit in\n",
            " |      memory at once.\n",
            " |\n",
            " |      This method has some performance and numerical stability overhead,\n",
            " |      hence it is better to call partial_fit on chunks of data that are\n",
            " |      as large as possible (as long as fitting in the memory budget) to\n",
            " |      hide the overhead.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Training vectors, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features.\n",
            " |\n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target values.\n",
            " |\n",
            " |      classes : array-like of shape (n_classes,), default=None\n",
            " |          List of all the classes that can possibly appear in the y vector.\n",
            " |\n",
            " |          Must be provided at the first call to partial_fit, can be omitted\n",
            " |          in subsequent calls.\n",
            " |\n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Weights applied to individual samples (1. for unweighted).\n",
            " |\n",
            " |          .. versionadded:: 0.17\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Returns the instance itself.\n",
            " |\n",
            " |  set_fit_request(self: sklearn.naive_bayes.GaussianNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.GaussianNB from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            " |      Request metadata passed to the ``fit`` method.\n",
            " |\n",
            " |      Note that this method is only relevant if\n",
            " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |\n",
            " |      The options for each parameter are:\n",
            " |\n",
            " |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n",
            " |\n",
            " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n",
            " |\n",
            " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            " |\n",
            " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            " |\n",
            " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            " |      existing request. This allows you to change the request for some\n",
            " |      parameters and not others.\n",
            " |\n",
            " |      .. versionadded:: 1.3\n",
            " |\n",
            " |      .. note::\n",
            " |          This method is only relevant if this estimator is used as a\n",
            " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          The updated object.\n",
            " |\n",
            " |  set_partial_fit_request(self: sklearn.naive_bayes.GaussianNB, *, classes: Union[bool, NoneType, str] = '$UNCHANGED$', sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.GaussianNB from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            " |      Request metadata passed to the ``partial_fit`` method.\n",
            " |\n",
            " |      Note that this method is only relevant if\n",
            " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |\n",
            " |      The options for each parameter are:\n",
            " |\n",
            " |      - ``True``: metadata is requested, and passed to ``partial_fit`` if provided. The request is ignored if metadata is not provided.\n",
            " |\n",
            " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``partial_fit``.\n",
            " |\n",
            " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            " |\n",
            " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            " |\n",
            " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            " |      existing request. This allows you to change the request for some\n",
            " |      parameters and not others.\n",
            " |\n",
            " |      .. versionadded:: 1.3\n",
            " |\n",
            " |      .. note::\n",
            " |          This method is only relevant if this estimator is used as a\n",
            " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      classes : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``classes`` parameter in ``partial_fit``.\n",
            " |\n",
            " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``sample_weight`` parameter in ``partial_fit``.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          The updated object.\n",
            " |\n",
            " |  set_score_request(self: sklearn.naive_bayes.GaussianNB, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.naive_bayes.GaussianNB from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n",
            " |      Request metadata passed to the ``score`` method.\n",
            " |\n",
            " |      Note that this method is only relevant if\n",
            " |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n",
            " |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |\n",
            " |      The options for each parameter are:\n",
            " |\n",
            " |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n",
            " |\n",
            " |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n",
            " |\n",
            " |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n",
            " |\n",
            " |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n",
            " |\n",
            " |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n",
            " |      existing request. This allows you to change the request for some\n",
            " |      parameters and not others.\n",
            " |\n",
            " |      .. versionadded:: 1.3\n",
            " |\n",
            " |      .. note::\n",
            " |          This method is only relevant if this estimator is used as a\n",
            " |          sub-estimator of a meta-estimator, e.g. used inside a\n",
            " |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n",
            " |          Metadata routing for ``sample_weight`` parameter in ``score``.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          The updated object.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __abstractmethods__ = frozenset()\n",
            " |\n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from _BaseNB:\n",
            " |\n",
            " |  predict(self, X)\n",
            " |      Perform classification on an array of test vectors X.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : ndarray of shape (n_samples,)\n",
            " |          Predicted target values for X.\n",
            " |\n",
            " |  predict_joint_log_proba(self, X)\n",
            " |      Return joint log probability estimates for the test vector X.\n",
            " |\n",
            " |      For each row x of X and class y, the joint log probability is given by\n",
            " |      ``log P(x, y) = log P(y) + log P(x|y),``\n",
            " |      where ``log P(y)`` is the class prior probability and ``log P(x|y)`` is\n",
            " |      the class-conditional probability.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : ndarray of shape (n_samples, n_classes)\n",
            " |          Returns the joint log-probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |\n",
            " |  predict_log_proba(self, X)\n",
            " |      Return log-probability estimates for the test vector X.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the log-probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |\n",
            " |  predict_proba(self, X)\n",
            " |      Return probability estimates for the test vector X.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      C : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the samples for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |\n",
            " |  __sklearn_tags__(self)\n",
            " |\n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |\n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |\n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |\n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |\n",
            " |  __getstate__(self)\n",
            " |      Helper for pickle.\n",
            " |\n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  __setstate__(self, state)\n",
            " |\n",
            " |  __sklearn_clone__(self)\n",
            " |\n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |\n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |\n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |\n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            " |\n",
            " |  get_metadata_routing(self)\n",
            " |      Get metadata routing of this object.\n",
            " |\n",
            " |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n",
            " |      mechanism works.\n",
            " |\n",
            " |      Returns\n",
            " |      -------\n",
            " |      routing : MetadataRequest\n",
            " |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n",
            " |          routing information.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n",
            " |\n",
            " |  __init_subclass__(**kwargs)\n",
            " |      Set the ``set_{method}_request`` methods.\n",
            " |\n",
            " |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n",
            " |      looks for the information available in the set default values which are\n",
            " |      set using ``__metadata_request__*`` class attributes, or inferred\n",
            " |      from method signatures.\n",
            " |\n",
            " |      The ``__metadata_request__*`` class attributes are used when a method\n",
            " |      does not explicitly accept a metadata through its arguments or if the\n",
            " |      developer would like to specify a request value for those metadata\n",
            " |      which are different from the default ``None``.\n",
            " |\n",
            " |      References\n",
            " |      ----------\n",
            " |      .. [1] https://www.python.org/dev/peps/pep-0487\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVbH0xRCGYjk"
      },
      "source": [
        "## 演習1．　乳がんデータに対してk近傍法を適用"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 乳がんデータの読み込み\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#csvファイルの読み込み\n",
        "bc_source_df = pd.read_csv('/content/drive/MyDrive/DataScience/breast-cancer-wisconsin.csv',sep=',')\n",
        "\n",
        "# 欠損文字の置換\n",
        "bc_df = bc_source_df.replace(\"?\", \"5\")\n",
        "\n",
        "# 数値への置換\n",
        "bc_df = bc_df.astype({'Bare Nuclei':'int64'})\n",
        "\n",
        "#変数の設定（説明変数は適当に選択）\n",
        "X = bc_df.iloc[:, [1,2,3,4,5,6,7,8,9]]\n",
        "y = bc_df.iloc[:,[10]]\n",
        "\n",
        "# k近傍法の適用\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_std, y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(knn.score(X_train_std, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(knn.score(X_test_std, y_test)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wIA1v7JxdL6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b60f63-027d-4c46-d5bb-f88560ed8f19"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.8026\n",
            "正解率（test):  0.7201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 演習2．　乳がんデータに対してニューラルネットワークを適用"
      ],
      "metadata": {
        "id": "NKM1coqvgx8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データは演習１を利用\n",
        "#多層パーセプトロンによる学習\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#ニューラルネットワークモデル（多層パーセプトロン）による宣言と学習\n",
        "knn = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n",
        "knn.fit(X_train_std,y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(knn.score(X_train_std, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(knn.score(X_test_std, y_test)))\n"
      ],
      "metadata": {
        "id": "FalLOarKg45R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32ad4067-5bb5-4a66-f682-41572d947948"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.7416\n",
            "正解率（test):  0.7090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 演習3．　乳がんデータに対してXGBoostを適用"
      ],
      "metadata": {
        "id": "56jmlkxqhDrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データは演習１を利用\n",
        "import xgboost as xgb\n",
        "\n",
        "knn = xgb.XGBClassifier()\n",
        "knn.fit(X_train_std, y_train)\n",
        "\n",
        "predict = knn.predict(X_test_std)\n",
        "\n",
        "print('正解率： {:.4f}'.format(accuracy_score(predict,y_test)))\n"
      ],
      "metadata": {
        "id": "eQ5c0CUAhJFk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d67cc0f5-6651-40ef-ffe0-adcb9ae125c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率： 0.7201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 演習4．　乳がんデータに対してナイーブベイズを適用"
      ],
      "metadata": {
        "id": "yyZ_pICXg5ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データは演習１を利用\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "#ナイーブベイズによる宣言と学習\n",
        "knn = GaussianNB()\n",
        "knn.fit(X_train_std,y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(knn.score(X_train_std, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(knn.score(X_test_std, y_test)))\n"
      ],
      "metadata": {
        "id": "xKQURbD0hBdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84284b57-07c0-40b0-da4f-842dd4b37006"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.6934\n",
            "正解率（test):  0.7015\n"
          ]
        }
      ]
    }
  ]
}