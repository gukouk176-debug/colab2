{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gukouk176-debug/colab2/blob/main/DataScience_08_ipynb_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_NXre4ACsbk"
      },
      "source": [
        "# 第8回講義 分類１\n",
        "損失関数型\n",
        "+ ロジスティック回帰    \n",
        "+ サポートベクターマシン（SVM）\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV4d4rb4Csbb"
      },
      "source": [
        "## 全講義共通初期設定\n",
        "+ 警告の非表示(実装時は非推奨)\n",
        "+ numpy pandas小数点以下桁数の表示設定\n",
        "+ pandas全データ表示設定\n",
        "+ Google driveへの接続"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K47orZqqCsbj"
      },
      "source": [
        "# ワーニングを非表示にする\n",
        "# この設定は不都合が見えなくなる為、お勧めしない\n",
        "# 今回は教育資料用に、出力を簡素化する為に利用する\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "# モジュールの読み込み\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 小数点以下桁数の表示設定\n",
        "np.set_printoptions(precision = 3)\n",
        "pd.options.display.precision = 3\n",
        "\n",
        "# pandasの全データ表示設定\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMKLiwyIuAVZ",
        "outputId": "afb34ff2-cbcc-490e-dd4c-7b0364660b67"
      },
      "source": [
        "#google driveに接続\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp7b1MJREYII",
        "outputId": "52fc419f-603b-403e-b51c-6d9bb8ad81a8"
      },
      "source": [
        "#google driveと接続できたかを確認\n",
        "!ls drive/MyDrive/DataScience"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'3(00000).txt'\t'3(00004).txt'\t\t       img\t\t   Wholesale_customers_data.csv\n",
            "'3(00001).txt'\t breast-cancer-wisconsin.csv   imports-85.csv\t   wine.csv\n",
            "'3(00002).txt'\t data\t\t\t       iris.csv\n",
            "'3(00003).txt'\t example.xlsx\t\t       titanic_train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpgDwH9pHU3g"
      },
      "source": [
        "# １．ロジスティック回帰\n",
        "\n",
        "+ 自動車価格データを用いる\n",
        "+ 自動車価格が10000(10K)ドル以上・以下を分類する"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#ライブラリpandasを使ったcsvデータの読み込み,sepで区切り記号を設定\n",
        "auto_source_df = pd.read_csv(\"/content/drive/MyDrive/DataScience/imports-85.csv\", sep=\",\", header=None)\n",
        "#最初の五行だけ表示\n",
        "auto_source_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "-rQqUsGrbSFV",
        "outputId": "cd0d38ee-5da1-4887-dd7a-c78333512a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   0    1            2    3    4     5            6    7      8     9      10  \\\n",
              "0   3    ?  alfa-romero  gas  std   two  convertible  rwd  front  88.6  168.8   \n",
              "1   3    ?  alfa-romero  gas  std   two  convertible  rwd  front  88.6  168.8   \n",
              "2   1    ?  alfa-romero  gas  std   two    hatchback  rwd  front  94.5  171.2   \n",
              "3   2  164         audi  gas  std  four        sedan  fwd  front  99.8  176.6   \n",
              "4   2  164         audi  gas  std  four        sedan  4wd  front  99.4  176.6   \n",
              "\n",
              "     11    12    13    14    15   16    17    18    19    20   21    22  23  \\\n",
              "0  64.1  48.8  2548  dohc  four  130  mpfi  3.47  2.68   9.0  111  5000  21   \n",
              "1  64.1  48.8  2548  dohc  four  130  mpfi  3.47  2.68   9.0  111  5000  21   \n",
              "2  65.5  52.4  2823  ohcv   six  152  mpfi  2.68  3.47   9.0  154  5000  19   \n",
              "3  66.2  54.3  2337   ohc  four  109  mpfi  3.19  3.40  10.0  102  5500  24   \n",
              "4  66.4  54.3  2824   ohc  five  136  mpfi  3.19  3.40   8.0  115  5500  18   \n",
              "\n",
              "   24     25  \n",
              "0  27  13495  \n",
              "1  27  16500  \n",
              "2  26  16500  \n",
              "3  30  13950  \n",
              "4  22  17450  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4c7efa7c-ba9b-4ba6-8d6f-855b2b34d41d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>?</td>\n",
              "      <td>alfa-romero</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>convertible</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548</td>\n",
              "      <td>dohc</td>\n",
              "      <td>four</td>\n",
              "      <td>130</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>111</td>\n",
              "      <td>5000</td>\n",
              "      <td>21</td>\n",
              "      <td>27</td>\n",
              "      <td>13495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>?</td>\n",
              "      <td>alfa-romero</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>convertible</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548</td>\n",
              "      <td>dohc</td>\n",
              "      <td>four</td>\n",
              "      <td>130</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>111</td>\n",
              "      <td>5000</td>\n",
              "      <td>21</td>\n",
              "      <td>27</td>\n",
              "      <td>16500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>alfa-romero</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>hatchback</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>94.5</td>\n",
              "      <td>171.2</td>\n",
              "      <td>65.5</td>\n",
              "      <td>52.4</td>\n",
              "      <td>2823</td>\n",
              "      <td>ohcv</td>\n",
              "      <td>six</td>\n",
              "      <td>152</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>2.68</td>\n",
              "      <td>3.47</td>\n",
              "      <td>9.0</td>\n",
              "      <td>154</td>\n",
              "      <td>5000</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>16500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>164</td>\n",
              "      <td>audi</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>fwd</td>\n",
              "      <td>front</td>\n",
              "      <td>99.8</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.2</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2337</td>\n",
              "      <td>ohc</td>\n",
              "      <td>four</td>\n",
              "      <td>109</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>10.0</td>\n",
              "      <td>102</td>\n",
              "      <td>5500</td>\n",
              "      <td>24</td>\n",
              "      <td>30</td>\n",
              "      <td>13950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>164</td>\n",
              "      <td>audi</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>4wd</td>\n",
              "      <td>front</td>\n",
              "      <td>99.4</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.4</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2824</td>\n",
              "      <td>ohc</td>\n",
              "      <td>five</td>\n",
              "      <td>136</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>8.0</td>\n",
              "      <td>115</td>\n",
              "      <td>5500</td>\n",
              "      <td>18</td>\n",
              "      <td>22</td>\n",
              "      <td>17450</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4c7efa7c-ba9b-4ba6-8d6f-855b2b34d41d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4c7efa7c-ba9b-4ba6-8d6f-855b2b34d41d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4c7efa7c-ba9b-4ba6-8d6f-855b2b34d41d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-834feb97-9ee2-43f3-af36-eb302d839b0c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-834feb97-9ee2-43f3-af36-eb302d839b0c')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-834feb97-9ee2-43f3-af36-eb302d839b0c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_source_df.columns = ['symboling', 'normalized-losses', 'make', 'fule-type', 'aspiration', 'num-of=doors',\n",
        "               'body-style', 'drive-weels', 'engine-location', 'wheel-base', 'length', 'width', 'height',\n",
        "               'curb-weight', 'engin-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore',\n",
        "               'stroke', 'compression-ratio', 'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
        "\n",
        "auto_source_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "WPletNh0bWKR",
        "outputId": "cb23544e-0f24-491f-f77c-fe97d8941b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   symboling normalized-losses         make fule-type aspiration num-of=doors  \\\n",
              "0          3                 ?  alfa-romero       gas        std          two   \n",
              "1          3                 ?  alfa-romero       gas        std          two   \n",
              "2          1                 ?  alfa-romero       gas        std          two   \n",
              "3          2               164         audi       gas        std         four   \n",
              "4          2               164         audi       gas        std         four   \n",
              "\n",
              "    body-style drive-weels engine-location  wheel-base  length  width  height  \\\n",
              "0  convertible         rwd           front        88.6   168.8   64.1    48.8   \n",
              "1  convertible         rwd           front        88.6   168.8   64.1    48.8   \n",
              "2    hatchback         rwd           front        94.5   171.2   65.5    52.4   \n",
              "3        sedan         fwd           front        99.8   176.6   66.2    54.3   \n",
              "4        sedan         4wd           front        99.4   176.6   66.4    54.3   \n",
              "\n",
              "   curb-weight engin-type num-of-cylinders  engine-size fuel-system  bore  \\\n",
              "0         2548       dohc             four          130        mpfi  3.47   \n",
              "1         2548       dohc             four          130        mpfi  3.47   \n",
              "2         2823       ohcv              six          152        mpfi  2.68   \n",
              "3         2337        ohc             four          109        mpfi  3.19   \n",
              "4         2824        ohc             five          136        mpfi  3.19   \n",
              "\n",
              "  stroke  compression-ratio horsepower peak-rpm  city-mpg  highway-mpg  price  \n",
              "0   2.68                9.0        111     5000        21           27  13495  \n",
              "1   2.68                9.0        111     5000        21           27  16500  \n",
              "2   3.47                9.0        154     5000        19           26  16500  \n",
              "3   3.40               10.0        102     5500        24           30  13950  \n",
              "4   3.40                8.0        115     5500        18           22  17450  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-431fa971-baeb-461d-b3a4-7fc0e458d504\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symboling</th>\n",
              "      <th>normalized-losses</th>\n",
              "      <th>make</th>\n",
              "      <th>fule-type</th>\n",
              "      <th>aspiration</th>\n",
              "      <th>num-of=doors</th>\n",
              "      <th>body-style</th>\n",
              "      <th>drive-weels</th>\n",
              "      <th>engine-location</th>\n",
              "      <th>wheel-base</th>\n",
              "      <th>length</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>curb-weight</th>\n",
              "      <th>engin-type</th>\n",
              "      <th>num-of-cylinders</th>\n",
              "      <th>engine-size</th>\n",
              "      <th>fuel-system</th>\n",
              "      <th>bore</th>\n",
              "      <th>stroke</th>\n",
              "      <th>compression-ratio</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>peak-rpm</th>\n",
              "      <th>city-mpg</th>\n",
              "      <th>highway-mpg</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>?</td>\n",
              "      <td>alfa-romero</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>convertible</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548</td>\n",
              "      <td>dohc</td>\n",
              "      <td>four</td>\n",
              "      <td>130</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>111</td>\n",
              "      <td>5000</td>\n",
              "      <td>21</td>\n",
              "      <td>27</td>\n",
              "      <td>13495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>?</td>\n",
              "      <td>alfa-romero</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>convertible</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>88.6</td>\n",
              "      <td>168.8</td>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>2548</td>\n",
              "      <td>dohc</td>\n",
              "      <td>four</td>\n",
              "      <td>130</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.47</td>\n",
              "      <td>2.68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>111</td>\n",
              "      <td>5000</td>\n",
              "      <td>21</td>\n",
              "      <td>27</td>\n",
              "      <td>16500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>?</td>\n",
              "      <td>alfa-romero</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>two</td>\n",
              "      <td>hatchback</td>\n",
              "      <td>rwd</td>\n",
              "      <td>front</td>\n",
              "      <td>94.5</td>\n",
              "      <td>171.2</td>\n",
              "      <td>65.5</td>\n",
              "      <td>52.4</td>\n",
              "      <td>2823</td>\n",
              "      <td>ohcv</td>\n",
              "      <td>six</td>\n",
              "      <td>152</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>2.68</td>\n",
              "      <td>3.47</td>\n",
              "      <td>9.0</td>\n",
              "      <td>154</td>\n",
              "      <td>5000</td>\n",
              "      <td>19</td>\n",
              "      <td>26</td>\n",
              "      <td>16500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>164</td>\n",
              "      <td>audi</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>fwd</td>\n",
              "      <td>front</td>\n",
              "      <td>99.8</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.2</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2337</td>\n",
              "      <td>ohc</td>\n",
              "      <td>four</td>\n",
              "      <td>109</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>10.0</td>\n",
              "      <td>102</td>\n",
              "      <td>5500</td>\n",
              "      <td>24</td>\n",
              "      <td>30</td>\n",
              "      <td>13950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>164</td>\n",
              "      <td>audi</td>\n",
              "      <td>gas</td>\n",
              "      <td>std</td>\n",
              "      <td>four</td>\n",
              "      <td>sedan</td>\n",
              "      <td>4wd</td>\n",
              "      <td>front</td>\n",
              "      <td>99.4</td>\n",
              "      <td>176.6</td>\n",
              "      <td>66.4</td>\n",
              "      <td>54.3</td>\n",
              "      <td>2824</td>\n",
              "      <td>ohc</td>\n",
              "      <td>five</td>\n",
              "      <td>136</td>\n",
              "      <td>mpfi</td>\n",
              "      <td>3.19</td>\n",
              "      <td>3.40</td>\n",
              "      <td>8.0</td>\n",
              "      <td>115</td>\n",
              "      <td>5500</td>\n",
              "      <td>18</td>\n",
              "      <td>22</td>\n",
              "      <td>17450</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-431fa971-baeb-461d-b3a4-7fc0e458d504')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-431fa971-baeb-461d-b3a4-7fc0e458d504 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-431fa971-baeb-461d-b3a4-7fc0e458d504');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ff5e73a0-b45e-4a45-b405-310104ab1152\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ff5e73a0-b45e-4a45-b405-310104ab1152')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ff5e73a0-b45e-4a45-b405-310104ab1152 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4つの列だけ抜き出す\n",
        "auto_df = auto_source_df[['width', 'height', 'horsepower', 'price']]\n",
        "\n",
        "# 不適切なデータ（？）を欠損値に置換\n",
        "auto_df = auto_df.replace('?', np.nan).dropna()\n",
        "\n",
        "#データ形式の確認\n",
        "auto_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVc58qXZbb7n",
        "outputId": "00739a08-9143-42a0-b150-85e072ee5765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(199, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_df = auto_df.assign(price=pd.to_numeric(auto_df.price))\n",
        "auto_df = auto_df.assign(horsepower=pd.to_numeric(auto_df.horsepower))\n",
        "\n",
        "auto_df.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov6ABxWHbsb0",
        "outputId": "283496d5-530c-4c4f-d1ad-2705748fbdfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "width         float64\n",
              "height        float64\n",
              "horsepower      int64\n",
              "price           int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_df['price_over_10K'] = auto_df['price'].map(lambda x: 1 if x > 10000 else 0)\n",
        "auto_df = auto_df[['width', 'height', 'horsepower', 'price_over_10K']]\n",
        "auto_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bI77wBy9btun",
        "outputId": "c96aee38-00a5-4ebc-8b89-bd7ea62e5bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     width  height  horsepower  price_over_10K\n",
              "0     64.1    48.8         111               1\n",
              "1     64.1    48.8         111               1\n",
              "2     65.5    52.4         154               1\n",
              "3     66.2    54.3         102               1\n",
              "4     66.4    54.3         115               1\n",
              "5     66.3    53.1         110               1\n",
              "6     71.4    55.7         110               1\n",
              "7     71.4    55.7         110               1\n",
              "8     71.4    55.9         140               1\n",
              "10    64.8    54.3         101               1\n",
              "11    64.8    54.3         101               1\n",
              "12    64.8    54.3         121               1\n",
              "13    64.8    54.3         121               1\n",
              "14    66.9    55.7         121               1\n",
              "15    66.9    55.7         182               1\n",
              "16    67.9    53.7         182               1\n",
              "17    70.9    56.3         182               1\n",
              "18    60.3    53.2          48               0\n",
              "19    63.6    52.0          70               0\n",
              "20    63.6    52.0          70               0\n",
              "21    63.8    50.8          68               0\n",
              "22    63.8    50.8          68               0\n",
              "23    63.8    50.8         102               0\n",
              "24    63.8    50.6          68               0\n",
              "25    63.8    50.6          68               0\n",
              "26    63.8    50.6          68               0\n",
              "27    63.8    50.6         102               0\n",
              "28    64.6    59.8          88               0\n",
              "29    66.3    50.2         145               1\n",
              "30    63.9    50.8          58               0\n",
              "31    63.9    50.8          76               0\n",
              "32    64.0    52.6          60               0\n",
              "33    64.0    52.6          76               0\n",
              "34    64.0    52.6          76               0\n",
              "35    64.0    54.5          76               0\n",
              "36    63.9    58.3          76               0\n",
              "37    65.2    53.3          86               0\n",
              "38    65.2    53.3          86               0\n",
              "39    65.2    54.1          86               0\n",
              "40    62.5    54.1          86               1\n",
              "41    65.2    54.1         101               1\n",
              "42    66.0    51.0         100               1\n",
              "43    61.8    53.5          78               0\n",
              "46    65.2    51.4          90               1\n",
              "47    69.6    52.8         176               1\n",
              "48    69.6    52.8         176               1\n",
              "49    70.6    47.8         262               1\n",
              "50    64.2    54.1          68               0\n",
              "51    64.2    54.1          68               0\n",
              "52    64.2    54.1          68               0\n",
              "53    64.2    54.1          68               0\n",
              "54    64.2    54.1          68               0\n",
              "55    65.7    49.6         101               1\n",
              "56    65.7    49.6         101               1\n",
              "57    65.7    49.6         101               1\n",
              "58    65.7    49.6         135               1\n",
              "59    66.5    53.7          84               0\n",
              "60    66.5    55.5          84               0\n",
              "61    66.5    53.7          84               1\n",
              "62    66.5    55.5          84               1\n",
              "63    66.5    55.5          64               1\n",
              "64    66.5    55.5          84               1\n",
              "65    66.1    54.4         120               1\n",
              "66    66.1    54.4          72               1\n",
              "67    70.3    56.5         123               1\n",
              "68    70.3    58.7         123               1\n",
              "69    70.3    54.9         123               1\n",
              "70    71.7    56.3         123               1\n",
              "71    71.7    56.5         155               1\n",
              "72    70.5    50.8         155               1\n",
              "73    71.7    56.7         184               1\n",
              "74    72.0    55.4         184               1\n",
              "75    68.0    54.8         175               1\n",
              "76    64.4    50.8          68               0\n",
              "77    64.4    50.8          68               0\n",
              "78    64.4    50.8          68               0\n",
              "79    63.8    50.8         102               0\n",
              "80    65.4    49.4         116               0\n",
              "81    65.4    49.4          88               0\n",
              "82    66.3    50.2         145               1\n",
              "83    66.3    50.2         145               1\n",
              "84    66.3    50.2         145               1\n",
              "85    65.4    51.6          88               0\n",
              "86    65.4    51.6          88               0\n",
              "87    65.4    51.6         116               0\n",
              "88    65.4    51.6         116               0\n",
              "89    63.8    54.5          69               0\n",
              "90    63.8    54.5          55               0\n",
              "91    63.8    54.5          69               0\n",
              "92    63.8    54.5          69               0\n",
              "93    63.8    53.5          69               0\n",
              "94    63.8    54.5          69               0\n",
              "95    63.8    53.3          69               0\n",
              "96    63.8    54.5          69               0\n",
              "97    63.8    53.5          69               0\n",
              "98    63.8    53.3          69               0\n",
              "99    65.2    54.7          97               0\n",
              "100   65.2    54.7          97               0\n",
              "101   66.5    55.1         152               1\n",
              "102   66.5    56.1         152               1\n",
              "103   66.5    55.1         152               1\n",
              "104   67.9    49.7         160               1\n",
              "105   67.9    49.7         200               1\n",
              "106   67.9    49.7         160               1\n",
              "107   68.4    56.7          97               1\n",
              "108   68.4    56.7          95               1\n",
              "109   68.4    58.7          97               1\n",
              "110   68.4    58.7          95               1\n",
              "111   68.4    56.7          95               1\n",
              "112   68.4    56.7          95               1\n",
              "113   68.4    56.7          95               1\n",
              "114   68.4    58.7          95               1\n",
              "115   68.4    56.7          97               1\n",
              "116   68.4    56.7          95               1\n",
              "117   68.3    56.0         142               1\n",
              "118   63.8    50.8          68               0\n",
              "119   63.8    50.8         102               0\n",
              "120   63.8    50.6          68               0\n",
              "121   63.8    50.8          68               0\n",
              "122   63.8    50.8          68               0\n",
              "123   64.6    59.8          88               0\n",
              "124   66.3    50.2         145               1\n",
              "125   68.3    50.2         143               1\n",
              "126   65.0    51.6         207               1\n",
              "127   65.0    51.6         207               1\n",
              "128   65.0    51.6         207               1\n",
              "132   66.5    56.1         110               1\n",
              "133   66.5    56.1         110               1\n",
              "134   66.5    56.1         110               1\n",
              "135   66.5    56.1         110               1\n",
              "136   66.5    56.1         160               1\n",
              "137   66.5    56.1         160               1\n",
              "138   63.4    53.7          69               0\n",
              "139   63.6    53.7          73               0\n",
              "140   63.8    55.7          73               0\n",
              "141   65.4    52.5          82               0\n",
              "142   65.4    52.5          82               0\n",
              "143   65.4    52.5          94               0\n",
              "144   65.4    54.3          82               0\n",
              "145   65.4    54.3         111               1\n",
              "146   65.4    53.0          82               0\n",
              "147   65.4    53.0          94               1\n",
              "148   65.4    54.9          82               0\n",
              "149   65.4    54.9         111               1\n",
              "150   63.6    54.5          62               0\n",
              "151   63.6    54.5          62               0\n",
              "152   63.6    54.5          62               0\n",
              "153   63.6    59.1          62               0\n",
              "154   63.6    59.1          62               0\n",
              "155   63.6    59.1          62               0\n",
              "156   64.4    53.0          70               0\n",
              "157   64.4    52.8          70               0\n",
              "158   64.4    53.0          56               0\n",
              "159   64.4    52.8          56               0\n",
              "160   64.4    53.0          70               0\n",
              "161   64.4    52.8          70               0\n",
              "162   64.4    52.8          70               0\n",
              "163   64.0    52.6          70               0\n",
              "164   64.0    52.6          70               0\n",
              "165   64.0    52.6         112               0\n",
              "166   64.0    52.6         112               0\n",
              "167   65.6    52.0         116               0\n",
              "168   65.6    52.0         116               0\n",
              "169   65.6    52.0         116               0\n",
              "170   65.6    52.0         116               1\n",
              "171   65.6    52.0         116               1\n",
              "172   65.6    53.0         116               1\n",
              "173   66.5    54.9          92               0\n",
              "174   66.5    54.9          73               1\n",
              "175   66.5    53.9          92               0\n",
              "176   66.5    54.9          92               1\n",
              "177   66.5    53.9          92               1\n",
              "178   67.7    52.0         161               1\n",
              "179   67.7    52.0         161               1\n",
              "180   66.5    54.1         156               1\n",
              "181   66.5    54.1         156               1\n",
              "182   65.5    55.7          52               0\n",
              "183   65.5    55.7          85               0\n",
              "184   65.5    55.7          52               0\n",
              "185   65.5    55.7          85               0\n",
              "186   65.5    55.7          85               0\n",
              "187   65.5    55.7          68               0\n",
              "188   65.5    55.7         100               0\n",
              "189   64.2    55.6          90               1\n",
              "190   64.0    51.4          90               0\n",
              "191   66.9    55.1         110               1\n",
              "192   66.9    55.1          68               1\n",
              "193   66.9    55.1          88               1\n",
              "194   67.2    56.2         114               1\n",
              "195   67.2    57.5         114               1\n",
              "196   67.2    56.2         114               1\n",
              "197   67.2    57.5         114               1\n",
              "198   67.2    56.2         162               1\n",
              "199   67.2    57.5         162               1\n",
              "200   68.9    55.5         114               1\n",
              "201   68.8    55.5         160               1\n",
              "202   68.9    55.5         134               1\n",
              "203   68.9    55.5         106               1\n",
              "204   68.9    55.5         114               1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a28fa736-d434-4eda-8d71-a66794f0ecb5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>price_over_10K</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>111</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>64.1</td>\n",
              "      <td>48.8</td>\n",
              "      <td>111</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>65.5</td>\n",
              "      <td>52.4</td>\n",
              "      <td>154</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>66.2</td>\n",
              "      <td>54.3</td>\n",
              "      <td>102</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>66.4</td>\n",
              "      <td>54.3</td>\n",
              "      <td>115</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>66.3</td>\n",
              "      <td>53.1</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>71.4</td>\n",
              "      <td>55.7</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>71.4</td>\n",
              "      <td>55.7</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>71.4</td>\n",
              "      <td>55.9</td>\n",
              "      <td>140</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>64.8</td>\n",
              "      <td>54.3</td>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>64.8</td>\n",
              "      <td>54.3</td>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>64.8</td>\n",
              "      <td>54.3</td>\n",
              "      <td>121</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>64.8</td>\n",
              "      <td>54.3</td>\n",
              "      <td>121</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>66.9</td>\n",
              "      <td>55.7</td>\n",
              "      <td>121</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>66.9</td>\n",
              "      <td>55.7</td>\n",
              "      <td>182</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>67.9</td>\n",
              "      <td>53.7</td>\n",
              "      <td>182</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>70.9</td>\n",
              "      <td>56.3</td>\n",
              "      <td>182</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>60.3</td>\n",
              "      <td>53.2</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>63.6</td>\n",
              "      <td>52.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>63.6</td>\n",
              "      <td>52.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.8</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.8</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.8</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.6</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.6</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.6</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.6</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>64.6</td>\n",
              "      <td>59.8</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>66.3</td>\n",
              "      <td>50.2</td>\n",
              "      <td>145</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>63.9</td>\n",
              "      <td>50.8</td>\n",
              "      <td>58</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>63.9</td>\n",
              "      <td>50.8</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>64.0</td>\n",
              "      <td>52.6</td>\n",
              "      <td>60</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>64.0</td>\n",
              "      <td>52.6</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>64.0</td>\n",
              "      <td>52.6</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>64.0</td>\n",
              "      <td>54.5</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>63.9</td>\n",
              "      <td>58.3</td>\n",
              "      <td>76</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>65.2</td>\n",
              "      <td>53.3</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>65.2</td>\n",
              "      <td>53.3</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>65.2</td>\n",
              "      <td>54.1</td>\n",
              "      <td>86</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>62.5</td>\n",
              "      <td>54.1</td>\n",
              "      <td>86</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>65.2</td>\n",
              "      <td>54.1</td>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>66.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>61.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>65.2</td>\n",
              "      <td>51.4</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>69.6</td>\n",
              "      <td>52.8</td>\n",
              "      <td>176</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>69.6</td>\n",
              "      <td>52.8</td>\n",
              "      <td>176</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>70.6</td>\n",
              "      <td>47.8</td>\n",
              "      <td>262</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>64.2</td>\n",
              "      <td>54.1</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>64.2</td>\n",
              "      <td>54.1</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>64.2</td>\n",
              "      <td>54.1</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>64.2</td>\n",
              "      <td>54.1</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>64.2</td>\n",
              "      <td>54.1</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>65.7</td>\n",
              "      <td>49.6</td>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>65.7</td>\n",
              "      <td>49.6</td>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>65.7</td>\n",
              "      <td>49.6</td>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>65.7</td>\n",
              "      <td>49.6</td>\n",
              "      <td>135</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>66.5</td>\n",
              "      <td>53.7</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>66.5</td>\n",
              "      <td>55.5</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>66.5</td>\n",
              "      <td>53.7</td>\n",
              "      <td>84</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>66.5</td>\n",
              "      <td>55.5</td>\n",
              "      <td>84</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>66.5</td>\n",
              "      <td>55.5</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>66.5</td>\n",
              "      <td>55.5</td>\n",
              "      <td>84</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>66.1</td>\n",
              "      <td>54.4</td>\n",
              "      <td>120</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>66.1</td>\n",
              "      <td>54.4</td>\n",
              "      <td>72</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>70.3</td>\n",
              "      <td>56.5</td>\n",
              "      <td>123</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>70.3</td>\n",
              "      <td>58.7</td>\n",
              "      <td>123</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>70.3</td>\n",
              "      <td>54.9</td>\n",
              "      <td>123</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>71.7</td>\n",
              "      <td>56.3</td>\n",
              "      <td>123</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>71.7</td>\n",
              "      <td>56.5</td>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>70.5</td>\n",
              "      <td>50.8</td>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>71.7</td>\n",
              "      <td>56.7</td>\n",
              "      <td>184</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>72.0</td>\n",
              "      <td>55.4</td>\n",
              "      <td>184</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>68.0</td>\n",
              "      <td>54.8</td>\n",
              "      <td>175</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>64.4</td>\n",
              "      <td>50.8</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>64.4</td>\n",
              "      <td>50.8</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>64.4</td>\n",
              "      <td>50.8</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.8</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>65.4</td>\n",
              "      <td>49.4</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>65.4</td>\n",
              "      <td>49.4</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>66.3</td>\n",
              "      <td>50.2</td>\n",
              "      <td>145</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>66.3</td>\n",
              "      <td>50.2</td>\n",
              "      <td>145</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>66.3</td>\n",
              "      <td>50.2</td>\n",
              "      <td>145</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>65.4</td>\n",
              "      <td>51.6</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>65.4</td>\n",
              "      <td>51.6</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>65.4</td>\n",
              "      <td>51.6</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>65.4</td>\n",
              "      <td>51.6</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>63.8</td>\n",
              "      <td>54.5</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>63.8</td>\n",
              "      <td>54.5</td>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>63.8</td>\n",
              "      <td>54.5</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>63.8</td>\n",
              "      <td>54.5</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>63.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>63.8</td>\n",
              "      <td>54.5</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>63.8</td>\n",
              "      <td>53.3</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>63.8</td>\n",
              "      <td>54.5</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>63.8</td>\n",
              "      <td>53.5</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>63.8</td>\n",
              "      <td>53.3</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>65.2</td>\n",
              "      <td>54.7</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>65.2</td>\n",
              "      <td>54.7</td>\n",
              "      <td>97</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>66.5</td>\n",
              "      <td>55.1</td>\n",
              "      <td>152</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>66.5</td>\n",
              "      <td>56.1</td>\n",
              "      <td>152</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>66.5</td>\n",
              "      <td>55.1</td>\n",
              "      <td>152</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>67.9</td>\n",
              "      <td>49.7</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>67.9</td>\n",
              "      <td>49.7</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>67.9</td>\n",
              "      <td>49.7</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>68.4</td>\n",
              "      <td>56.7</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>68.4</td>\n",
              "      <td>56.7</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>68.4</td>\n",
              "      <td>58.7</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>68.4</td>\n",
              "      <td>58.7</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>68.4</td>\n",
              "      <td>56.7</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>68.4</td>\n",
              "      <td>56.7</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>68.4</td>\n",
              "      <td>56.7</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>68.4</td>\n",
              "      <td>58.7</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>68.4</td>\n",
              "      <td>56.7</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>68.4</td>\n",
              "      <td>56.7</td>\n",
              "      <td>95</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>68.3</td>\n",
              "      <td>56.0</td>\n",
              "      <td>142</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.8</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.8</td>\n",
              "      <td>102</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.6</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.8</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>63.8</td>\n",
              "      <td>50.8</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>64.6</td>\n",
              "      <td>59.8</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>66.3</td>\n",
              "      <td>50.2</td>\n",
              "      <td>145</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>68.3</td>\n",
              "      <td>50.2</td>\n",
              "      <td>143</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>65.0</td>\n",
              "      <td>51.6</td>\n",
              "      <td>207</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>65.0</td>\n",
              "      <td>51.6</td>\n",
              "      <td>207</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>65.0</td>\n",
              "      <td>51.6</td>\n",
              "      <td>207</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>66.5</td>\n",
              "      <td>56.1</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>66.5</td>\n",
              "      <td>56.1</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>66.5</td>\n",
              "      <td>56.1</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>66.5</td>\n",
              "      <td>56.1</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>66.5</td>\n",
              "      <td>56.1</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>66.5</td>\n",
              "      <td>56.1</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>63.4</td>\n",
              "      <td>53.7</td>\n",
              "      <td>69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>63.6</td>\n",
              "      <td>53.7</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>63.8</td>\n",
              "      <td>55.7</td>\n",
              "      <td>73</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>65.4</td>\n",
              "      <td>52.5</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>65.4</td>\n",
              "      <td>52.5</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>65.4</td>\n",
              "      <td>52.5</td>\n",
              "      <td>94</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>65.4</td>\n",
              "      <td>54.3</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>65.4</td>\n",
              "      <td>54.3</td>\n",
              "      <td>111</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>65.4</td>\n",
              "      <td>53.0</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>65.4</td>\n",
              "      <td>53.0</td>\n",
              "      <td>94</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>65.4</td>\n",
              "      <td>54.9</td>\n",
              "      <td>82</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>65.4</td>\n",
              "      <td>54.9</td>\n",
              "      <td>111</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>63.6</td>\n",
              "      <td>54.5</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>63.6</td>\n",
              "      <td>54.5</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>63.6</td>\n",
              "      <td>54.5</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>63.6</td>\n",
              "      <td>59.1</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>63.6</td>\n",
              "      <td>59.1</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>63.6</td>\n",
              "      <td>59.1</td>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>64.4</td>\n",
              "      <td>53.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>64.4</td>\n",
              "      <td>52.8</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>64.4</td>\n",
              "      <td>53.0</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>64.4</td>\n",
              "      <td>52.8</td>\n",
              "      <td>56</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>64.4</td>\n",
              "      <td>53.0</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>64.4</td>\n",
              "      <td>52.8</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>64.4</td>\n",
              "      <td>52.8</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>64.0</td>\n",
              "      <td>52.6</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>64.0</td>\n",
              "      <td>52.6</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>64.0</td>\n",
              "      <td>52.6</td>\n",
              "      <td>112</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>64.0</td>\n",
              "      <td>52.6</td>\n",
              "      <td>112</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>65.6</td>\n",
              "      <td>52.0</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>65.6</td>\n",
              "      <td>52.0</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>65.6</td>\n",
              "      <td>52.0</td>\n",
              "      <td>116</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>65.6</td>\n",
              "      <td>52.0</td>\n",
              "      <td>116</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>65.6</td>\n",
              "      <td>52.0</td>\n",
              "      <td>116</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>65.6</td>\n",
              "      <td>53.0</td>\n",
              "      <td>116</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>66.5</td>\n",
              "      <td>54.9</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>66.5</td>\n",
              "      <td>54.9</td>\n",
              "      <td>73</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>66.5</td>\n",
              "      <td>53.9</td>\n",
              "      <td>92</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>66.5</td>\n",
              "      <td>54.9</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>66.5</td>\n",
              "      <td>53.9</td>\n",
              "      <td>92</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>67.7</td>\n",
              "      <td>52.0</td>\n",
              "      <td>161</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>67.7</td>\n",
              "      <td>52.0</td>\n",
              "      <td>161</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>66.5</td>\n",
              "      <td>54.1</td>\n",
              "      <td>156</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>66.5</td>\n",
              "      <td>54.1</td>\n",
              "      <td>156</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>65.5</td>\n",
              "      <td>55.7</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>65.5</td>\n",
              "      <td>55.7</td>\n",
              "      <td>85</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>65.5</td>\n",
              "      <td>55.7</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>65.5</td>\n",
              "      <td>55.7</td>\n",
              "      <td>85</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>65.5</td>\n",
              "      <td>55.7</td>\n",
              "      <td>85</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>65.5</td>\n",
              "      <td>55.7</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>65.5</td>\n",
              "      <td>55.7</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>64.2</td>\n",
              "      <td>55.6</td>\n",
              "      <td>90</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>64.0</td>\n",
              "      <td>51.4</td>\n",
              "      <td>90</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>66.9</td>\n",
              "      <td>55.1</td>\n",
              "      <td>110</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>66.9</td>\n",
              "      <td>55.1</td>\n",
              "      <td>68</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>66.9</td>\n",
              "      <td>55.1</td>\n",
              "      <td>88</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>67.2</td>\n",
              "      <td>56.2</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>67.2</td>\n",
              "      <td>57.5</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>67.2</td>\n",
              "      <td>56.2</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>67.2</td>\n",
              "      <td>57.5</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>67.2</td>\n",
              "      <td>56.2</td>\n",
              "      <td>162</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>67.2</td>\n",
              "      <td>57.5</td>\n",
              "      <td>162</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>68.9</td>\n",
              "      <td>55.5</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>68.8</td>\n",
              "      <td>55.5</td>\n",
              "      <td>160</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>68.9</td>\n",
              "      <td>55.5</td>\n",
              "      <td>134</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>68.9</td>\n",
              "      <td>55.5</td>\n",
              "      <td>106</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>68.9</td>\n",
              "      <td>55.5</td>\n",
              "      <td>114</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a28fa736-d434-4eda-8d71-a66794f0ecb5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a28fa736-d434-4eda-8d71-a66794f0ecb5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a28fa736-d434-4eda-8d71-a66794f0ecb5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-daa7ec89-4d66-404d-b85e-e95d8b9d1cf7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-daa7ec89-4d66-404d-b85e-e95d8b9d1cf7')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-daa7ec89-4d66-404d-b85e-e95d8b9d1cf7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "auto_df.groupby('price_over_10K').size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf6Wd0m_ddrb",
        "outputId": "5ea0a2a3-ee7a-4349-d7c6-f41cd00c19ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "price_over_10K\n",
              "0     96\n",
              "1    103\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fCSS_mACsbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9979cc58-75ec-4083-a2ef-255035d99f23"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#説明変数，目的変数の設定\n",
        "X = auto_df.drop('price_over_10K', axis=1)\n",
        "y = auto_df['price_over_10K']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None )\n",
        "\n",
        "# ロジスティック回帰のモデル（インスタンス）生成\n",
        "lr = LogisticRegression()\n",
        "#トレーニングデータによる学習\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(lr.score(X_train, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(lr.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.8849\n",
            "正解率（test):  0.8167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPr916dcCsbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde07ec3-7e8e-4a8d-aa1c-2c36c07549d0"
      },
      "source": [
        "help( LogisticRegression)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
            "\n",
            "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
            " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
            " |  \n",
            " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
            " |  \n",
            " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
            " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
            " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
            " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
            " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
            " |  \n",
            " |  This class implements regularized logistic regression using the\n",
            " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
            " |  that regularization is applied by default**. It can handle both dense\n",
            " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
            " |  floats for optimal performance; any other input format will be converted\n",
            " |  (and copied).\n",
            " |  \n",
            " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
            " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
            " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
            " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
            " |  'saga' solver.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n",
            " |      Specify the norm of the penalty:\n",
            " |  \n",
            " |      - `None`: no penalty is added;\n",
            " |      - `'l2'`: add a L2 penalty term and it is the default choice;\n",
            " |      - `'l1'`: add a L1 penalty term;\n",
            " |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n",
            " |  \n",
            " |      .. warning::\n",
            " |         Some penalties may not work with some solvers. See the parameter\n",
            " |         `solver` below, to know the compatibility between the penalty and\n",
            " |         solver.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
            " |  \n",
            " |      .. deprecated:: 1.2\n",
            " |         The 'none' option was deprecated in version 1.2, and will be removed\n",
            " |         in 1.4. Use `None` instead.\n",
            " |  \n",
            " |  dual : bool, default=False\n",
            " |      Dual or primal formulation. Dual formulation is only implemented for\n",
            " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
            " |      n_samples > n_features.\n",
            " |  \n",
            " |  tol : float, default=1e-4\n",
            " |      Tolerance for stopping criteria.\n",
            " |  \n",
            " |  C : float, default=1.0\n",
            " |      Inverse of regularization strength; must be a positive float.\n",
            " |      Like in support vector machines, smaller values specify stronger\n",
            " |      regularization.\n",
            " |  \n",
            " |  fit_intercept : bool, default=True\n",
            " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
            " |      added to the decision function.\n",
            " |  \n",
            " |  intercept_scaling : float, default=1\n",
            " |      Useful only when the solver 'liblinear' is used\n",
            " |      and self.fit_intercept is set to True. In this case, x becomes\n",
            " |      [x, self.intercept_scaling],\n",
            " |      i.e. a \"synthetic\" feature with constant value equal to\n",
            " |      intercept_scaling is appended to the instance vector.\n",
            " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
            " |  \n",
            " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
            " |      as all other features.\n",
            " |      To lessen the effect of regularization on synthetic feature weight\n",
            " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
            " |  \n",
            " |  class_weight : dict or 'balanced', default=None\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If not given, all classes are supposed to have weight one.\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         *class_weight='balanced'*\n",
            " |  \n",
            " |  random_state : int, RandomState instance, default=None\n",
            " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
            " |      data. See :term:`Glossary <random_state>` for details.\n",
            " |  \n",
            " |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n",
            " |  \n",
            " |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n",
            " |      To choose a solver, you might want to consider the following aspects:\n",
            " |  \n",
            " |          - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n",
            " |            and 'saga' are faster for large ones;\n",
            " |          - For multiclass problems, only 'newton-cg', 'sag', 'saga' and\n",
            " |            'lbfgs' handle multinomial loss;\n",
            " |          - 'liblinear' is limited to one-versus-rest schemes.\n",
            " |          - 'newton-cholesky' is a good choice for `n_samples` >> `n_features`,\n",
            " |            especially with one-hot encoded categorical features with rare\n",
            " |            categories. Note that it is limited to binary classification and the\n",
            " |            one-versus-rest reduction for multiclass classification. Be aware that\n",
            " |            the memory usage of this solver has a quadratic dependency on\n",
            " |            `n_features` because it explicitly computes the Hessian matrix.\n",
            " |  \n",
            " |      .. warning::\n",
            " |         The choice of the algorithm depends on the penalty chosen.\n",
            " |         Supported penalties by solver:\n",
            " |  \n",
            " |         - 'lbfgs'           -   ['l2', None]\n",
            " |         - 'liblinear'       -   ['l1', 'l2']\n",
            " |         - 'newton-cg'       -   ['l2', None]\n",
            " |         - 'newton-cholesky' -   ['l2', None]\n",
            " |         - 'sag'             -   ['l2', None]\n",
            " |         - 'saga'            -   ['elasticnet', 'l1', 'l2', None]\n",
            " |  \n",
            " |      .. note::\n",
            " |         'sag' and 'saga' fast convergence is only guaranteed on features\n",
            " |         with approximately the same scale. You can preprocess the data with\n",
            " |         a scaler from :mod:`sklearn.preprocessing`.\n",
            " |  \n",
            " |      .. seealso::\n",
            " |         Refer to the User Guide for more information regarding\n",
            " |         :class:`LogisticRegression` and more specifically the\n",
            " |         :ref:`Table <Logistic_regression>`\n",
            " |         summarizing solver/penalty supports.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         Stochastic Average Gradient descent solver.\n",
            " |      .. versionadded:: 0.19\n",
            " |         SAGA solver.\n",
            " |      .. versionchanged:: 0.22\n",
            " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
            " |      .. versionadded:: 1.2\n",
            " |         newton-cholesky solver.\n",
            " |  \n",
            " |  max_iter : int, default=100\n",
            " |      Maximum number of iterations taken for the solvers to converge.\n",
            " |  \n",
            " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
            " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
            " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
            " |      across the entire probability distribution, *even when the data is\n",
            " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
            " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
            " |      and otherwise selects 'multinomial'.\n",
            " |  \n",
            " |      .. versionadded:: 0.18\n",
            " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
            " |      .. versionchanged:: 0.22\n",
            " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
            " |  \n",
            " |  verbose : int, default=0\n",
            " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
            " |      number for verbosity.\n",
            " |  \n",
            " |  warm_start : bool, default=False\n",
            " |      When set to True, reuse the solution of the previous call to fit as\n",
            " |      initialization, otherwise, just erase the previous solution.\n",
            " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
            " |  \n",
            " |  n_jobs : int, default=None\n",
            " |      Number of CPU cores used when parallelizing over classes if\n",
            " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
            " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
            " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            " |      context. ``-1`` means using all processors.\n",
            " |      See :term:`Glossary <n_jobs>` for more details.\n",
            " |  \n",
            " |  l1_ratio : float, default=None\n",
            " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
            " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
            " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
            " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
            " |      combination of L1 and L2.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  \n",
            " |  classes_ : ndarray of shape (n_classes, )\n",
            " |      A list of class labels known to the classifier.\n",
            " |  \n",
            " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
            " |      Coefficient of the features in the decision function.\n",
            " |  \n",
            " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
            " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
            " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
            " |  \n",
            " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
            " |      Intercept (a.k.a. bias) added to the decision function.\n",
            " |  \n",
            " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
            " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
            " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
            " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
            " |      outcome 0 (False).\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
            " |      Actual number of iterations for all classes. If binary or multinomial,\n",
            " |      it returns only 1 element. For liblinear solver, only the maximum\n",
            " |      number of iteration across all classes is given.\n",
            " |  \n",
            " |      .. versionchanged:: 0.20\n",
            " |  \n",
            " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
            " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
            " |      the parameter ``loss=\"log\"``).\n",
            " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The underlying C implementation uses a random number generator to\n",
            " |  select features when fitting the model. It is thus not uncommon,\n",
            " |  to have slightly different results for the same input data. If\n",
            " |  that happens, try with a smaller tol parameter.\n",
            " |  \n",
            " |  Predict output may not match that of standalone liblinear in certain\n",
            " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
            " |  in the narrative documentation.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  \n",
            " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
            " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
            " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
            " |  \n",
            " |  LIBLINEAR -- A Library for Large Linear Classification\n",
            " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
            " |  \n",
            " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
            " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
            " |      https://hal.inria.fr/hal-00860051/document\n",
            " |  \n",
            " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
            " |          :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n",
            " |          for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n",
            " |  \n",
            " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
            " |      methods for logistic regression and maximum entropy models.\n",
            " |      Machine Learning 85(1-2):41-75.\n",
            " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.datasets import load_iris\n",
            " |  >>> from sklearn.linear_model import LogisticRegression\n",
            " |  >>> X, y = load_iris(return_X_y=True)\n",
            " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
            " |  >>> clf.predict(X[:2, :])\n",
            " |  array([0, 0])\n",
            " |  >>> clf.predict_proba(X[:2, :])\n",
            " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
            " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
            " |  >>> clf.score(X, y)\n",
            " |  0.97...\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LogisticRegression\n",
            " |      sklearn.linear_model._base.LinearClassifierMixin\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.linear_model._base.SparseCoefMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit the model according to the given training data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training vector, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target vector relative to X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,) default=None\n",
            " |          Array of weights that are assigned to individual samples.\n",
            " |          If not provided, then each sample is given unit weight.\n",
            " |      \n",
            " |          .. versionadded:: 0.17\n",
            " |             *sample_weight* support to LogisticRegression.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |          Fitted estimator.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict logarithm of probability estimates.\n",
            " |      \n",
            " |      The returned estimates for all classes are ordered by the\n",
            " |      label of classes.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Vector to be scored, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      T : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the log-probability of the sample for each class in the\n",
            " |          model, where classes are ordered as they are in ``self.classes_``.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Probability estimates.\n",
            " |      \n",
            " |      The returned estimates for all classes are ordered by the\n",
            " |      label of classes.\n",
            " |      \n",
            " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
            " |      the softmax function is used to find the predicted probability of\n",
            " |      each class.\n",
            " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
            " |      of each class assuming it to be positive using the logistic function.\n",
            " |      and normalize these values across all the classes.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Vector to be scored, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      T : array-like of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the sample for each class in the model,\n",
            " |          where classes are ordered as they are in ``self.classes_``.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
            " |  \n",
            " |  decision_function(self, X)\n",
            " |      Predict confidence scores for samples.\n",
            " |      \n",
            " |      The confidence score for a sample is proportional to the signed\n",
            " |      distance of that sample to the hyperplane.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data matrix for which we want to get the confidence scores.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
            " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
            " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
            " |          this class would be predicted.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict class labels for samples in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data matrix for which we want to get the predictions.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y_pred : ndarray of shape (n_samples,)\n",
            " |          Vector containing the class labels for each sample.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
            " |  \n",
            " |  densify(self)\n",
            " |      Convert coefficient matrix to dense array format.\n",
            " |      \n",
            " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
            " |      default format of ``coef_`` and is required for fitting, so calling\n",
            " |      this method is only required on models that have previously been\n",
            " |      sparsified; otherwise, it is a no-op.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  sparsify(self)\n",
            " |      Convert coefficient matrix to sparse format.\n",
            " |      \n",
            " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
            " |      L1-regularized models can be much more memory- and storage-efficient\n",
            " |      than the usual numpy.ndarray representation.\n",
            " |      \n",
            " |      The ``intercept_`` member is not converted.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |          Fitted estimator.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
            " |      this may actually *increase* memory usage, so use this method with\n",
            " |      care. A rule of thumb is that the number of zero elements, which can\n",
            " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
            " |      to provide significant benefits.\n",
            " |      \n",
            " |      After calling this method, further fitting with the partial_fit\n",
            " |      method (if any) will not work until you call densify.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFyWa3I_Csbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9fbf809-f375-44b1-c68a-cbe31d7bdd08"
      },
      "source": [
        "#各クラスに所属する確率表示\n",
        "lr.predict_proba(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.215e-01, 1.785e-01],\n",
              "       [8.086e-01, 1.914e-01],\n",
              "       [6.830e-01, 3.170e-01],\n",
              "       [5.218e-01, 4.782e-01],\n",
              "       [9.965e-04, 9.990e-01],\n",
              "       [9.687e-01, 3.131e-02],\n",
              "       [5.334e-01, 4.666e-01],\n",
              "       [6.768e-01, 3.232e-01],\n",
              "       [1.374e-02, 9.863e-01],\n",
              "       [6.393e-01, 3.607e-01],\n",
              "       [9.808e-01, 1.917e-02],\n",
              "       [9.902e-01, 9.757e-03],\n",
              "       [6.890e-04, 9.993e-01],\n",
              "       [3.429e-01, 6.571e-01],\n",
              "       [9.808e-01, 1.917e-02],\n",
              "       [7.766e-01, 2.234e-01],\n",
              "       [9.687e-01, 3.131e-02],\n",
              "       [9.680e-01, 3.202e-02],\n",
              "       [2.827e-03, 9.972e-01],\n",
              "       [7.890e-01, 2.110e-01],\n",
              "       [2.089e-02, 9.791e-01],\n",
              "       [1.169e-04, 9.999e-01],\n",
              "       [3.785e-04, 9.996e-01],\n",
              "       [3.542e-02, 9.646e-01],\n",
              "       [5.089e-08, 1.000e+00],\n",
              "       [3.542e-02, 9.646e-01],\n",
              "       [4.434e-02, 9.557e-01],\n",
              "       [9.784e-01, 2.156e-02],\n",
              "       [4.946e-05, 1.000e+00],\n",
              "       [7.601e-01, 2.399e-01],\n",
              "       [9.886e-01, 1.137e-02],\n",
              "       [7.395e-04, 9.993e-01],\n",
              "       [2.383e-01, 7.617e-01],\n",
              "       [9.907e-01, 9.316e-03],\n",
              "       [1.005e-01, 8.995e-01],\n",
              "       [9.833e-01, 1.670e-02],\n",
              "       [2.383e-01, 7.617e-01],\n",
              "       [2.089e-02, 9.791e-01],\n",
              "       [9.884e-01, 1.163e-02],\n",
              "       [9.874e-01, 1.259e-02],\n",
              "       [9.886e-01, 1.137e-02],\n",
              "       [9.744e-01, 2.562e-02],\n",
              "       [7.703e-05, 9.999e-01],\n",
              "       [1.720e-03, 9.983e-01],\n",
              "       [9.884e-01, 1.163e-02],\n",
              "       [4.434e-02, 9.557e-01],\n",
              "       [3.468e-02, 9.653e-01],\n",
              "       [5.395e-06, 1.000e+00],\n",
              "       [3.433e-01, 6.567e-01],\n",
              "       [9.808e-01, 1.917e-02],\n",
              "       [9.888e-01, 1.124e-02],\n",
              "       [4.692e-01, 5.308e-01],\n",
              "       [5.334e-01, 4.666e-01],\n",
              "       [8.215e-01, 1.785e-01],\n",
              "       [7.395e-04, 9.993e-01],\n",
              "       [2.852e-01, 7.148e-01],\n",
              "       [6.283e-01, 3.717e-01],\n",
              "       [3.429e-01, 6.571e-01],\n",
              "       [1.907e-02, 9.809e-01],\n",
              "       [9.640e-01, 3.603e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9me7Y2JCCsbw"
      },
      "source": [
        "### 演習１．他の説明変数によるロジスティック回帰"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWTu_Xl5Csbw"
      },
      "source": [
        "# 目的変数 price_over_10K\n",
        "# 説明変数を別のもの（例えば，width, length, engine-size, mpg）に変えて score を求める\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7dmsw1uCsbw"
      },
      "source": [
        "### 参考：シグモイド関数表示"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQCKIeGwCsbw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "dd3ba071-179c-454d-e88d-014cbb07b8e4"
      },
      "source": [
        "#シグモイド関数表示\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "x = np.arange(-10, 10, 0.1)\n",
        "e = math.e\n",
        "\n",
        "y = 1 / (1 + e**-x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.title('sigmoid')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6XIeEKTCsbx"
      },
      "source": [
        "# ２.SVM（サポートベクターマシン）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HsAL2x7Csbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e981d585-4b8e-43aa-cc08-0953e26bc7da"
      },
      "source": [
        "# 通常のSVM（カーネル法）による学習と予測\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 線形SVMのモデル（インスタンス）を生成\n",
        "sv = SVC(C=10,kernel='linear', gamma=0.15,random_state=None)\n",
        "\n",
        "#線形SVMによるトレーニングデータを用いた学習\n",
        "sv.fit(X_train, y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(sv.score(X_train, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(sv.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.8921\n",
            "正解率（test):  0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK4hu6F_Csbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29785ac7-9d22-4690-bb9a-d0d5e81a031c"
      },
      "source": [
        "help(SVC)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class SVC in module sklearn.svm._classes:\n",
            "\n",
            "class SVC(sklearn.svm._base.BaseSVC)\n",
            " |  SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
            " |  \n",
            " |  C-Support Vector Classification.\n",
            " |  \n",
            " |  The implementation is based on libsvm. The fit time scales at least\n",
            " |  quadratically with the number of samples and may be impractical\n",
            " |  beyond tens of thousands of samples. For large datasets\n",
            " |  consider using :class:`~sklearn.svm.LinearSVC` or\n",
            " |  :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
            " |  :class:`~sklearn.kernel_approximation.Nystroem` transformer or\n",
            " |  other :ref:`kernel_approximation`.\n",
            " |  \n",
            " |  The multiclass support is handled according to a one-vs-one scheme.\n",
            " |  \n",
            " |  For details on the precise mathematical formulation of the provided\n",
            " |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
            " |  other, see the corresponding section in the narrative documentation:\n",
            " |  :ref:`svm_kernels`.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  C : float, default=1.0\n",
            " |      Regularization parameter. The strength of the regularization is\n",
            " |      inversely proportional to C. Must be strictly positive. The penalty\n",
            " |      is a squared l2 penalty.\n",
            " |  \n",
            " |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n",
            " |      Specifies the kernel type to be used in the algorithm.\n",
            " |      If none is given, 'rbf' will be used. If a callable is given it is\n",
            " |      used to pre-compute the kernel matrix from data matrices; that matrix\n",
            " |      should be an array of shape ``(n_samples, n_samples)``.\n",
            " |  \n",
            " |  degree : int, default=3\n",
            " |      Degree of the polynomial kernel function ('poly').\n",
            " |      Must be non-negative. Ignored by all other kernels.\n",
            " |  \n",
            " |  gamma : {'scale', 'auto'} or float, default='scale'\n",
            " |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
            " |  \n",
            " |      - if ``gamma='scale'`` (default) is passed then it uses\n",
            " |        1 / (n_features * X.var()) as value of gamma,\n",
            " |      - if 'auto', uses 1 / n_features\n",
            " |      - if float, must be non-negative.\n",
            " |  \n",
            " |      .. versionchanged:: 0.22\n",
            " |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
            " |  \n",
            " |  coef0 : float, default=0.0\n",
            " |      Independent term in kernel function.\n",
            " |      It is only significant in 'poly' and 'sigmoid'.\n",
            " |  \n",
            " |  shrinking : bool, default=True\n",
            " |      Whether to use the shrinking heuristic.\n",
            " |      See the :ref:`User Guide <shrinking_svm>`.\n",
            " |  \n",
            " |  probability : bool, default=False\n",
            " |      Whether to enable probability estimates. This must be enabled prior\n",
            " |      to calling `fit`, will slow down that method as it internally uses\n",
            " |      5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
            " |      `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
            " |  \n",
            " |  tol : float, default=1e-3\n",
            " |      Tolerance for stopping criterion.\n",
            " |  \n",
            " |  cache_size : float, default=200\n",
            " |      Specify the size of the kernel cache (in MB).\n",
            " |  \n",
            " |  class_weight : dict or 'balanced', default=None\n",
            " |      Set the parameter C of class i to class_weight[i]*C for\n",
            " |      SVC. If not given, all classes are supposed to have\n",
            " |      weight one.\n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
            " |  \n",
            " |  verbose : bool, default=False\n",
            " |      Enable verbose output. Note that this setting takes advantage of a\n",
            " |      per-process runtime setting in libsvm that, if enabled, may not work\n",
            " |      properly in a multithreaded context.\n",
            " |  \n",
            " |  max_iter : int, default=-1\n",
            " |      Hard limit on iterations within solver, or -1 for no limit.\n",
            " |  \n",
            " |  decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
            " |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
            " |      (n_samples, n_classes) as all other classifiers, or the original\n",
            " |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
            " |      (n_samples, n_classes * (n_classes - 1) / 2). However, note that\n",
            " |      internally, one-vs-one ('ovo') is always used as a multi-class strategy\n",
            " |      to train models; an ovr matrix is only constructed from the ovo matrix.\n",
            " |      The parameter is ignored for binary classification.\n",
            " |  \n",
            " |      .. versionchanged:: 0.19\n",
            " |          decision_function_shape is 'ovr' by default.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         *decision_function_shape='ovr'* is recommended.\n",
            " |  \n",
            " |      .. versionchanged:: 0.17\n",
            " |         Deprecated *decision_function_shape='ovo' and None*.\n",
            " |  \n",
            " |  break_ties : bool, default=False\n",
            " |      If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
            " |      :term:`predict` will break ties according to the confidence values of\n",
            " |      :term:`decision_function`; otherwise the first class among the tied\n",
            " |      classes is returned. Please note that breaking ties comes at a\n",
            " |      relatively high computational cost compared to a simple predict.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, default=None\n",
            " |      Controls the pseudo random number generation for shuffling the data for\n",
            " |      probability estimates. Ignored when `probability` is False.\n",
            " |      Pass an int for reproducible output across multiple function calls.\n",
            " |      See :term:`Glossary <random_state>`.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  class_weight_ : ndarray of shape (n_classes,)\n",
            " |      Multipliers of parameter C for each class.\n",
            " |      Computed based on the ``class_weight`` parameter.\n",
            " |  \n",
            " |  classes_ : ndarray of shape (n_classes,)\n",
            " |      The classes labels.\n",
            " |  \n",
            " |  coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
            " |      Weights assigned to the features (coefficients in the primal\n",
            " |      problem). This is only available in the case of a linear kernel.\n",
            " |  \n",
            " |      `coef_` is a readonly property derived from `dual_coef_` and\n",
            " |      `support_vectors_`.\n",
            " |  \n",
            " |  dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
            " |      Dual coefficients of the support vector in the decision\n",
            " |      function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
            " |      their targets.\n",
            " |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
            " |      The layout of the coefficients in the multiclass case is somewhat\n",
            " |      non-trivial. See the :ref:`multi-class section of the User Guide\n",
            " |      <svm_multi_class>` for details.\n",
            " |  \n",
            " |  fit_status_ : int\n",
            " |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
            " |  \n",
            " |  intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
            " |      Constants in decision function.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  n_iter_ : ndarray of shape (n_classes * (n_classes - 1) // 2,)\n",
            " |      Number of iterations run by the optimization routine to fit the model.\n",
            " |      The shape of this attribute depends on the number of models optimized\n",
            " |      which in turn depends on the number of classes.\n",
            " |  \n",
            " |      .. versionadded:: 1.1\n",
            " |  \n",
            " |  support_ : ndarray of shape (n_SV)\n",
            " |      Indices of support vectors.\n",
            " |  \n",
            " |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
            " |      Support vectors.\n",
            " |  \n",
            " |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
            " |      Number of support vectors for each class.\n",
            " |  \n",
            " |  probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
            " |  probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
            " |      If `probability=True`, it corresponds to the parameters learned in\n",
            " |      Platt scaling to produce probability estimates from decision values.\n",
            " |      If `probability=False`, it's an empty array. Platt scaling uses the\n",
            " |      logistic function\n",
            " |      ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
            " |      where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
            " |      more information on the multiclass case and training procedure see\n",
            " |      section 8 of [1]_.\n",
            " |  \n",
            " |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
            " |      Array dimensions of training vector ``X``.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  SVR : Support Vector Machine for Regression implemented using libsvm.\n",
            " |  \n",
            " |  LinearSVC : Scalable Linear Support Vector Machine for classification\n",
            " |      implemented using liblinear. Check the See Also section of\n",
            " |      LinearSVC for more comparison element.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
            " |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
            " |  \n",
            " |  .. [2] `Platt, John (1999). \"Probabilistic Outputs for Support Vector\n",
            " |      Machines and Comparisons to Regularized Likelihood Methods\"\n",
            " |      <https://citeseerx.ist.psu.edu/doc_view/pid/42e5ed832d4310ce4378c44d05570439df28a393>`_\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> from sklearn.pipeline import make_pipeline\n",
            " |  >>> from sklearn.preprocessing import StandardScaler\n",
            " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
            " |  >>> y = np.array([1, 1, 2, 2])\n",
            " |  >>> from sklearn.svm import SVC\n",
            " |  >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
            " |  >>> clf.fit(X, y)\n",
            " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
            " |                  ('svc', SVC(gamma='auto'))])\n",
            " |  \n",
            " |  >>> print(clf.predict([[-0.8, -1]]))\n",
            " |  [1]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      SVC\n",
            " |      sklearn.svm._base.BaseSVC\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.svm._base.BaseLibSVM\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  __annotations__ = {}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.svm._base.BaseSVC:\n",
            " |  \n",
            " |  decision_function(self, X)\n",
            " |      Evaluate the decision function for the samples in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n",
            " |          Returns the decision function of the sample for each class\n",
            " |          in the model.\n",
            " |          If decision_function_shape='ovr', the shape is (n_samples,\n",
            " |          n_classes).\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If decision_function_shape='ovo', the function values are proportional\n",
            " |      to the distance of the samples X to the separating hyperplane. If the\n",
            " |      exact distances are required, divide the function values by the norm of\n",
            " |      the weight vector (``coef_``). See also `this question\n",
            " |      <https://stats.stackexchange.com/questions/14876/\n",
            " |      interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n",
            " |      If decision_function_shape='ovr', the decision function is a monotonic\n",
            " |      transformation of ovo decision function.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Perform classification on samples in X.\n",
            " |      \n",
            " |      For an one-class model, +1 or -1 is returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
            " |          For kernel=\"precomputed\", the expected shape of X is\n",
            " |          (n_samples_test, n_samples_train).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y_pred : ndarray of shape (n_samples,)\n",
            " |          Class labels for samples in X.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Compute log probabilities of possible outcomes for samples in X.\n",
            " |      \n",
            " |      The model need to have probability information computed at training\n",
            " |      time: fit with attribute `probability` set to True.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
            " |          For kernel=\"precomputed\", the expected shape of X is\n",
            " |          (n_samples_test, n_samples_train).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      T : ndarray of shape (n_samples, n_classes)\n",
            " |          Returns the log-probabilities of the sample for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The probability model is created using cross validation, so\n",
            " |      the results can be slightly different than those obtained by\n",
            " |      predict. Also, it will produce meaningless results on very small\n",
            " |      datasets.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Compute probabilities of possible outcomes for samples in X.\n",
            " |      \n",
            " |      The model need to have probability information computed at training\n",
            " |      time: fit with attribute `probability` set to True.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          For kernel=\"precomputed\", the expected shape of X is\n",
            " |          (n_samples_test, n_samples_train).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      T : ndarray of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the sample for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The probability model is created using cross validation, so\n",
            " |      the results can be slightly different than those obtained by\n",
            " |      predict. Also, it will produce meaningless results on very small\n",
            " |      datasets.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from sklearn.svm._base.BaseSVC:\n",
            " |  \n",
            " |  probA_\n",
            " |      Parameter learned in Platt scaling when `probability=True`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ndarray of shape  (n_classes * (n_classes - 1) / 2)\n",
            " |  \n",
            " |  probB_\n",
            " |      Parameter learned in Platt scaling when `probability=True`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ndarray of shape  (n_classes * (n_classes - 1) / 2)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from sklearn.svm._base.BaseSVC:\n",
            " |  \n",
            " |  unused_param = 'nu'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.svm._base.BaseLibSVM:\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit the SVM model according to the given training data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
            " |          Training vectors, where `n_samples` is the number of samples\n",
            " |          and `n_features` is the number of features.\n",
            " |          For kernel=\"precomputed\", the expected shape of X is\n",
            " |          (n_samples, n_samples).\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target values (class labels in classification, real numbers in\n",
            " |          regression).\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Per-sample weights. Rescale C per sample. Higher weights\n",
            " |          force the classifier to put more emphasis on these points.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted estimator.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
            " |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
            " |      \n",
            " |      If X is a dense array, then the other methods will not support sparse\n",
            " |      matrices as input.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from sklearn.svm._base.BaseLibSVM:\n",
            " |  \n",
            " |  coef_\n",
            " |      Weights assigned to the features when `kernel=\"linear\"`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ndarray of shape (n_features, n_classes)\n",
            " |  \n",
            " |  n_support_\n",
            " |      Number of support vectors for each class.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgPLEJqsCsbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3b51049-06ca-4b1b-b438-e80ebccda642"
      },
      "source": [
        "#線形カーネルに特化した線形SVMによる学習と予測\n",
        "from sklearn.svm import LinearSVC\n",
        "# 線形SVMのモデル（インスタンス）を生成\n",
        "lsv = LinearSVC(C=0.1, random_state=0)\n",
        "\n",
        "#線形SVMによるトレーニングデータを用いた学習\n",
        "lsv.fit(X_train, y_train)\n",
        "\n",
        "#トレーニングデータによるフィッティング結果とテストデータによる学習モデルの精度の検証\n",
        "print('正解率（train):  {:.4f}'.format(lsv.score(X_train, y_train)))\n",
        "print('正解率（test):  {:.4f}'.format(lsv.score(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正解率（train):  0.7986\n",
            "正解率（test):  0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39qg9-KQCsby",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a1bcd3f-c2a1-415c-e701-177896a8c203"
      },
      "source": [
        "help(LinearSVC)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class LinearSVC in module sklearn.svm._classes:\n",
            "\n",
            "class LinearSVC(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
            " |  LinearSVC(penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
            " |  \n",
            " |  Linear Support Vector Classification.\n",
            " |  \n",
            " |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
            " |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
            " |  penalties and loss functions and should scale better to large numbers of\n",
            " |  samples.\n",
            " |  \n",
            " |  This class supports both dense and sparse input and the multiclass support\n",
            " |  is handled according to a one-vs-the-rest scheme.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  penalty : {'l1', 'l2'}, default='l2'\n",
            " |      Specifies the norm used in the penalization. The 'l2'\n",
            " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
            " |      vectors that are sparse.\n",
            " |  \n",
            " |  loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n",
            " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
            " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
            " |      square of the hinge loss. The combination of ``penalty='l1'``\n",
            " |      and ``loss='hinge'`` is not supported.\n",
            " |  \n",
            " |  dual : bool, default=True\n",
            " |      Select the algorithm to either solve the dual or primal\n",
            " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
            " |  \n",
            " |  tol : float, default=1e-4\n",
            " |      Tolerance for stopping criteria.\n",
            " |  \n",
            " |  C : float, default=1.0\n",
            " |      Regularization parameter. The strength of the regularization is\n",
            " |      inversely proportional to C. Must be strictly positive.\n",
            " |  \n",
            " |  multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n",
            " |      Determines the multi-class strategy if `y` contains more than\n",
            " |      two classes.\n",
            " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
            " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
            " |      While `crammer_singer` is interesting from a theoretical perspective\n",
            " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
            " |      to better accuracy and is more expensive to compute.\n",
            " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
            " |      will be ignored.\n",
            " |  \n",
            " |  fit_intercept : bool, default=True\n",
            " |      Whether to calculate the intercept for this model. If set\n",
            " |      to false, no intercept will be used in calculations\n",
            " |      (i.e. data is expected to be already centered).\n",
            " |  \n",
            " |  intercept_scaling : float, default=1.0\n",
            " |      When self.fit_intercept is True, instance vector x becomes\n",
            " |      ``[x, self.intercept_scaling]``,\n",
            " |      i.e. a \"synthetic\" feature with constant value equals to\n",
            " |      intercept_scaling is appended to the instance vector.\n",
            " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
            " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
            " |      as all other features.\n",
            " |      To lessen the effect of regularization on synthetic feature weight\n",
            " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
            " |  \n",
            " |  class_weight : dict or 'balanced', default=None\n",
            " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
            " |      SVC. If not given, all classes are supposed to have\n",
            " |      weight one.\n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
            " |  \n",
            " |  verbose : int, default=0\n",
            " |      Enable verbose output. Note that this setting takes advantage of a\n",
            " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
            " |      properly in a multithreaded context.\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, default=None\n",
            " |      Controls the pseudo random number generation for shuffling the data for\n",
            " |      the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n",
            " |      underlying implementation of :class:`LinearSVC` is not random and\n",
            " |      ``random_state`` has no effect on the results.\n",
            " |      Pass an int for reproducible output across multiple function calls.\n",
            " |      See :term:`Glossary <random_state>`.\n",
            " |  \n",
            " |  max_iter : int, default=1000\n",
            " |      The maximum number of iterations to be run.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n",
            " |      Weights assigned to the features (coefficients in the primal\n",
            " |      problem).\n",
            " |  \n",
            " |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
            " |      follows the internal memory layout of liblinear.\n",
            " |  \n",
            " |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
            " |      Constants in decision function.\n",
            " |  \n",
            " |  classes_ : ndarray of shape (n_classes,)\n",
            " |      The unique classes labels.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  n_iter_ : int\n",
            " |      Maximum number of iterations run across all classes.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  SVC : Implementation of Support Vector Machine classifier using libsvm:\n",
            " |      the kernel can be non-linear but its SMO algorithm does not\n",
            " |      scale to large number of samples as LinearSVC does.\n",
            " |  \n",
            " |      Furthermore SVC multi-class mode is implemented using one\n",
            " |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
            " |      possible to implement one vs the rest with SVC by using the\n",
            " |      :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
            " |  \n",
            " |      Finally SVC can fit dense data without memory copy if the input\n",
            " |      is C-contiguous. Sparse data will still incur memory copy though.\n",
            " |  \n",
            " |  sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n",
            " |      cost function as LinearSVC\n",
            " |      by adjusting the penalty and loss parameters. In addition it requires\n",
            " |      less memory, allows incremental (online) learning, and implements\n",
            " |      various loss functions and regularization regimes.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The underlying C implementation uses a random number generator to\n",
            " |  select features when fitting the model. It is thus not uncommon\n",
            " |  to have slightly different results for the same input data. If\n",
            " |  that happens, try with a smaller ``tol`` parameter.\n",
            " |  \n",
            " |  The underlying implementation, liblinear, uses a sparse internal\n",
            " |  representation for the data that will incur a memory copy.\n",
            " |  \n",
            " |  Predict output may not match that of standalone liblinear in certain\n",
            " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
            " |  in the narrative documentation.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  `LIBLINEAR: A Library for Large Linear Classification\n",
            " |  <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.svm import LinearSVC\n",
            " |  >>> from sklearn.pipeline import make_pipeline\n",
            " |  >>> from sklearn.preprocessing import StandardScaler\n",
            " |  >>> from sklearn.datasets import make_classification\n",
            " |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
            " |  >>> clf = make_pipeline(StandardScaler(),\n",
            " |  ...                     LinearSVC(random_state=0, tol=1e-5))\n",
            " |  >>> clf.fit(X, y)\n",
            " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
            " |                  ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
            " |  \n",
            " |  >>> print(clf.named_steps['linearsvc'].coef_)\n",
            " |  [[0.141...   0.526... 0.679... 0.493...]]\n",
            " |  \n",
            " |  >>> print(clf.named_steps['linearsvc'].intercept_)\n",
            " |  [0.1693...]\n",
            " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
            " |  [1]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      LinearSVC\n",
            " |      sklearn.linear_model._base.LinearClassifierMixin\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.linear_model._base.SparseCoefMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit the model according to the given training data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          Training vector, where `n_samples` is the number of samples and\n",
            " |          `n_features` is the number of features.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target vector relative to X.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Array of weights that are assigned to individual\n",
            " |          samples. If not provided,\n",
            " |          then each sample is given unit weight.\n",
            " |      \n",
            " |          .. versionadded:: 0.18\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          An instance of the estimator.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
            " |  \n",
            " |  decision_function(self, X)\n",
            " |      Predict confidence scores for samples.\n",
            " |      \n",
            " |      The confidence score for a sample is proportional to the signed\n",
            " |      distance of that sample to the hyperplane.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data matrix for which we want to get the confidence scores.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
            " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
            " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
            " |          this class would be predicted.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict class labels for samples in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The data matrix for which we want to get the predictions.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y_pred : ndarray of shape (n_samples,)\n",
            " |          Vector containing the class labels for each sample.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
            " |  \n",
            " |  densify(self)\n",
            " |      Convert coefficient matrix to dense array format.\n",
            " |      \n",
            " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
            " |      default format of ``coef_`` and is required for fitting, so calling\n",
            " |      this method is only required on models that have previously been\n",
            " |      sparsified; otherwise, it is a no-op.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  sparsify(self)\n",
            " |      Convert coefficient matrix to sparse format.\n",
            " |      \n",
            " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
            " |      L1-regularized models can be much more memory- and storage-efficient\n",
            " |      than the usual numpy.ndarray representation.\n",
            " |      \n",
            " |      The ``intercept_`` member is not converted.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self\n",
            " |          Fitted estimator.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
            " |      this may actually *increase* memory usage, so use this method with\n",
            " |      care. A rule of thumb is that the number of zero elements, which can\n",
            " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
            " |      to provide significant benefits.\n",
            " |      \n",
            " |      After calling this method, further fitting with the partial_fit\n",
            " |      method (if any) will not work until you call densify.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbwZOWlxCsby"
      },
      "source": [
        "### 演習２．標準化したデータに対してSVMを実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZgrKi83Csby"
      },
      "source": [
        "#標準化した後に，SVMを実行\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}